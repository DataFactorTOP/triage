# postmodel-analysis
Library of post modeling analysis functions. This library will be used in two scenarios:
1. Explore and compare a small number of models: After you've built thousands of models over different time windows and done some preliminary analysis, you have a small number of models you want to select the "final" model from.
2. Analyze the final model you've selected

## Components
* Score Distributions: Takes model_id, test_time as input and produces a plot or list as output ([example](https://github.com/dssg/postmodel-analysis/blob/master/examples/example_scoring.py))

* List Comparison: Compare the lists generated by a set of models. Takes model_id(s), test_time as input and calculate the following metrics:
    - jaccard similarity over top k
    - overall rank correlation
    - rank correlation of the top k of each pair (non symetric)
    - where we do have labels, we can calculate overlap in 0s vs 1s
    - Output: 
      - metrics as dictionary 
      - TODO: heatmap (model by model overlap)
    - ([example](https://github.com/dssg/postmodel-analysis/blob/master/examples/example_list_compare.py))

* Feature Importance Comparison: Takes model_id(s) and calculates:
    - overall pairwise rank correlation 
    - pairwise jaccard similarity over top n
    - pairwise kl divergence over the feature importance distributions (normalized binned feature importance scores)
    - Output: 
       - metrics as dictionary 
       - TODO: heatmap (model by model overlap)
     - ([example](https://github.com/dssg/postmodel-analysis/blob/master/examples/example_feature_list_compare.py))   



* Probablity Calibration Curves: Takes a model_id and  test_time as input (need labels) and calclulates probability by each score decile.
    - ([example](https://github.com/dssg/postmodel-analysis/blob/master/examples/example_probabilty_decile.py))

* Error Analysis: decision tree analysis to analyze when the model is making errors, possibly of different types. Takes model_id, test_start_time, test_end_time as input (need labels) and build deep-ish decision trees for error analysis. Output is tree and rules

* Cross-Tab Generation: Takes model_id(s), test_time, feature(s) as input and generates cross-tabs.
* Bias Analysis: Takes model_id(s), test_time, feature(s) to calculate bias metrics on.
* evaluation report [for later]

