{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Triage Risk modeling and prediction Predictive analytics projects require the coordination of many different tasks, such as feature generation, classifier training, evaluation, and list generation. These tasks are complicated in their own right, but in addition have to be combined in different ways throughout the course of the project. Triage aims to provide interfaces to these different phases of a project, such as an Experiment . Each phase is defined by configuration specific to the needs of the project, and an arrangement of core data science components that work together to produce the output of that phase. The first phase implemented in triage is the Experiment . An experiment represents the initial research work of creating design matrices from source data, and training/testing/evaluating a model grid on those matrices. At the end of the experiment, a relational database with results metadata is populated, allowing for evaluation by the researcher. Running an Experiment See the Running an Experiment documentation. Upgrading an Experiment config v5 -> v6 v3/v4 -> v5 Background Triage is developed at the University of Chicago's Center For Data Science and Public Policy . We created it in response to commonly occuring challenges we've encountered and patterns we've developed while working on projects for our partners. Major Components Used by Triage Triage makes use of many core data science components developed at DSaPP. These components can be useful in their own right, and are worth checking out if Architect : Plan, design and build train and test matrices. Includes feature and label generation. Collate : Aggregation SQL Query Builder. This is used by the Architect to build features. Timechop : Generate temporal cross-validation time windows for matrix creation Metta-Data : Train and test matrix storage Catwalk : Training, testing, and evaluating machine learning classifier models Results Schema : Generate a database schema suitable for storing the results of modeling runs Design Goals There are two overarching design goals for Triage: All configuration necessary to run the full experiment from the external interface (ie, Experiment subclasses) from beginning to end must be easily serializable and machine-constructable, to allow the eventual development of tools for users to design experiments. All core functionality must be usable outside of a specific pipeline context or workflow manager. There are many good workflow managers; everybody has their favorite, and core functionality should not be designed to work with specific execution expectations. Future Plans Generation and Management of lists (ie for inspections) by various criteria Integration of components with various workflow managers, like Drain and Luigi . Comprehensive leakage testing of an experiment's modeling run Feature Generation Wizard","title":"Home"},{"location":"#triage","text":"Risk modeling and prediction Predictive analytics projects require the coordination of many different tasks, such as feature generation, classifier training, evaluation, and list generation. These tasks are complicated in their own right, but in addition have to be combined in different ways throughout the course of the project. Triage aims to provide interfaces to these different phases of a project, such as an Experiment . Each phase is defined by configuration specific to the needs of the project, and an arrangement of core data science components that work together to produce the output of that phase. The first phase implemented in triage is the Experiment . An experiment represents the initial research work of creating design matrices from source data, and training/testing/evaluating a model grid on those matrices. At the end of the experiment, a relational database with results metadata is populated, allowing for evaluation by the researcher.","title":"Triage"},{"location":"#running-an-experiment","text":"See the Running an Experiment documentation.","title":"Running an Experiment"},{"location":"#upgrading-an-experiment-config","text":"v5 -> v6 v3/v4 -> v5","title":"Upgrading an Experiment config"},{"location":"#background","text":"Triage is developed at the University of Chicago's Center For Data Science and Public Policy . We created it in response to commonly occuring challenges we've encountered and patterns we've developed while working on projects for our partners.","title":"Background"},{"location":"#major-components-used-by-triage","text":"Triage makes use of many core data science components developed at DSaPP. These components can be useful in their own right, and are worth checking out if Architect : Plan, design and build train and test matrices. Includes feature and label generation. Collate : Aggregation SQL Query Builder. This is used by the Architect to build features. Timechop : Generate temporal cross-validation time windows for matrix creation Metta-Data : Train and test matrix storage Catwalk : Training, testing, and evaluating machine learning classifier models Results Schema : Generate a database schema suitable for storing the results of modeling runs","title":"Major Components Used by Triage"},{"location":"#design-goals","text":"There are two overarching design goals for Triage: All configuration necessary to run the full experiment from the external interface (ie, Experiment subclasses) from beginning to end must be easily serializable and machine-constructable, to allow the eventual development of tools for users to design experiments. All core functionality must be usable outside of a specific pipeline context or workflow manager. There are many good workflow managers; everybody has their favorite, and core functionality should not be designed to work with specific execution expectations.","title":"Design Goals"},{"location":"#future-plans","text":"Generation and Management of lists (ie for inspections) by various criteria Integration of components with various workflow managers, like Drain and Luigi . Comprehensive leakage testing of an experiment's modeling run Feature Generation Wizard","title":"Future Plans"},{"location":"triage.experiments.base/","text":"Source: triage/experiments/base.py#L0 Global Variables CONFIG_VERSION dt_from_str dt_from_str(dt_str) ExperimentBase The Base class for all Experiments. ExperimentBase.all_as_of_times All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes ExperimentBase.all_label_windows All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config ExperimentBase.collate_aggregations collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects ExperimentBase.feature_dicts Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names ExperimentBase.feature_table_tasks All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands ExperimentBase.full_matrix_definitions Full matrix definitions Returns: (list) temporal and feature information for each matrix ExperimentBase.master_feature_dictionary All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names ExperimentBase.matrix_build_tasks Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts ExperimentBase.split_definitions Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] } ExperimentBase. __init__ __init__(self, config, db_engine, model_storage_class=None, project_path=None, replace=True) Initialize self. See help(type(self)) for accurate signature. ExperimentBase.build_matrices build_matrices(self) Generate labels, features, and matrices ExperimentBase.catwalk catwalk(self) Train, test, and evaluate models ExperimentBase.generate_labels generate_labels(self) Generate labels based on experiment configuration Results are stored in the database, not returned ExperimentBase.generate_sparse_states generate_sparse_states(self) ExperimentBase.initialize_components initialize_components(self) ExperimentBase.initialize_factories initialize_factories(self) ExperimentBase.log_split log_split(self, split_num, split) ExperimentBase.matrix_store matrix_store(self, matrix_uuid) Construct a matrix store for a given matrix uuid, using the Experiment's #matrix_store_class Args: matrix_uuid (string) A uuid for a matrix ExperimentBase.run run(self) ExperimentBase.update_split_definitions update_split_definitions(self, new_split_definitions) Update split definitions Args: (dict) split definitions (should have matrix uuids)","title":"Triage.experiments.base"},{"location":"triage.experiments.base/#global-variables","text":"CONFIG_VERSION","title":"Global Variables"},{"location":"triage.experiments.base/#dt_from_str","text":"dt_from_str(dt_str)","title":"dt_from_str"},{"location":"triage.experiments.base/#experimentbase","text":"The Base class for all Experiments.","title":"ExperimentBase"},{"location":"triage.experiments.base/#experimentbaseall_as_of_times","text":"All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes","title":"ExperimentBase.all_as_of_times"},{"location":"triage.experiments.base/#experimentbaseall_label_windows","text":"All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config","title":"ExperimentBase.all_label_windows"},{"location":"triage.experiments.base/#experimentbasecollate_aggregations","text":"collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects","title":"ExperimentBase.collate_aggregations"},{"location":"triage.experiments.base/#experimentbasefeature_dicts","text":"Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names","title":"ExperimentBase.feature_dicts"},{"location":"triage.experiments.base/#experimentbasefeature_table_tasks","text":"All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands","title":"ExperimentBase.feature_table_tasks"},{"location":"triage.experiments.base/#experimentbasefull_matrix_definitions","text":"Full matrix definitions Returns: (list) temporal and feature information for each matrix","title":"ExperimentBase.full_matrix_definitions"},{"location":"triage.experiments.base/#experimentbasemaster_feature_dictionary","text":"All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names","title":"ExperimentBase.master_feature_dictionary"},{"location":"triage.experiments.base/#experimentbasematrix_build_tasks","text":"Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts","title":"ExperimentBase.matrix_build_tasks"},{"location":"triage.experiments.base/#experimentbasesplit_definitions","text":"Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] }","title":"ExperimentBase.split_definitions"},{"location":"triage.experiments.base/#experimentbase__init__","text":"__init__(self, config, db_engine, model_storage_class=None, project_path=None, replace=True) Initialize self. See help(type(self)) for accurate signature.","title":"ExperimentBase.__init__"},{"location":"triage.experiments.base/#experimentbasebuild_matrices","text":"build_matrices(self) Generate labels, features, and matrices","title":"ExperimentBase.build_matrices"},{"location":"triage.experiments.base/#experimentbasecatwalk","text":"catwalk(self) Train, test, and evaluate models","title":"ExperimentBase.catwalk"},{"location":"triage.experiments.base/#experimentbasegenerate_labels","text":"generate_labels(self) Generate labels based on experiment configuration Results are stored in the database, not returned","title":"ExperimentBase.generate_labels"},{"location":"triage.experiments.base/#experimentbasegenerate_sparse_states","text":"generate_sparse_states(self)","title":"ExperimentBase.generate_sparse_states"},{"location":"triage.experiments.base/#experimentbaseinitialize_components","text":"initialize_components(self)","title":"ExperimentBase.initialize_components"},{"location":"triage.experiments.base/#experimentbaseinitialize_factories","text":"initialize_factories(self)","title":"ExperimentBase.initialize_factories"},{"location":"triage.experiments.base/#experimentbaselog_split","text":"log_split(self, split_num, split)","title":"ExperimentBase.log_split"},{"location":"triage.experiments.base/#experimentbasematrix_store","text":"matrix_store(self, matrix_uuid) Construct a matrix store for a given matrix uuid, using the Experiment's #matrix_store_class Args: matrix_uuid (string) A uuid for a matrix","title":"ExperimentBase.matrix_store"},{"location":"triage.experiments.base/#experimentbaserun","text":"run(self)","title":"ExperimentBase.run"},{"location":"triage.experiments.base/#experimentbaseupdate_split_definitions","text":"update_split_definitions(self, new_split_definitions) Update split definitions Args: (dict) split definitions (should have matrix uuids)","title":"ExperimentBase.update_split_definitions"},{"location":"triage.experiments.multicore/","text":"Source: triage/experiments/multicore.py#L0 insert_into_table insert_into_table(insert_statements, feature_generator_factory, db_connection_string) build_matrix build_matrix(build_tasks, planner_factory, db_connection_string) train_model train_model(train_tasks, trainer_factory, db_connection_string) test_and_evaluate test_and_evaluate(model_ids, predictor_factory, evaluator_factory, indiv_importance_factory, \\ test_store, db_connection_string, split_def, train_matrix_columns, config) MultiCoreExperiment The Base class for all Experiments. MultiCoreExperiment.all_as_of_times All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes MultiCoreExperiment.all_label_windows All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config MultiCoreExperiment.collate_aggregations collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects MultiCoreExperiment.feature_dicts Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names MultiCoreExperiment.feature_table_tasks All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands MultiCoreExperiment.full_matrix_definitions Full matrix definitions Returns: (list) temporal and feature information for each matrix MultiCoreExperiment.master_feature_dictionary All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names MultiCoreExperiment.matrix_build_tasks Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts MultiCoreExperiment.split_definitions Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] } MultiCoreExperiment. __init__ __init__(self, n_processes=1, n_db_processes=1, *args, **kwargs) Initialize self. See help(type(self)) for accurate signature. MultiCoreExperiment.build_matrices build_matrices(self) Generate labels, features, and matrices MultiCoreExperiment.catwalk catwalk(self) Train, test, and evaluate models MultiCoreExperiment.parallelize parallelize(self, partially_bound_function, tasks, n_processes, chunksize=1) MultiCoreExperiment.parallelize_with_success_count parallelize_with_success_count(self, partially_bound_function, tasks, n_processes, chunksize=1)","title":"Triage.experiments.multicore"},{"location":"triage.experiments.multicore/#insert_into_table","text":"insert_into_table(insert_statements, feature_generator_factory, db_connection_string)","title":"insert_into_table"},{"location":"triage.experiments.multicore/#build_matrix","text":"build_matrix(build_tasks, planner_factory, db_connection_string)","title":"build_matrix"},{"location":"triage.experiments.multicore/#train_model","text":"train_model(train_tasks, trainer_factory, db_connection_string)","title":"train_model"},{"location":"triage.experiments.multicore/#test_and_evaluate","text":"test_and_evaluate(model_ids, predictor_factory, evaluator_factory, indiv_importance_factory, \\ test_store, db_connection_string, split_def, train_matrix_columns, config)","title":"test_and_evaluate"},{"location":"triage.experiments.multicore/#multicoreexperiment","text":"The Base class for all Experiments.","title":"MultiCoreExperiment"},{"location":"triage.experiments.multicore/#multicoreexperimentall_as_of_times","text":"All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes","title":"MultiCoreExperiment.all_as_of_times"},{"location":"triage.experiments.multicore/#multicoreexperimentall_label_windows","text":"All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config","title":"MultiCoreExperiment.all_label_windows"},{"location":"triage.experiments.multicore/#multicoreexperimentcollate_aggregations","text":"collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects","title":"MultiCoreExperiment.collate_aggregations"},{"location":"triage.experiments.multicore/#multicoreexperimentfeature_dicts","text":"Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names","title":"MultiCoreExperiment.feature_dicts"},{"location":"triage.experiments.multicore/#multicoreexperimentfeature_table_tasks","text":"All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands","title":"MultiCoreExperiment.feature_table_tasks"},{"location":"triage.experiments.multicore/#multicoreexperimentfull_matrix_definitions","text":"Full matrix definitions Returns: (list) temporal and feature information for each matrix","title":"MultiCoreExperiment.full_matrix_definitions"},{"location":"triage.experiments.multicore/#multicoreexperimentmaster_feature_dictionary","text":"All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names","title":"MultiCoreExperiment.master_feature_dictionary"},{"location":"triage.experiments.multicore/#multicoreexperimentmatrix_build_tasks","text":"Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts","title":"MultiCoreExperiment.matrix_build_tasks"},{"location":"triage.experiments.multicore/#multicoreexperimentsplit_definitions","text":"Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] }","title":"MultiCoreExperiment.split_definitions"},{"location":"triage.experiments.multicore/#multicoreexperiment__init__","text":"__init__(self, n_processes=1, n_db_processes=1, *args, **kwargs) Initialize self. See help(type(self)) for accurate signature.","title":"MultiCoreExperiment.__init__"},{"location":"triage.experiments.multicore/#multicoreexperimentbuild_matrices","text":"build_matrices(self) Generate labels, features, and matrices","title":"MultiCoreExperiment.build_matrices"},{"location":"triage.experiments.multicore/#multicoreexperimentcatwalk","text":"catwalk(self) Train, test, and evaluate models","title":"MultiCoreExperiment.catwalk"},{"location":"triage.experiments.multicore/#multicoreexperimentparallelize","text":"parallelize(self, partially_bound_function, tasks, n_processes, chunksize=1)","title":"MultiCoreExperiment.parallelize"},{"location":"triage.experiments.multicore/#multicoreexperimentparallelize_with_success_count","text":"parallelize_with_success_count(self, partially_bound_function, tasks, n_processes, chunksize=1)","title":"MultiCoreExperiment.parallelize_with_success_count"},{"location":"triage.experiments.singlethreaded/","text":"Source: triage/experiments/singlethreaded.py#L0 SingleThreadedExperiment The Base class for all Experiments. SingleThreadedExperiment.all_as_of_times All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes SingleThreadedExperiment.all_label_windows All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config SingleThreadedExperiment.collate_aggregations collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects SingleThreadedExperiment.feature_dicts Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names SingleThreadedExperiment.feature_table_tasks All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands SingleThreadedExperiment.full_matrix_definitions Full matrix definitions Returns: (list) temporal and feature information for each matrix SingleThreadedExperiment.master_feature_dictionary All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names SingleThreadedExperiment.matrix_build_tasks Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts SingleThreadedExperiment.split_definitions Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] } SingleThreadedExperiment. __init__ __init__(self, config, db_engine, model_storage_class=None, project_path=None, replace=True) Initialize self. See help(type(self)) for accurate signature. SingleThreadedExperiment.build_matrices build_matrices(self) Generate labels, features, and matrices SingleThreadedExperiment.catwalk catwalk(self) Train, test, and evaluate models","title":"Triage.experiments.singlethreaded"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperiment","text":"The Base class for all Experiments.","title":"SingleThreadedExperiment"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentall_as_of_times","text":"All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes","title":"SingleThreadedExperiment.all_as_of_times"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentall_label_windows","text":"All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config","title":"SingleThreadedExperiment.all_label_windows"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentcollate_aggregations","text":"collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects","title":"SingleThreadedExperiment.collate_aggregations"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentfeature_dicts","text":"Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names","title":"SingleThreadedExperiment.feature_dicts"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentfeature_table_tasks","text":"All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands","title":"SingleThreadedExperiment.feature_table_tasks"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentfull_matrix_definitions","text":"Full matrix definitions Returns: (list) temporal and feature information for each matrix","title":"SingleThreadedExperiment.full_matrix_definitions"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentmaster_feature_dictionary","text":"All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names","title":"SingleThreadedExperiment.master_feature_dictionary"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentmatrix_build_tasks","text":"Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts","title":"SingleThreadedExperiment.matrix_build_tasks"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentsplit_definitions","text":"Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] }","title":"SingleThreadedExperiment.split_definitions"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperiment__init__","text":"__init__(self, config, db_engine, model_storage_class=None, project_path=None, replace=True) Initialize self. See help(type(self)) for accurate signature.","title":"SingleThreadedExperiment.__init__"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentbuild_matrices","text":"build_matrices(self) Generate labels, features, and matrices","title":"SingleThreadedExperiment.build_matrices"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentcatwalk","text":"catwalk(self) Train, test, and evaluate models","title":"SingleThreadedExperiment.catwalk"},{"location":"experiments/algorithm/","text":"Experiment Algorithm Deep Dive This guide's purpose is to provide familiarity of the inner workings of a Triage Experiment to people with some experience in data science and Python. A Triage Experiment is a highly structured way of defining the experimentation phase of a data science project. To those wondering whether this Experiment structure is flexible enough to fit their needs, this should help. 1. Temporal Validation Setup First, the given temporal_config section in the experiment definition is transformed into train and test splits, including as_of_times for each matrix. We create these splits by figuring out the latest reasonable split time from the inputs, and moving backwards in time at the rate of the given model_update_frequency , until we get to the earliest reasonable split time. For each split, we create as_of_times by moving either backwards from the split time towards the max_training_history (for train matrices) or forwards from the split time towards the test_duration (for test matrices) at the provided data_frequency . Many of these configured values may be lists, in which case we generate the cross-product of all the possible values and generate more splits. For a more detailed look at the temporal validation logic, see Temporal Validation Deep Dive . The train and test splits themselves are not used until the Building Matrices section, but a flat list of all computed as_of_times for all matrices needed in the experiment is used in the next section, Transforming Data . 2. Transforming Data With all of the as_of_times for this Experiment now computed, it's now possible to transform the input data into features and labels as of all the required times. Labels The Experiment populates a 'labels' table using the following input: 1. A query, provided by the user in the configuration file, that generates entity_ids and outcomes for a given as_of_date and label_timespan. Each as_of_date and label_timespan defined in temporal config For instance, an inspections-style query (for the given timespan, return the entity and outcome of any matching inspections) would look like: select events.entity_id, bool_or(outcome::bool)::integer as outcome from events where '{as_of_date}' <= outcome_date and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id This binary labels table is scoped to the entire Experiment, so all as_of_time (computed in step 1) and label_timespan (taken straight from temporal_config ) combinations are present. Additionally, the 'label_name' and 'label_type' are also recorded with each row in the table. At this point, the 'labels' table may not have entries for all entities and dates that need to be in a given matrix. How these rows have their labels represented is up to the configured include_missing_labels_in_train_as value in the experiment. This value is not processed when we generate the labels table, but later on when the matrix is built (see 'Retrieving Data and Saving Completed Matrix') State Table The Experiment keeps track of the state of the entities. Based on the configuration, certain entities can be included or excluded for different time periods in feature imputation, creating matrices, or both. In code, it does this by computing what it calls the 'sparse' state table for an experiment. This is a table with a boolean flag entry for every entity, as_of_time, and state. The structure of this table allows for state filtering based on SQL conditions given by the user. Based on configuration, it can be created through one of three code paths: If the user passes what we call a 'dense states' table, with the following structure: entity id/state/start/end, and a list of state filters, this 'dense states' table holds time ranges that entities were in for specific states. When converting this to a sparse table, we take each as_of_time present in the Experiment, and for each known state (that is, the distinct values found in the 'dense states' table), see if there is any entry in the dense states table with this state whose range overlaps with the as_of_time. If so, the entity is considered to be in that state as of that date. If the user passes what we call an 'entities' table, containing an entity_id, it will simply use all distinct entities present in said table, and mark them as 'active' for every as_of_time in the experiment. Any other columns are ignored. If the user passes a query, parameterized with an as of date, we will populate the table by running it for each as_of_date. This table is created and exists until matrices are built, at which point it is considered unnecessary and then dropped. Features Each provided feature_aggregation configures the creation and population of several feature tables in the 'features' schema: one for each of the groups specified in the config, one that merges the groups together into one table, and one that fills in null values from the merged table with imputed values based on imputation config. Generating Aggregation SQL To generate the SQL that creates the pre-imputation table, the Experiment assembles building blocks from the feature aggregation config, as well as the experiment's list of as_of_times : from_obj represents, well, the object of the FROM clause in the SQL query. Often this is just a table, but can be configured to be a subquery. This holds all the data that we want to aggregate into features Each as_of_time in the experiment and interval in the feature_aggregation is combined with the knowledge_date_column to create a WHERE clause representing a valid window of events to aggregate in the from_obj : e.g ( where {knowledge_date_column} >= {as_of_time} - interval {interval} ) Each aggregate , categorical , or array_categorical represents a SELECT clause. For aggregates, the quantity is a column or SQL expression representing a numeric quantity present in the from_obj , and the metrics are any number of aggregate functions we want to use. The aggregate function is applied to the quantity. Each group is a column applied to the GROUP BY clause. Generally this is 'entity_id', but higher-level groupings (for instance, 'zip_code') can be used as long as they can be rolled up to 'entity_id'. So a simplified version of a typical query would look like: SELECT {group}, {metric}({quantity}) FROM {from_obj} WHERE {knowledge_date_column} >= {as_of_time} - interval {interval} GROUP BY {group} Writing Group-wide Feature Tables For each as_of_time , the results from the generated query are written to a table whose name is prefixed with the prefix , and suffixed with the group . For instance, if the configuration specifies zipcode-level aggregates and entity-level aggregates, there will be a table for each, keyed on its group plus the as_of_date. Merging into Aggregation-wide Feature Tables Each generated group table is combined into one representing the whole aggregation with a left join. Given that the groups originally came from the same table (the from_obj of the aggregation) and therefore we know the zipcode for each entity, what we do now is create a table that would be keyed on entity and as_of_date, and contain all entity-level and zipcode-level aggregates from both tables. This aggregation-level table represents all of the features in the aggregation, pre-imputation. Its output location is generally {prefix}_aggregation Imputing Values A table that looks similar, but with imputed values is created. The state table from above is passed into collate as the comprehensive set of entities and dates for which output should be generated, regardless if they exist in the from_obj . Each feature column has an imputation rule, inherited from some level of the feature definition. The imputation rules that are based on data (e.g. mean ) use the rows from the as_of_time to produce the imputed value. Its output location is generally {prefix}_aggregation_imputed Recap At this point, we have at least three tables that are used to populate matrices: labels with computed labels tmp_states_{experiment hash} that tracks what as_of_times each entity was in each state. A features.{prefix}_aggregation_imputed table for each feature aggregation present in the experiment config. 3. Building Matrices At this point, we have to build actual train and test matrices that can be processed by machine learning algorithms, save at the user's specified path, either on the local filesystem or s3 depending on the scheme portion of the path (e.g. s3://bucket-name/project_directory ) First we have to figure out exactly what matrices we have to build. The split definitions from step 1 are a good start -- they are our train and test splits -- but sometimes we also want to test different subsets of the data, like feature groups (e.g. 'how does using group of features A perform against using all features?'). So there's a layer of iteration we introduce for each split, that may produce many more matrices. What do we iterate over? Feature List - All subsets of features that the user wants to cycle through. This is the end result of the feature group generation and mixing process, which is described more below. States - All configured state_filters in the experiment config. These take the form of boolean SQL clauses that are applied to the sparse states table, and the purpose of this is to test different cohorts against each other. Generally there is just one here. Label names - In theory we can take in different labels (e.g. complaints, sustained complaints) in the same experiment. Right now there is no support for multiple label names, but the label name used is configurable through the optional 'label_config'->'name' config value Label types - In theory we can take in different label types (e.g. binary) in the same experiment. Right now this isn't done, there is one label type and it is hardcoded as 'binary'. Feature Lists How do we arrive at the feature lists? There are two pieces of config that are used: feature group_definition and feature_group_strategies . Feature group definitions are just ways to define logical blocks of features, most often features that come from the same source, or describing a particular type of event. These groups within the experiment as a list of feature names, representing some subset of all potential features for the experiment. Feature group strategies are ways to take feature groups and mix them together in various ways. The feature group strategies take these subsets of features and convert them into another list of subsets of features, which is the final list iterated over to create different matrices. Feature Group Definition Feature groups, at present, can be defined as either a prefix (the prefix of the feature name), a table (the feature table that the feature resides in), or all (all features). Each argument is passed as a list, and each entry in the list is interpreted as a group. So, a feature group config of {'table': ['complaints_aggregate_imputed', 'incidents_aggregate_imputed']} would result in two feature groups: one with all the features in complaints_aggregate_imputed , and one with all the features in incidents_aggregate_imputed . Note that this requires a bit of knowledge on the user's part of how the feature table names will be constructed. prefix works on the prefix of the feature name as it exists in the database. So this also requires some knowledge of how these get created. The general format is: {aggregation_prefix}_{group}_{timeperiod}_{quantity} , so with some knowledge the user can create groups with the aggregation's configured prefix (common), or the aggregations configured prefix + group (in case they want to compare, for instance, zip-code level features versus entity level features). all , with a single value of True , will include a feature group with all defined features. If no feature group definition is sent, this is the default. Either way, at the end of this process the experiment will be aware of some list of feature groups, even if the list is just length 1 with all features as one group. Feature Group Mixing A few basic feature group mixing strategies are implemented: leave-one-in , leave-one-out , and all . These are sent in the experiment definition as a list, so different strategies can be tried in the same experiment. Each included strategy will be applied to the list of feature groups from the previous step, to convert them into For instance, 'leave-one-in' will cycle through each feature group, and for each one create a list of features that just represents that feature group, so for some matrices we would only use features from that particular group. leave-one-out does the opposite, for each feature group creating a list of features that includes all other feature groups but that one. all just creates a list of features that represents all feature groups together. Iteration and Matrix Creation At this point, matrices are created by looping through all train/test splits and data subsets (e.g. feature groups, state definitions), grabbing the data corresponding to each from the database, and assembling that data into a design matrix that is saved along with the metadata that defines it. As an example, if the experiment defines 3 train/test splits (one test per train in this example, for simplicity), 3 feature groups that are mixed using the 'leave-one-out' and 'all' strategies, and 1 state definition, we'll expect 18 matrices to be saved: 9 splits after multiplying the time splits by the feature groups, and each one creating a train and test matrix. Associating Matrices with Experiment After all matrices for the Experiment are defined but before any are built, the Experiment is associated with each Matrix in the database through the model_metadata.experiment_matrices table. This means that whether or not the Experiment has to end up building a matrix, after the fact a user can query the database to see if it used said matrix. Retrieving Data and Saving Completed Matrix Each matrix that has to be built (i.e. has not been built by some prior experiment) is built by retrieving its data out of the database. How do we get the data for an individual matrix out of the database? Create an entity-date table for this specific matrix. There is some logic applied to decide what rows show up. There are two possible sets of rows that could show up. all valid entity dates . These dates come from the entity-date-state table for the experiment (populated using the rules defined in the 'cohort_config'), filtered down to the entity-date pairs that match both the state filter and the list of as-of-dates for this matrix . all labeled entity dates . These dates consist of all the valid entity dates from above, that also have an entry in the labels table. If the matrix is a test matrix, all valid entity dates will be present. If the matrix is a train matrix, whether or not valid but unlabeled examples show up is decided by the include_missing_labels_in_train_as configuration value. If it is present in any form, these labels will be in the matrix. Otherwise, they will be filtered out. Write features data from tables to disk in CSV format using a COPY command, table by table. Each table is joined with the matrix-specific entity-date table to only include the desired rows. Write labels data to disk in CSV format using a COPY command. These labels will consist of the rows in the matrix-specific entity-date table left joined to the labels table. Rows not present in the labels table will have their label filled in (either True or False) based on the value of the include_missing_labels_in_train_as configuration key. Merge the features and labels CSV files horizontally, in pandas. They are expected to be of the same shape, which is enforced by the entity-date table. The resulting matrix is indexed on entity_id and as_of_date , and then saved to disk (in CSV format, more formats to come) along with its metadata: time, feature, label, index, and state information. along with any user metadata the experiment config specified. The filename is decided by a hash of this metadata, and the metadata is saved in a YAML file with the same hash and directory. The metadata is additionally added to a database table 'matrices'. Matrix metadata reference: - Train matrix temporal info - Test matrix temporal info - Feature, label, index, cohort, user metadata Recap At this point, all finished matrices and metadata will be saved under the project_path supplied by the user to the Experiment constructor, in the subdirectory matrices . 4. Running Models The last phase of an Experiment run uses the completed design matrices to train, test, and evaluate classifiers. This procedure writes a lot of metadata to the 3 schemas: 'model_metadata', 'train_results', and 'test_results'. Associating Models with Experiment Every combination of training matrix + classifier + hyperparameter is considered a Model. Before any Models are trained, the Experiment is associated with each Model in the database through the model_metadata.experiment_models table. This means that whether or not the Experiment has to end up training a model, after the fact a user can query the database to see if it used said model. Train Each matrix marked for training is sent through the configured grid in the experiment's grid_config . This works much like the scikit-learn ParameterGrid (and in fact uses it on the backend). It cycles through all of the classifiers and hyperparameter combinations contained herein, and calls .fit() with that train matrix. Any classifier that adheres to the scikit-learn .fit/.transform interface and is available in the Python environment will work here, whether it is a standard scikit-learn classifier, a third-party library like XGBoost, or a custom-built one in the calling repository (for instance, one that implements the problem domain's baseline heuristic algorithm for comparison). Metadata about the trained classifier is written to the model_metadata.models Postgres table. The trained model is saved to a filename with the model hash (see Model Hash section below). Model Groups Each model is assigned a 'model group'. A model group represents a number of trained classifiers that we want to treat as equivalent by some criteria. By default, this is aimed at defining models which are equivalent across time splits, to make analyzing model stability easier. This default is accomplished with a set of 'model group keys' that includes data about the classifier (module, hyperparameters), temporal intervals used to create the train matrix (label timespan, training history, as-of-date frequency), and metadata describing the data in the train matrix (features and feature groups, label name, cohort name). The user can override this set of model_group_keys in the experiment definition, with all of the default information plus other matrix metadata at their disposal (See end of 'Retrieving Data and Saving Completed Matrix' section for more about matrix metadata). This data is stored in the model_metadata.model_groups table, along with a model_group_id that is used as a foreign key in the model_metadata.models table. Model Hash Each trained model is assigned a hash, for the purpose of uniquely defining and caching the model. This hash is based on the training matrix metadata, classifier path, hyperparameters (except those which concern execution and do not affect results of the classifier, such as n_jobs ), and the given project path for the Experiment. This hash can be found in each row of the model_metadata.models table. It is enforced as a unique key in the table. Global Feature Importance The training phase also writes global feature importances to the database, in the train_results.feature_importances table. A few methods are queried to attempt to compute feature importances: The bulk of these are computed using the trained model's .feature_importances_ attribute, if it exists. For sklearn's SVC models with a linear kernel, the model's .coef_.squeeze() is used. For sklearn's LogisticRegression models, np.exp(model.coef_).squeeze() is used. Otherwise, no feature importances are written. Test Matrix For each test matrix, predictions, individual importances, and the user-specified testing evaluation metrics are written to the 'test_results' schema. For each train matrix, predictions and the user-specified training evaluation metrics are written to the 'train_results' schema. Predictions The trained model's prediction probabilities ( predict_proba() ) are computed both for the matrix it was trained on and any testing matrices. The predictions for the training matrix are saved in train_results.predictions and those for the testing matrices are saved in the test_results.predictions . More specifically, predict_proba returns the probabilities for each label (false and true), but in this case only the probabilities for the true label are saved in the {train or test}_predictions table. The entity_id and as_of_date are retrieved from the matrix's index, and stored in the database table along with the probability score, label value (if it has one), as well as other metadata. Individual Feature Importance Feature importances (of a configurable number of top features, defaulting to 5) for each prediction are computed and written to the test_results.individual_importances table. Right now, there are no sophisticated calculation methods integrated into the experiment; simply the top 5 global feature importances for the model are copied to the individual_importances table. Metrics Triage allows for the computation of both testing set and training set evaluation metrics. Evaluation metrics, such as precision and recall at various thresholds, are written to either the train_results.evaluations table or the test_results.evaluations . Triage defines a number of Evaluation Metrics metrics that can be addressed by name in the experiment definition, along with a list of thresholds and/or other parameters (such as the 'beta' value for fbeta) to iterate through. Thresholding is done either via absolute value (top k) or percentile by sorting the predictions and labels by the row's predicted probability score, with ties broken at random (the random seed can be passed in the config file to make this deterministic), and assigning the predicted value as True for those above the threshold. Note that the percentile thresholds are in terms of the population percentage, not a cutoff threshold for the predicted probability. Sometimes test matrices may not have labels for every row, so it's worth mentioning here how that is handled and interacts with thresholding. Rows with missing labels are not considered in the metric calculations, and if some of these rows are in the top k of the test matrix, no more rows are taken from the rest of the list for consideration. So if the experiment is calculating precision at the top 100 rows, and 40 of the top 100 rows are missing a label, the precision will actually be calculated on the 60 of the top 100 rows that do have a label. To make the results of this more transparent for users, a few extra pieces of metadata are written to the evaluations table for each metric score. num_labeled_examples - The number of rows in the test matrix that have labels num_labeled_above_threshold - The number of rows above the configured threshold for this metric score that have labels num_positive_labels - The number of positive labels in the test matrix Recap At this point, the 'model_metadata', 'train_results', and 'test_results' database schemas are fully populated with data about models, model groups, predictions, feature importances, and evaluation metrics for the researcher to query. In addition, the trained model pickle files are saved in the configured project path. The experiment is considered finished.","title":"Experiment Algorithm"},{"location":"experiments/algorithm/#experiment-algorithm-deep-dive","text":"This guide's purpose is to provide familiarity of the inner workings of a Triage Experiment to people with some experience in data science and Python. A Triage Experiment is a highly structured way of defining the experimentation phase of a data science project. To those wondering whether this Experiment structure is flexible enough to fit their needs, this should help.","title":"Experiment Algorithm Deep Dive"},{"location":"experiments/algorithm/#1-temporal-validation-setup","text":"First, the given temporal_config section in the experiment definition is transformed into train and test splits, including as_of_times for each matrix. We create these splits by figuring out the latest reasonable split time from the inputs, and moving backwards in time at the rate of the given model_update_frequency , until we get to the earliest reasonable split time. For each split, we create as_of_times by moving either backwards from the split time towards the max_training_history (for train matrices) or forwards from the split time towards the test_duration (for test matrices) at the provided data_frequency . Many of these configured values may be lists, in which case we generate the cross-product of all the possible values and generate more splits. For a more detailed look at the temporal validation logic, see Temporal Validation Deep Dive . The train and test splits themselves are not used until the Building Matrices section, but a flat list of all computed as_of_times for all matrices needed in the experiment is used in the next section, Transforming Data .","title":"1. Temporal Validation Setup"},{"location":"experiments/algorithm/#2-transforming-data","text":"With all of the as_of_times for this Experiment now computed, it's now possible to transform the input data into features and labels as of all the required times.","title":"2. Transforming Data"},{"location":"experiments/algorithm/#labels","text":"The Experiment populates a 'labels' table using the following input: 1. A query, provided by the user in the configuration file, that generates entity_ids and outcomes for a given as_of_date and label_timespan. Each as_of_date and label_timespan defined in temporal config For instance, an inspections-style query (for the given timespan, return the entity and outcome of any matching inspections) would look like: select events.entity_id, bool_or(outcome::bool)::integer as outcome from events where '{as_of_date}' <= outcome_date and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id This binary labels table is scoped to the entire Experiment, so all as_of_time (computed in step 1) and label_timespan (taken straight from temporal_config ) combinations are present. Additionally, the 'label_name' and 'label_type' are also recorded with each row in the table. At this point, the 'labels' table may not have entries for all entities and dates that need to be in a given matrix. How these rows have their labels represented is up to the configured include_missing_labels_in_train_as value in the experiment. This value is not processed when we generate the labels table, but later on when the matrix is built (see 'Retrieving Data and Saving Completed Matrix')","title":"Labels"},{"location":"experiments/algorithm/#state-table","text":"The Experiment keeps track of the state of the entities. Based on the configuration, certain entities can be included or excluded for different time periods in feature imputation, creating matrices, or both. In code, it does this by computing what it calls the 'sparse' state table for an experiment. This is a table with a boolean flag entry for every entity, as_of_time, and state. The structure of this table allows for state filtering based on SQL conditions given by the user. Based on configuration, it can be created through one of three code paths: If the user passes what we call a 'dense states' table, with the following structure: entity id/state/start/end, and a list of state filters, this 'dense states' table holds time ranges that entities were in for specific states. When converting this to a sparse table, we take each as_of_time present in the Experiment, and for each known state (that is, the distinct values found in the 'dense states' table), see if there is any entry in the dense states table with this state whose range overlaps with the as_of_time. If so, the entity is considered to be in that state as of that date. If the user passes what we call an 'entities' table, containing an entity_id, it will simply use all distinct entities present in said table, and mark them as 'active' for every as_of_time in the experiment. Any other columns are ignored. If the user passes a query, parameterized with an as of date, we will populate the table by running it for each as_of_date. This table is created and exists until matrices are built, at which point it is considered unnecessary and then dropped.","title":"State Table"},{"location":"experiments/algorithm/#features","text":"Each provided feature_aggregation configures the creation and population of several feature tables in the 'features' schema: one for each of the groups specified in the config, one that merges the groups together into one table, and one that fills in null values from the merged table with imputed values based on imputation config.","title":"Features"},{"location":"experiments/algorithm/#generating-aggregation-sql","text":"To generate the SQL that creates the pre-imputation table, the Experiment assembles building blocks from the feature aggregation config, as well as the experiment's list of as_of_times : from_obj represents, well, the object of the FROM clause in the SQL query. Often this is just a table, but can be configured to be a subquery. This holds all the data that we want to aggregate into features Each as_of_time in the experiment and interval in the feature_aggregation is combined with the knowledge_date_column to create a WHERE clause representing a valid window of events to aggregate in the from_obj : e.g ( where {knowledge_date_column} >= {as_of_time} - interval {interval} ) Each aggregate , categorical , or array_categorical represents a SELECT clause. For aggregates, the quantity is a column or SQL expression representing a numeric quantity present in the from_obj , and the metrics are any number of aggregate functions we want to use. The aggregate function is applied to the quantity. Each group is a column applied to the GROUP BY clause. Generally this is 'entity_id', but higher-level groupings (for instance, 'zip_code') can be used as long as they can be rolled up to 'entity_id'. So a simplified version of a typical query would look like: SELECT {group}, {metric}({quantity}) FROM {from_obj} WHERE {knowledge_date_column} >= {as_of_time} - interval {interval} GROUP BY {group}","title":"Generating Aggregation SQL"},{"location":"experiments/algorithm/#writing-group-wide-feature-tables","text":"For each as_of_time , the results from the generated query are written to a table whose name is prefixed with the prefix , and suffixed with the group . For instance, if the configuration specifies zipcode-level aggregates and entity-level aggregates, there will be a table for each, keyed on its group plus the as_of_date.","title":"Writing Group-wide Feature Tables"},{"location":"experiments/algorithm/#merging-into-aggregation-wide-feature-tables","text":"Each generated group table is combined into one representing the whole aggregation with a left join. Given that the groups originally came from the same table (the from_obj of the aggregation) and therefore we know the zipcode for each entity, what we do now is create a table that would be keyed on entity and as_of_date, and contain all entity-level and zipcode-level aggregates from both tables. This aggregation-level table represents all of the features in the aggregation, pre-imputation. Its output location is generally {prefix}_aggregation","title":"Merging into Aggregation-wide Feature Tables"},{"location":"experiments/algorithm/#imputing-values","text":"A table that looks similar, but with imputed values is created. The state table from above is passed into collate as the comprehensive set of entities and dates for which output should be generated, regardless if they exist in the from_obj . Each feature column has an imputation rule, inherited from some level of the feature definition. The imputation rules that are based on data (e.g. mean ) use the rows from the as_of_time to produce the imputed value. Its output location is generally {prefix}_aggregation_imputed","title":"Imputing Values"},{"location":"experiments/algorithm/#recap","text":"At this point, we have at least three tables that are used to populate matrices: labels with computed labels tmp_states_{experiment hash} that tracks what as_of_times each entity was in each state. A features.{prefix}_aggregation_imputed table for each feature aggregation present in the experiment config.","title":"Recap"},{"location":"experiments/algorithm/#3-building-matrices","text":"At this point, we have to build actual train and test matrices that can be processed by machine learning algorithms, save at the user's specified path, either on the local filesystem or s3 depending on the scheme portion of the path (e.g. s3://bucket-name/project_directory ) First we have to figure out exactly what matrices we have to build. The split definitions from step 1 are a good start -- they are our train and test splits -- but sometimes we also want to test different subsets of the data, like feature groups (e.g. 'how does using group of features A perform against using all features?'). So there's a layer of iteration we introduce for each split, that may produce many more matrices. What do we iterate over? Feature List - All subsets of features that the user wants to cycle through. This is the end result of the feature group generation and mixing process, which is described more below. States - All configured state_filters in the experiment config. These take the form of boolean SQL clauses that are applied to the sparse states table, and the purpose of this is to test different cohorts against each other. Generally there is just one here. Label names - In theory we can take in different labels (e.g. complaints, sustained complaints) in the same experiment. Right now there is no support for multiple label names, but the label name used is configurable through the optional 'label_config'->'name' config value Label types - In theory we can take in different label types (e.g. binary) in the same experiment. Right now this isn't done, there is one label type and it is hardcoded as 'binary'.","title":"3. Building Matrices"},{"location":"experiments/algorithm/#feature-lists","text":"How do we arrive at the feature lists? There are two pieces of config that are used: feature group_definition and feature_group_strategies . Feature group definitions are just ways to define logical blocks of features, most often features that come from the same source, or describing a particular type of event. These groups within the experiment as a list of feature names, representing some subset of all potential features for the experiment. Feature group strategies are ways to take feature groups and mix them together in various ways. The feature group strategies take these subsets of features and convert them into another list of subsets of features, which is the final list iterated over to create different matrices.","title":"Feature Lists"},{"location":"experiments/algorithm/#feature-group-definition","text":"Feature groups, at present, can be defined as either a prefix (the prefix of the feature name), a table (the feature table that the feature resides in), or all (all features). Each argument is passed as a list, and each entry in the list is interpreted as a group. So, a feature group config of {'table': ['complaints_aggregate_imputed', 'incidents_aggregate_imputed']} would result in two feature groups: one with all the features in complaints_aggregate_imputed , and one with all the features in incidents_aggregate_imputed . Note that this requires a bit of knowledge on the user's part of how the feature table names will be constructed. prefix works on the prefix of the feature name as it exists in the database. So this also requires some knowledge of how these get created. The general format is: {aggregation_prefix}_{group}_{timeperiod}_{quantity} , so with some knowledge the user can create groups with the aggregation's configured prefix (common), or the aggregations configured prefix + group (in case they want to compare, for instance, zip-code level features versus entity level features). all , with a single value of True , will include a feature group with all defined features. If no feature group definition is sent, this is the default. Either way, at the end of this process the experiment will be aware of some list of feature groups, even if the list is just length 1 with all features as one group.","title":"Feature Group Definition"},{"location":"experiments/algorithm/#feature-group-mixing","text":"A few basic feature group mixing strategies are implemented: leave-one-in , leave-one-out , and all . These are sent in the experiment definition as a list, so different strategies can be tried in the same experiment. Each included strategy will be applied to the list of feature groups from the previous step, to convert them into For instance, 'leave-one-in' will cycle through each feature group, and for each one create a list of features that just represents that feature group, so for some matrices we would only use features from that particular group. leave-one-out does the opposite, for each feature group creating a list of features that includes all other feature groups but that one. all just creates a list of features that represents all feature groups together.","title":"Feature Group Mixing"},{"location":"experiments/algorithm/#iteration-and-matrix-creation","text":"At this point, matrices are created by looping through all train/test splits and data subsets (e.g. feature groups, state definitions), grabbing the data corresponding to each from the database, and assembling that data into a design matrix that is saved along with the metadata that defines it. As an example, if the experiment defines 3 train/test splits (one test per train in this example, for simplicity), 3 feature groups that are mixed using the 'leave-one-out' and 'all' strategies, and 1 state definition, we'll expect 18 matrices to be saved: 9 splits after multiplying the time splits by the feature groups, and each one creating a train and test matrix.","title":"Iteration and Matrix Creation"},{"location":"experiments/algorithm/#associating-matrices-with-experiment","text":"After all matrices for the Experiment are defined but before any are built, the Experiment is associated with each Matrix in the database through the model_metadata.experiment_matrices table. This means that whether or not the Experiment has to end up building a matrix, after the fact a user can query the database to see if it used said matrix.","title":"Associating Matrices with Experiment"},{"location":"experiments/algorithm/#retrieving-data-and-saving-completed-matrix","text":"Each matrix that has to be built (i.e. has not been built by some prior experiment) is built by retrieving its data out of the database. How do we get the data for an individual matrix out of the database? Create an entity-date table for this specific matrix. There is some logic applied to decide what rows show up. There are two possible sets of rows that could show up. all valid entity dates . These dates come from the entity-date-state table for the experiment (populated using the rules defined in the 'cohort_config'), filtered down to the entity-date pairs that match both the state filter and the list of as-of-dates for this matrix . all labeled entity dates . These dates consist of all the valid entity dates from above, that also have an entry in the labels table. If the matrix is a test matrix, all valid entity dates will be present. If the matrix is a train matrix, whether or not valid but unlabeled examples show up is decided by the include_missing_labels_in_train_as configuration value. If it is present in any form, these labels will be in the matrix. Otherwise, they will be filtered out. Write features data from tables to disk in CSV format using a COPY command, table by table. Each table is joined with the matrix-specific entity-date table to only include the desired rows. Write labels data to disk in CSV format using a COPY command. These labels will consist of the rows in the matrix-specific entity-date table left joined to the labels table. Rows not present in the labels table will have their label filled in (either True or False) based on the value of the include_missing_labels_in_train_as configuration key. Merge the features and labels CSV files horizontally, in pandas. They are expected to be of the same shape, which is enforced by the entity-date table. The resulting matrix is indexed on entity_id and as_of_date , and then saved to disk (in CSV format, more formats to come) along with its metadata: time, feature, label, index, and state information. along with any user metadata the experiment config specified. The filename is decided by a hash of this metadata, and the metadata is saved in a YAML file with the same hash and directory. The metadata is additionally added to a database table 'matrices'. Matrix metadata reference: - Train matrix temporal info - Test matrix temporal info - Feature, label, index, cohort, user metadata","title":"Retrieving Data and Saving Completed Matrix"},{"location":"experiments/algorithm/#recap_1","text":"At this point, all finished matrices and metadata will be saved under the project_path supplied by the user to the Experiment constructor, in the subdirectory matrices .","title":"Recap"},{"location":"experiments/algorithm/#4-running-models","text":"The last phase of an Experiment run uses the completed design matrices to train, test, and evaluate classifiers. This procedure writes a lot of metadata to the 3 schemas: 'model_metadata', 'train_results', and 'test_results'.","title":"4. Running Models"},{"location":"experiments/algorithm/#associating-models-with-experiment","text":"Every combination of training matrix + classifier + hyperparameter is considered a Model. Before any Models are trained, the Experiment is associated with each Model in the database through the model_metadata.experiment_models table. This means that whether or not the Experiment has to end up training a model, after the fact a user can query the database to see if it used said model.","title":"Associating Models with Experiment"},{"location":"experiments/algorithm/#train","text":"Each matrix marked for training is sent through the configured grid in the experiment's grid_config . This works much like the scikit-learn ParameterGrid (and in fact uses it on the backend). It cycles through all of the classifiers and hyperparameter combinations contained herein, and calls .fit() with that train matrix. Any classifier that adheres to the scikit-learn .fit/.transform interface and is available in the Python environment will work here, whether it is a standard scikit-learn classifier, a third-party library like XGBoost, or a custom-built one in the calling repository (for instance, one that implements the problem domain's baseline heuristic algorithm for comparison). Metadata about the trained classifier is written to the model_metadata.models Postgres table. The trained model is saved to a filename with the model hash (see Model Hash section below).","title":"Train"},{"location":"experiments/algorithm/#model-groups","text":"Each model is assigned a 'model group'. A model group represents a number of trained classifiers that we want to treat as equivalent by some criteria. By default, this is aimed at defining models which are equivalent across time splits, to make analyzing model stability easier. This default is accomplished with a set of 'model group keys' that includes data about the classifier (module, hyperparameters), temporal intervals used to create the train matrix (label timespan, training history, as-of-date frequency), and metadata describing the data in the train matrix (features and feature groups, label name, cohort name). The user can override this set of model_group_keys in the experiment definition, with all of the default information plus other matrix metadata at their disposal (See end of 'Retrieving Data and Saving Completed Matrix' section for more about matrix metadata). This data is stored in the model_metadata.model_groups table, along with a model_group_id that is used as a foreign key in the model_metadata.models table.","title":"Model Groups"},{"location":"experiments/algorithm/#model-hash","text":"Each trained model is assigned a hash, for the purpose of uniquely defining and caching the model. This hash is based on the training matrix metadata, classifier path, hyperparameters (except those which concern execution and do not affect results of the classifier, such as n_jobs ), and the given project path for the Experiment. This hash can be found in each row of the model_metadata.models table. It is enforced as a unique key in the table.","title":"Model Hash"},{"location":"experiments/algorithm/#global-feature-importance","text":"The training phase also writes global feature importances to the database, in the train_results.feature_importances table. A few methods are queried to attempt to compute feature importances: The bulk of these are computed using the trained model's .feature_importances_ attribute, if it exists. For sklearn's SVC models with a linear kernel, the model's .coef_.squeeze() is used. For sklearn's LogisticRegression models, np.exp(model.coef_).squeeze() is used. Otherwise, no feature importances are written.","title":"Global Feature Importance"},{"location":"experiments/algorithm/#test-matrix","text":"For each test matrix, predictions, individual importances, and the user-specified testing evaluation metrics are written to the 'test_results' schema. For each train matrix, predictions and the user-specified training evaluation metrics are written to the 'train_results' schema.","title":"Test Matrix"},{"location":"experiments/algorithm/#predictions","text":"The trained model's prediction probabilities ( predict_proba() ) are computed both for the matrix it was trained on and any testing matrices. The predictions for the training matrix are saved in train_results.predictions and those for the testing matrices are saved in the test_results.predictions . More specifically, predict_proba returns the probabilities for each label (false and true), but in this case only the probabilities for the true label are saved in the {train or test}_predictions table. The entity_id and as_of_date are retrieved from the matrix's index, and stored in the database table along with the probability score, label value (if it has one), as well as other metadata.","title":"Predictions"},{"location":"experiments/algorithm/#individual-feature-importance","text":"Feature importances (of a configurable number of top features, defaulting to 5) for each prediction are computed and written to the test_results.individual_importances table. Right now, there are no sophisticated calculation methods integrated into the experiment; simply the top 5 global feature importances for the model are copied to the individual_importances table.","title":"Individual Feature Importance"},{"location":"experiments/algorithm/#metrics","text":"Triage allows for the computation of both testing set and training set evaluation metrics. Evaluation metrics, such as precision and recall at various thresholds, are written to either the train_results.evaluations table or the test_results.evaluations . Triage defines a number of Evaluation Metrics metrics that can be addressed by name in the experiment definition, along with a list of thresholds and/or other parameters (such as the 'beta' value for fbeta) to iterate through. Thresholding is done either via absolute value (top k) or percentile by sorting the predictions and labels by the row's predicted probability score, with ties broken at random (the random seed can be passed in the config file to make this deterministic), and assigning the predicted value as True for those above the threshold. Note that the percentile thresholds are in terms of the population percentage, not a cutoff threshold for the predicted probability. Sometimes test matrices may not have labels for every row, so it's worth mentioning here how that is handled and interacts with thresholding. Rows with missing labels are not considered in the metric calculations, and if some of these rows are in the top k of the test matrix, no more rows are taken from the rest of the list for consideration. So if the experiment is calculating precision at the top 100 rows, and 40 of the top 100 rows are missing a label, the precision will actually be calculated on the 60 of the top 100 rows that do have a label. To make the results of this more transparent for users, a few extra pieces of metadata are written to the evaluations table for each metric score. num_labeled_examples - The number of rows in the test matrix that have labels num_labeled_above_threshold - The number of rows above the configured threshold for this metric score that have labels num_positive_labels - The number of positive labels in the test matrix","title":"Metrics"},{"location":"experiments/algorithm/#recap_2","text":"At this point, the 'model_metadata', 'train_results', and 'test_results' database schemas are fully populated with data about models, model groups, predictions, feature importances, and evaluation metrics for the researcher to query. In addition, the trained model pickle files are saved in the configured project path. The experiment is considered finished.","title":"Recap"},{"location":"experiments/architecture/","text":"Experiment Architecture This doc is coming soon","title":"Experiment Architecture"},{"location":"experiments/architecture/#experiment-architecture","text":"This doc is coming soon","title":"Experiment Architecture"},{"location":"experiments/defining/","text":"Defining an Experiment This doc is coming soon. In the meantime, check out: Example Experiment Definition for an overview of the different sections of an experiment definition. Dirty Duck for a beginning-to-end walkthrough to Triage, include deep dives on creating experiment configuration.","title":"Defining an Experiment"},{"location":"experiments/defining/#defining-an-experiment","text":"This doc is coming soon. In the meantime, check out: Example Experiment Definition for an overview of the different sections of an experiment definition. Dirty Duck for a beginning-to-end walkthrough to Triage, include deep dives on creating experiment configuration.","title":"Defining an Experiment"},{"location":"experiments/feature-testing/","text":"Testing a Feature Aggregation Developing features for Triage experiments can be a daunting task. There are a lot of things to configure, a small amount of configuration can result in a ton of SQL, and it can take a long time to validate your feature configuration in the context of an Experiment being run on real data. To speed up the process of iterating on features, you can run a list of feature aggregations, without imputation, on just one as-of-date. This functionality can be accessed through the triage command line tool or called directly from code (say, in a Jupyter notebook) using the FeatureGenerator component. Using Triage CLI The command-line interface for testing features takes in two arguments: - A feature config file. Refer to example_feature_config.yaml . Essentially this is the content of the example_experiment_config.yaml 's feature_aggregations section. It consists of a YAML list, with one or more feature_aggregation rows present. - An as-of-date. This should be in the format 2016-01-01 . Example: triage experiment featuretest example_feature_config.yaml 2016-01-01 All given feature aggregations will be processed for the given date. You will see a bunch of queries pass by in your terminal, populating tables in the features_test schema which you can inspect afterwards. Using Python Code If you'd like to call this from a notebook or from any other Python code, the arguments look similar but are a bit different. You have to supply your own sqlalchemy database engine to create a 'FeatureGenerator' object, and then call the create_features_before_imputation method with your feature config as a list of dictionaries, along with an as-of-date as a string. Make sure your logging level is set to INFO if you want to see all of the queries. from triage.component.architect.feature_generators import FeatureGenerator from triage.util.db import create_engine import logging import yaml logging.basicConfig(level=logging.INFO) # create a db_engine db_url = 'your db url here' db_engine = create_engine(db_url) feature_config = [{ 'prefix': 'aprefix', 'aggregates': [ { 'quantity': 'quantity_one', 'metrics': ['sum', 'count'], ], 'categoricals': [ { 'column': 'cat_one', 'choices': ['good', 'bad'], 'metrics': ['sum'] }, ], 'groups': ['entity_id', 'zip_code'], 'intervals': ['all'], 'knowledge_date_column': 'knowledge_date', 'from_obj': 'data' }] FeatureGenerator(db_engine, 'features_test').create_features_before_imputation( feature_aggregation_config=feature_config, feature_dates=['2016-01-01'] )","title":"Testing Feature Configuration"},{"location":"experiments/feature-testing/#testing-a-feature-aggregation","text":"Developing features for Triage experiments can be a daunting task. There are a lot of things to configure, a small amount of configuration can result in a ton of SQL, and it can take a long time to validate your feature configuration in the context of an Experiment being run on real data. To speed up the process of iterating on features, you can run a list of feature aggregations, without imputation, on just one as-of-date. This functionality can be accessed through the triage command line tool or called directly from code (say, in a Jupyter notebook) using the FeatureGenerator component.","title":"Testing a Feature Aggregation"},{"location":"experiments/feature-testing/#using-triage-cli","text":"The command-line interface for testing features takes in two arguments: - A feature config file. Refer to example_feature_config.yaml . Essentially this is the content of the example_experiment_config.yaml 's feature_aggregations section. It consists of a YAML list, with one or more feature_aggregation rows present. - An as-of-date. This should be in the format 2016-01-01 . Example: triage experiment featuretest example_feature_config.yaml 2016-01-01 All given feature aggregations will be processed for the given date. You will see a bunch of queries pass by in your terminal, populating tables in the features_test schema which you can inspect afterwards.","title":"Using Triage CLI"},{"location":"experiments/feature-testing/#using-python-code","text":"If you'd like to call this from a notebook or from any other Python code, the arguments look similar but are a bit different. You have to supply your own sqlalchemy database engine to create a 'FeatureGenerator' object, and then call the create_features_before_imputation method with your feature config as a list of dictionaries, along with an as-of-date as a string. Make sure your logging level is set to INFO if you want to see all of the queries. from triage.component.architect.feature_generators import FeatureGenerator from triage.util.db import create_engine import logging import yaml logging.basicConfig(level=logging.INFO) # create a db_engine db_url = 'your db url here' db_engine = create_engine(db_url) feature_config = [{ 'prefix': 'aprefix', 'aggregates': [ { 'quantity': 'quantity_one', 'metrics': ['sum', 'count'], ], 'categoricals': [ { 'column': 'cat_one', 'choices': ['good', 'bad'], 'metrics': ['sum'] }, ], 'groups': ['entity_id', 'zip_code'], 'intervals': ['all'], 'knowledge_date_column': 'knowledge_date', 'from_obj': 'data' }] FeatureGenerator(db_engine, 'features_test').create_features_before_imputation( feature_aggregation_config=feature_config, feature_dates=['2016-01-01'] )","title":"Using Python Code"},{"location":"experiments/features/","text":"Feature Generation Recipe Book This document is a collection of 'collate' aggregate features that we have found useful to create in Triage that may not be apparent at first. For an introduction to feature generation in Triage, refer to Dirty Duck Feature Generation Age You can calculate age from a date of birth column using the collate_date special variable. This variable is marked as a placeholder in the feature quantity input, but is replaced with each as-of-date when features are being calculated. Combined with the Postgres age function, this calculates a person's age at each as-of-date as a feature. For this example, let's assume you have a column called 'dob' that is a timestamp (or anything that can be cast to a date) in your source table. The feature_aggregation 's quantity would be: EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE)) If Triage is calculating this for the as-of-date '2016-01-01', it will internally expand the collate_date out to: EXTRACT(YEAR FROM AGE('2016-01-01'::DATE, dob::DATE)) In context, a feature aggregate that uses age may look more like: aggregates: - # age in years quantity: age: \"EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE))\" metrics: ['max'] Here, we call the feature 'age' and since everything in collate is defined as an aggregate, we pick 'max'; Any records for the same person and as-of-date should have the same 'dob', so there are many aggregates you can use that will arrive at the same answer (e.g. 'min', 'avg'). In these cases 'max' is the standard aggregate metric of choice in Triage.","title":"Feature Generation Recipe Book"},{"location":"experiments/features/#feature-generation-recipe-book","text":"This document is a collection of 'collate' aggregate features that we have found useful to create in Triage that may not be apparent at first. For an introduction to feature generation in Triage, refer to Dirty Duck Feature Generation","title":"Feature Generation Recipe Book"},{"location":"experiments/features/#age","text":"You can calculate age from a date of birth column using the collate_date special variable. This variable is marked as a placeholder in the feature quantity input, but is replaced with each as-of-date when features are being calculated. Combined with the Postgres age function, this calculates a person's age at each as-of-date as a feature. For this example, let's assume you have a column called 'dob' that is a timestamp (or anything that can be cast to a date) in your source table. The feature_aggregation 's quantity would be: EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE)) If Triage is calculating this for the as-of-date '2016-01-01', it will internally expand the collate_date out to: EXTRACT(YEAR FROM AGE('2016-01-01'::DATE, dob::DATE)) In context, a feature aggregate that uses age may look more like: aggregates: - # age in years quantity: age: \"EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE))\" metrics: ['max'] Here, we call the feature 'age' and since everything in collate is defined as an aggregate, we pick 'max'; Any records for the same person and as-of-date should have the same 'dob', so there are many aggregates you can use that will arrive at the same answer (e.g. 'min', 'avg'). In these cases 'max' is the standard aggregate metric of choice in Triage.","title":"Age"},{"location":"experiments/running/","text":"Running an Experiment Prerequisites To use a Triage experiment, you first need: Python 3.5 A PostgreSQL database with your source data (events, geographical data, etc) loaded. Ample space on an available disk (or S3) to store the needed matrices and models for your experiment An experiment definition (see Defining an Experiment ) You may run a Triage experiment two ways: through the Triage command line tool, or through instantiating an Experiment object in your own Python code and running it. The rest of this document will introduce experiment runs of increasing complexity, through both the CLI and Python interfaces. Simple Example To run an experiment, you need to tell triage at a minimum where to find the experiment file (in YAML format), and how to connect to the database, In this simple example, we're assuming that the experiment will be run with only one process, and that the matrices and models should be stored on the local filesystem. CLI The Triage CLI defaults database connection information to a file stored in 'database.yaml', so with this you can omit any mention of the database. In addition, if you leave out the project path. In addition, the 'project path' (where matrices and models are stored) defaults to the current working directory. So this is the simplest possible invocation: triage experiment example_experiment_config.yaml If you have the database information stored somewhere else, you may pass it to the top-level 'triage' command: triage -d mydbconfig.yaml experiment example_experiment_config.yaml Assuming you want the matrices and models stored somewhere else, pass it as the --project-path : triage -d mydbconfig.yaml experiment example_experiment_config.yaml --project-path '/path/to/directory/to/save/data' Python When running an experiment in Python, the database information is passed in the form of a SQLAlchemy database engine, and the experiment information is passed as a dictionary rather as YAML specifically. from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), # http://docs.sqlalchemy.org/en/latest/core/engines.html project_path='/path/to/directory/to/save/data' ) experiment.run() Multicore example Triage also offers the ability to parallelize both CPU-heavy and database-heavy tasks. Triage uses the multiprocessing library to perform both of these, but they are separately configurable as the database tasks will more likely be bounded by the number of connections/cores available on the database server instead of the number of cores available on the experiment running machine. CLI The Triage CLI allows parallelization to be specified through the --n-processes and --n-db-processes parameters. triage experiment example_experiment_config.yaml --project-path '/path/to/directory/to/save/data' --n-db-processes 4 --n-processes 8 Python In Python, you can use the MultiCoreExperiment instead of the SingleThreadedExperiment , and similarly pass the n_processes and n_db_processes parameters. We also recommend using triage.create_engine . It will create a serializable version of the engine that will be fully reconstructed in multiprocess contexts. If you pass a regular SQLAlchemy engine, in these contexts the engine will be reconstructed with the database URL only , which may cancel other settings you have used to configure your engine. from triage.experiments import MultiCoreExperiment from triage import create_engine experiment = MultiCoreExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), project_path='/path/to/directory/to/save/data', n_db_processes=4, n_processes=8, ) experiment.run() Using S3 to store matrices and models Triage can operate on different storage engines for matrices and models, and besides the standard filesystem engine comes with S3 support out of the box. To use this, just use the s3:// scheme for your project_path (this is similar for both Python and the CLI). CLI triage experiment example_experiment_config.yaml --project-path 's3://bucket/directory/to/save/data' Python from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), project_path='s3://bucket/directory/to/save/data' ) experiment.run() Validating an Experiment Configuring an experiment is complex, and running an experiment can take a long time as data scales up. If there are any misconfigured values, it's going to help out a lot to figure out what they are before we run the Experiment. So when you have completed your experiment config and want to test it out, it's best to validate the Experiment first. If any problems are detectable in your Experiment, either in configuration or the database tables referenced by it, this method will throw an exception. For instance, if I refer to the cat_complaints table in a feature aggregation but it doesn't exist, I'll see something like this: *** ValueError: from_obj query does not run. from_obj: \"cat_complaints\" Full error: (psycopg2.ProgrammingError) relation \"cat_complaints\" does not exist LINE 1: explain select * from cat_complaints ^ [SQL: 'explain select * from cat_complaints'] CLI The CLI, by default, validates before running. You can tweak this behavior, and make it not validate, or make it only validate. triage experiment example_experiment_config.yaml --project-path '/path/to/directory/to/save/data' --no-validate triage experiment example_experiment_config.yaml --project-path '/path/to/directory/to/save/data' --validate-only Python Experiments expose a validate method that can be run as needed. Experiment instantiation doesn't change from the run examples at all. experiment.validate() By default, the validate method will stop as soon as it encounters an error ('strict' mode). If you would like it to validate each section without stopping (i.e. if you have only written part of the experiment configuration), call validate(strict=False) and all of the errors will be changed to warnings. We'd like to add more validations for common misconfiguration problems over time. If you got an unexpected error that turned out to be related to a confusing configuration value, help us out by adding to the validation module and submitting a pull request! Restarting an Experiment If an experiment fails for any reason, you can restart it. Each matrix and each model file is saved with a filename matching a hash of its unique attributes, so when the experiment is rerun, it will by default reuse the matrix or model instead of rebuilding it. If you would like to change this behavior and replace existing versions of matrices and models, set the 'replace' flag. CLI triage experiment example_experiment_config.yaml --project-path '/path/to/directory/to/save/data' --replace Python from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), project_path='s3://bucket/directory/to/save/data', replace=True ) experiment.run() Running parts of an Experiment If you would like incrementally build, or just incrementally run parts of the Experiment look at their outputs, you can do so. Running a full experiment requires the experiment config to be filled out, but when you're getting started using Triage it can be easier to build the experiment piece by piece and see the results as they come in. Make sure logging is set to INFO level before running this to ensure you get all the log messages. Running parts of an experiment is only supported through the Python interface. Python experiment.run() will run until it no longer has enough configuration to proceed. You will see information in the logs telling you about the steps it was able to perform. If you initialize the Experiment with cleanup=False , you can view the intermediate tables that are built. They are modified with the experiment hash that the experiment calculates, but this will be printed out in the log messages. labels_*<experiment_hash>* for the labels generated per entity and as of date. tmp_sparse_states_*<experiment_hash>* for the membership in each cohort per entity and as_of_date To reproduce the entire Experiment piece by piece, you can run the following. Each one of these methods requires some portion of experiment config to be passed: experiment.split_definitions will parse temporal config and create time splits. It only requires temporal_config . experiment.generate_cohort() will use the cohort config and as of dates from the temporal config to generate an internal table keeping track of what entities are in the cohort on different dates. It requires temporal_config and cohort_config . experiment.generate_labels() will use the label config and as of dates from the temporal config to generate an internal labels table. It requires temporal_config and label_config . experiment.generate_preimputation_features() will use the feature aggregation config and as of dates from the temporal config to generate internal features tables. It requires temporal_config and feature_aggregations . experiment.generate_imputed_features() will use the imputation sections of the feature aggregation config and the results from the preimputed features to create internal imputed features tables. It requires temporal_config and feature_aggregations . experiment.build_matrices() will use all of the internal tables generated before this point, along with feature grouping config, to generate all needed matrices. It requires temporal_config , cohort_config , label_config , and feature_aggregations , though it will also use feature_group_definitions , feature_group_strategies , and user_metadata if present. experiment.train_and_test_models() will use the generated matrices, grid config and evaluation metric config to train and test all needed models. It requires all configuration keys. Evaluating results of an Experiment After the experiment run, a variety of schemas and tables will be created and populated in the configured database: model_metadata.experiments - The experiment configuration and a hash model_metadata.matrices - Each train or test matrix that is built has a row here, with some basic metadata model_metadata.experiment_matrices - A many-to-many table between experiments and matrices. This will have a row if the experiment used the matrix, regardless of whether or not it had to build it model_metadata.models - A model describes a trained classifier; you'll have one row for each trained file that gets saved. model_metadata.experiment_models - A many-to-many table between experiments and models. This will have a row if the experiment used the model, regardless of whether or not it had to build it model_metadata.model_groups - A model groups refers to all models that share parameters like classifier type, hyperparameters, etc, but have different training windows . Look at these to see how classifiers perform over different training windows. model_metadata.matrices - Each matrix that was used for training and testing has metadata written about it such as the matrix hash, length, and time configuration. train_results.feature_importances - The sklearn feature importances results for each trained model train_results.predictions - Prediction probabilities for train matrix entities generated against trained models train_results.evaluations - Metric scores of trained models on the training data. test_results.predictions - Prediction probabilities for test matrix entities generated against trained models test_results.evaluations - Metric scores of trained models over given testing windows test_results.individual_importances - Individual feature importance scores for test matrix entities. Here's an example query, which returns the top 10 model groups by precision at the top 100 entities: select model_groups.model_group_id, model_groups.model_type, model_groups.hyperparameters, max(test_evaluations.value) as max_precision from model_metadata.model_groups join model_metadata.models using (model_group_id) join test_results.evaluations using (model_id) where metric = 'precision@' and parameter = '100_abs' group by 1,2,3 order by 4 desc limit 10 Inspecting an Experiment before running Before you run an experiment, you can inspect properties of the Experiment object to ensure that it is configured in the way you want. Some examples: experiment.all_as_of_times for debugging temporal config. This will show all dates that features and labels will be calculated at. experiment.feature_dicts will output a list of feature dictionaries, representing the feature tables and columns configured in this experiment experiment.matrix_build_tasks will output a list representing each matrix that will be built. Experiment Classes SingleThreadedExperiment : An experiment that performs all tasks serially in a single thread. Good for simple use on small datasets, or for understanding the general flow of data through a pipeline. MultiCoreExperiment : An experiment that makes use of the multiprocessing library to parallelize various time-consuming steps. Takes an n_processes keyword argument to control how many workers to use. RQExperiment : An experiment that makes use of the python-rq library to enqueue individual tasks onto the default queue, and wait for the jobs to be finished before moving on. python-rq requires Redis and any number of worker processes running the Triage codebase. Triage does not set up any of this needed infrastructure for you. Available through the RQ extra ( pip install triage[rq] )","title":"Running an Experiment"},{"location":"experiments/running/#running-an-experiment","text":"","title":"Running an Experiment"},{"location":"experiments/running/#prerequisites","text":"To use a Triage experiment, you first need: Python 3.5 A PostgreSQL database with your source data (events, geographical data, etc) loaded. Ample space on an available disk (or S3) to store the needed matrices and models for your experiment An experiment definition (see Defining an Experiment ) You may run a Triage experiment two ways: through the Triage command line tool, or through instantiating an Experiment object in your own Python code and running it. The rest of this document will introduce experiment runs of increasing complexity, through both the CLI and Python interfaces.","title":"Prerequisites"},{"location":"experiments/running/#simple-example","text":"To run an experiment, you need to tell triage at a minimum where to find the experiment file (in YAML format), and how to connect to the database, In this simple example, we're assuming that the experiment will be run with only one process, and that the matrices and models should be stored on the local filesystem.","title":"Simple Example"},{"location":"experiments/running/#cli","text":"The Triage CLI defaults database connection information to a file stored in 'database.yaml', so with this you can omit any mention of the database. In addition, if you leave out the project path. In addition, the 'project path' (where matrices and models are stored) defaults to the current working directory. So this is the simplest possible invocation: triage experiment example_experiment_config.yaml If you have the database information stored somewhere else, you may pass it to the top-level 'triage' command: triage -d mydbconfig.yaml experiment example_experiment_config.yaml Assuming you want the matrices and models stored somewhere else, pass it as the --project-path : triage -d mydbconfig.yaml experiment example_experiment_config.yaml --project-path '/path/to/directory/to/save/data'","title":"CLI"},{"location":"experiments/running/#python","text":"When running an experiment in Python, the database information is passed in the form of a SQLAlchemy database engine, and the experiment information is passed as a dictionary rather as YAML specifically. from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), # http://docs.sqlalchemy.org/en/latest/core/engines.html project_path='/path/to/directory/to/save/data' ) experiment.run()","title":"Python"},{"location":"experiments/running/#multicore-example","text":"Triage also offers the ability to parallelize both CPU-heavy and database-heavy tasks. Triage uses the multiprocessing library to perform both of these, but they are separately configurable as the database tasks will more likely be bounded by the number of connections/cores available on the database server instead of the number of cores available on the experiment running machine.","title":"Multicore example"},{"location":"experiments/running/#cli_1","text":"The Triage CLI allows parallelization to be specified through the --n-processes and --n-db-processes parameters. triage experiment example_experiment_config.yaml --project-path '/path/to/directory/to/save/data' --n-db-processes 4 --n-processes 8","title":"CLI"},{"location":"experiments/running/#python_1","text":"In Python, you can use the MultiCoreExperiment instead of the SingleThreadedExperiment , and similarly pass the n_processes and n_db_processes parameters. We also recommend using triage.create_engine . It will create a serializable version of the engine that will be fully reconstructed in multiprocess contexts. If you pass a regular SQLAlchemy engine, in these contexts the engine will be reconstructed with the database URL only , which may cancel other settings you have used to configure your engine. from triage.experiments import MultiCoreExperiment from triage import create_engine experiment = MultiCoreExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), project_path='/path/to/directory/to/save/data', n_db_processes=4, n_processes=8, ) experiment.run()","title":"Python"},{"location":"experiments/running/#using-s3-to-store-matrices-and-models","text":"Triage can operate on different storage engines for matrices and models, and besides the standard filesystem engine comes with S3 support out of the box. To use this, just use the s3:// scheme for your project_path (this is similar for both Python and the CLI).","title":"Using S3 to store matrices and models"},{"location":"experiments/running/#cli_2","text":"triage experiment example_experiment_config.yaml --project-path 's3://bucket/directory/to/save/data'","title":"CLI"},{"location":"experiments/running/#python_2","text":"from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), project_path='s3://bucket/directory/to/save/data' ) experiment.run()","title":"Python"},{"location":"experiments/running/#validating-an-experiment","text":"Configuring an experiment is complex, and running an experiment can take a long time as data scales up. If there are any misconfigured values, it's going to help out a lot to figure out what they are before we run the Experiment. So when you have completed your experiment config and want to test it out, it's best to validate the Experiment first. If any problems are detectable in your Experiment, either in configuration or the database tables referenced by it, this method will throw an exception. For instance, if I refer to the cat_complaints table in a feature aggregation but it doesn't exist, I'll see something like this: *** ValueError: from_obj query does not run. from_obj: \"cat_complaints\" Full error: (psycopg2.ProgrammingError) relation \"cat_complaints\" does not exist LINE 1: explain select * from cat_complaints ^ [SQL: 'explain select * from cat_complaints']","title":"Validating an Experiment"},{"location":"experiments/running/#cli_3","text":"The CLI, by default, validates before running. You can tweak this behavior, and make it not validate, or make it only validate. triage experiment example_experiment_config.yaml --project-path '/path/to/directory/to/save/data' --no-validate triage experiment example_experiment_config.yaml --project-path '/path/to/directory/to/save/data' --validate-only","title":"CLI"},{"location":"experiments/running/#python_3","text":"Experiments expose a validate method that can be run as needed. Experiment instantiation doesn't change from the run examples at all. experiment.validate() By default, the validate method will stop as soon as it encounters an error ('strict' mode). If you would like it to validate each section without stopping (i.e. if you have only written part of the experiment configuration), call validate(strict=False) and all of the errors will be changed to warnings. We'd like to add more validations for common misconfiguration problems over time. If you got an unexpected error that turned out to be related to a confusing configuration value, help us out by adding to the validation module and submitting a pull request!","title":"Python"},{"location":"experiments/running/#restarting-an-experiment","text":"If an experiment fails for any reason, you can restart it. Each matrix and each model file is saved with a filename matching a hash of its unique attributes, so when the experiment is rerun, it will by default reuse the matrix or model instead of rebuilding it. If you would like to change this behavior and replace existing versions of matrices and models, set the 'replace' flag.","title":"Restarting an Experiment"},{"location":"experiments/running/#cli_4","text":"triage experiment example_experiment_config.yaml --project-path '/path/to/directory/to/save/data' --replace","title":"CLI"},{"location":"experiments/running/#python_4","text":"from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), project_path='s3://bucket/directory/to/save/data', replace=True ) experiment.run()","title":"Python"},{"location":"experiments/running/#running-parts-of-an-experiment","text":"If you would like incrementally build, or just incrementally run parts of the Experiment look at their outputs, you can do so. Running a full experiment requires the experiment config to be filled out, but when you're getting started using Triage it can be easier to build the experiment piece by piece and see the results as they come in. Make sure logging is set to INFO level before running this to ensure you get all the log messages. Running parts of an experiment is only supported through the Python interface.","title":"Running parts of an Experiment"},{"location":"experiments/running/#python_5","text":"experiment.run() will run until it no longer has enough configuration to proceed. You will see information in the logs telling you about the steps it was able to perform. If you initialize the Experiment with cleanup=False , you can view the intermediate tables that are built. They are modified with the experiment hash that the experiment calculates, but this will be printed out in the log messages. labels_*<experiment_hash>* for the labels generated per entity and as of date. tmp_sparse_states_*<experiment_hash>* for the membership in each cohort per entity and as_of_date To reproduce the entire Experiment piece by piece, you can run the following. Each one of these methods requires some portion of experiment config to be passed: experiment.split_definitions will parse temporal config and create time splits. It only requires temporal_config . experiment.generate_cohort() will use the cohort config and as of dates from the temporal config to generate an internal table keeping track of what entities are in the cohort on different dates. It requires temporal_config and cohort_config . experiment.generate_labels() will use the label config and as of dates from the temporal config to generate an internal labels table. It requires temporal_config and label_config . experiment.generate_preimputation_features() will use the feature aggregation config and as of dates from the temporal config to generate internal features tables. It requires temporal_config and feature_aggregations . experiment.generate_imputed_features() will use the imputation sections of the feature aggregation config and the results from the preimputed features to create internal imputed features tables. It requires temporal_config and feature_aggregations . experiment.build_matrices() will use all of the internal tables generated before this point, along with feature grouping config, to generate all needed matrices. It requires temporal_config , cohort_config , label_config , and feature_aggregations , though it will also use feature_group_definitions , feature_group_strategies , and user_metadata if present. experiment.train_and_test_models() will use the generated matrices, grid config and evaluation metric config to train and test all needed models. It requires all configuration keys.","title":"Python"},{"location":"experiments/running/#evaluating-results-of-an-experiment","text":"After the experiment run, a variety of schemas and tables will be created and populated in the configured database: model_metadata.experiments - The experiment configuration and a hash model_metadata.matrices - Each train or test matrix that is built has a row here, with some basic metadata model_metadata.experiment_matrices - A many-to-many table between experiments and matrices. This will have a row if the experiment used the matrix, regardless of whether or not it had to build it model_metadata.models - A model describes a trained classifier; you'll have one row for each trained file that gets saved. model_metadata.experiment_models - A many-to-many table between experiments and models. This will have a row if the experiment used the model, regardless of whether or not it had to build it model_metadata.model_groups - A model groups refers to all models that share parameters like classifier type, hyperparameters, etc, but have different training windows . Look at these to see how classifiers perform over different training windows. model_metadata.matrices - Each matrix that was used for training and testing has metadata written about it such as the matrix hash, length, and time configuration. train_results.feature_importances - The sklearn feature importances results for each trained model train_results.predictions - Prediction probabilities for train matrix entities generated against trained models train_results.evaluations - Metric scores of trained models on the training data. test_results.predictions - Prediction probabilities for test matrix entities generated against trained models test_results.evaluations - Metric scores of trained models over given testing windows test_results.individual_importances - Individual feature importance scores for test matrix entities. Here's an example query, which returns the top 10 model groups by precision at the top 100 entities: select model_groups.model_group_id, model_groups.model_type, model_groups.hyperparameters, max(test_evaluations.value) as max_precision from model_metadata.model_groups join model_metadata.models using (model_group_id) join test_results.evaluations using (model_id) where metric = 'precision@' and parameter = '100_abs' group by 1,2,3 order by 4 desc limit 10","title":"Evaluating results of an Experiment"},{"location":"experiments/running/#inspecting-an-experiment-before-running","text":"Before you run an experiment, you can inspect properties of the Experiment object to ensure that it is configured in the way you want. Some examples: experiment.all_as_of_times for debugging temporal config. This will show all dates that features and labels will be calculated at. experiment.feature_dicts will output a list of feature dictionaries, representing the feature tables and columns configured in this experiment experiment.matrix_build_tasks will output a list representing each matrix that will be built.","title":"Inspecting an Experiment before running"},{"location":"experiments/running/#experiment-classes","text":"SingleThreadedExperiment : An experiment that performs all tasks serially in a single thread. Good for simple use on small datasets, or for understanding the general flow of data through a pipeline. MultiCoreExperiment : An experiment that makes use of the multiprocessing library to parallelize various time-consuming steps. Takes an n_processes keyword argument to control how many workers to use. RQExperiment : An experiment that makes use of the python-rq library to enqueue individual tasks onto the default queue, and wait for the jobs to be finished before moving on. python-rq requires Redis and any number of worker processes running the Triage codebase. Triage does not set up any of this needed infrastructure for you. Available through the RQ extra ( pip install triage[rq] )","title":"Experiment Classes"},{"location":"experiments/temporal-validation/","text":"Temporal Validation Deep Dive A temporal validation deep dive is currently available in the Dirty Duck tutorial. Dirty Duck - Temporal Cross-validation You can produce the time graphs detailed in the Dirty Duck deep dive using the Triage CLI or through calling Python code directly. The graphs use matplotlib, so you'll need a matplotlib backend to use. Refer to the matplotlib docs for more details. Python Code Plotting is supported through the visualize_chops function, which takes a fully configured Timechop object. You may store the configuration for this object in a YAML file if you wish and load from a file, but in this example we directly set the parameters as arguments to the Timechop object. This would enable faster iteration of time config in a notebook setting. from triage.component.timechop.plotting import visualize_chops from triage.component.timechop import Timechop chopper = Timechop( feature_start_time='2010-01-01' feature_end_time='2015-01-01' # latest date included in features label_start_time='2012-01-01' # earliest date for which labels are avialable label_end_time='2015-01-01' # day AFTER last label date (all dates in any model are < this date) model_update_frequency='6month' # how frequently to retrain models training_as_of_date_frequencies='1day' # time between as of dates for same entity in train matrix test_as_of_date_frequencies='3month' # time between as of dates for same entity in test matrix max_training_histories=['6month', '3month'] # length of time included in a train matrix test_durations=['0day', '1month', '2month'] # length of time included in a test matrix (0 days will give a single prediction immediately after training end) training_label_timespans=['1month'] # time period across which outcomes are labeled in train matrices test_label_timespans=['7day'] # time period across which outcomes are labeled in test matrices ) visualize_chops(chopper) Triage CLI The Triage CLI exposes the showtimechops command which just takes a YAML file as input. This YAML file is expected to have a temporal_config section with Timechop parameters. You can use a full experiment config, or just create a YAML file with only temporal config parameters; the temporal config just has to be present. Here, we use the example_experiment_config.yaml from the Triage repository root as an example. triage showtimechops example_experiment_config.yaml Result Using either method, you should see output similar to this:","title":"Temporal Validation Deep Dive"},{"location":"experiments/temporal-validation/#temporal-validation-deep-dive","text":"A temporal validation deep dive is currently available in the Dirty Duck tutorial. Dirty Duck - Temporal Cross-validation You can produce the time graphs detailed in the Dirty Duck deep dive using the Triage CLI or through calling Python code directly. The graphs use matplotlib, so you'll need a matplotlib backend to use. Refer to the matplotlib docs for more details.","title":"Temporal Validation Deep Dive"},{"location":"experiments/temporal-validation/#python-code","text":"Plotting is supported through the visualize_chops function, which takes a fully configured Timechop object. You may store the configuration for this object in a YAML file if you wish and load from a file, but in this example we directly set the parameters as arguments to the Timechop object. This would enable faster iteration of time config in a notebook setting. from triage.component.timechop.plotting import visualize_chops from triage.component.timechop import Timechop chopper = Timechop( feature_start_time='2010-01-01' feature_end_time='2015-01-01' # latest date included in features label_start_time='2012-01-01' # earliest date for which labels are avialable label_end_time='2015-01-01' # day AFTER last label date (all dates in any model are < this date) model_update_frequency='6month' # how frequently to retrain models training_as_of_date_frequencies='1day' # time between as of dates for same entity in train matrix test_as_of_date_frequencies='3month' # time between as of dates for same entity in test matrix max_training_histories=['6month', '3month'] # length of time included in a train matrix test_durations=['0day', '1month', '2month'] # length of time included in a test matrix (0 days will give a single prediction immediately after training end) training_label_timespans=['1month'] # time period across which outcomes are labeled in train matrices test_label_timespans=['7day'] # time period across which outcomes are labeled in test matrices ) visualize_chops(chopper)","title":"Python Code"},{"location":"experiments/temporal-validation/#triage-cli","text":"The Triage CLI exposes the showtimechops command which just takes a YAML file as input. This YAML file is expected to have a temporal_config section with Timechop parameters. You can use a full experiment config, or just create a YAML file with only temporal config parameters; the temporal config just has to be present. Here, we use the example_experiment_config.yaml from the Triage repository root as an example. triage showtimechops example_experiment_config.yaml","title":"Triage CLI"},{"location":"experiments/temporal-validation/#result","text":"Using either method, you should see output similar to this:","title":"Result"},{"location":"experiments/upgrade-to-v5/","text":"Upgrading your experiment configuration to v5 This document details the steps needed to update a triage v3 or v4 configuration to v5, mimicing the old behavior (as opposed to taking advantage of new options) as much as possible. In the experiment configuration v5, several things were changed: state_config becomes cohort_config , and receives new options label_config is changed to take a parameterized query model_group_keys is changed to have more robust defaults, and values specified in the config file act as overrides for the defaults instead of additions to them. state_config -> cohort_config Upgrading the state config is fairly straightforward, as no functionality was removed. The key at which the state table-based configuration can be passed has changed. Before it resided at the top-level state_config key, whereas now it is in the optional dense_states key within the top-level cohort_config key. Old: state_config: table_name: 'states' state_filters: - 'state_one AND state_two' - '(state_one OR state_two) AND state_three' New: cohort_config: dense_states: table_name: 'states' state_filters: - 'state_one AND state_two' - '(state_one OR state_two) AND state_three' label_config The label config has had functionality changed, so there is more conversion that needs to happen. Instead of taking in an 'events' table and making assumptions suitable for inspections tasks based on that table, for transparency and flexibility this now takes a parameterized query, as well as an optional include_missing_labels_in_train_as boolean. Leaving out this boolean value reproduces the inspections behavior (missing labels are treated as null), so to upgrade old configurations it is not needed. Old: events_table: 'events' New: label_config: query: | select events.entity_id, bool_or(outcome::bool)::integer as outcome from events where '{as_of_date}' <= outcome_date and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id model_group_keys The model group configuration was changed quite a bit. Before, the Experiment defined a few default grouping keys and would treat anything included in the config as additional. In practice, there were many keys that were almost always included as additional model group keys, and these are now default. There are also other keys that generally make sense if certain things are iterated on (e.g. feature groups). The goal is for most projects to simply leave out this configuration value entirely. If possible, this is the recommended route to go. But for the purposes of this guide, this change should duplicate the old behavior exactly. Old (empty, using defaults): New: model_group_keys: ['class_path', 'parameters', 'feature_names'] Old (more standard in practice, adding some temporal parameters): model_group_keys: ['label_timespan', 'as_of_date_frequency', 'max_training_history'] New: model_group_keys: ['class_path', 'parameters', 'feature_names', 'label_timespan', 'as_of_date_frequency', 'max_training_history'] Upgrading the experiment config version At this point, you should be able to bump the top-level experiment config version to v5: Old: config_version: 'v4' New: config_version: 'v5'","title":"Upgrading your experiment configuration to v5"},{"location":"experiments/upgrade-to-v5/#upgrading-your-experiment-configuration-to-v5","text":"This document details the steps needed to update a triage v3 or v4 configuration to v5, mimicing the old behavior (as opposed to taking advantage of new options) as much as possible. In the experiment configuration v5, several things were changed: state_config becomes cohort_config , and receives new options label_config is changed to take a parameterized query model_group_keys is changed to have more robust defaults, and values specified in the config file act as overrides for the defaults instead of additions to them.","title":"Upgrading your experiment configuration to v5"},{"location":"experiments/upgrade-to-v5/#state_config-cohort_config","text":"Upgrading the state config is fairly straightforward, as no functionality was removed. The key at which the state table-based configuration can be passed has changed. Before it resided at the top-level state_config key, whereas now it is in the optional dense_states key within the top-level cohort_config key. Old: state_config: table_name: 'states' state_filters: - 'state_one AND state_two' - '(state_one OR state_two) AND state_three' New: cohort_config: dense_states: table_name: 'states' state_filters: - 'state_one AND state_two' - '(state_one OR state_two) AND state_three'","title":"state_config -&gt; cohort_config"},{"location":"experiments/upgrade-to-v5/#label_config","text":"The label config has had functionality changed, so there is more conversion that needs to happen. Instead of taking in an 'events' table and making assumptions suitable for inspections tasks based on that table, for transparency and flexibility this now takes a parameterized query, as well as an optional include_missing_labels_in_train_as boolean. Leaving out this boolean value reproduces the inspections behavior (missing labels are treated as null), so to upgrade old configurations it is not needed. Old: events_table: 'events' New: label_config: query: | select events.entity_id, bool_or(outcome::bool)::integer as outcome from events where '{as_of_date}' <= outcome_date and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id","title":"label_config"},{"location":"experiments/upgrade-to-v5/#model_group_keys","text":"The model group configuration was changed quite a bit. Before, the Experiment defined a few default grouping keys and would treat anything included in the config as additional. In practice, there were many keys that were almost always included as additional model group keys, and these are now default. There are also other keys that generally make sense if certain things are iterated on (e.g. feature groups). The goal is for most projects to simply leave out this configuration value entirely. If possible, this is the recommended route to go. But for the purposes of this guide, this change should duplicate the old behavior exactly. Old (empty, using defaults): New: model_group_keys: ['class_path', 'parameters', 'feature_names'] Old (more standard in practice, adding some temporal parameters): model_group_keys: ['label_timespan', 'as_of_date_frequency', 'max_training_history'] New: model_group_keys: ['class_path', 'parameters', 'feature_names', 'label_timespan', 'as_of_date_frequency', 'max_training_history']","title":"model_group_keys"},{"location":"experiments/upgrade-to-v5/#upgrading-the-experiment-config-version","text":"At this point, you should be able to bump the top-level experiment config version to v5: Old: config_version: 'v4' New: config_version: 'v5'","title":"Upgrading the experiment config version"},{"location":"experiments/upgrade-to-v6/","text":"Upgrading your experiment configuration to v6 This document details the steps needed to update a triage v5 configuration to v6, mimicking the old behavior. Experiment configuration v6 includes only one change from v5: When specifying the cohort_config , if a query is given , the {af_of_date} is no longer quoted or casted by Triage. Instead, the user must perform the quoting and casting, as is done already for the label_config . Old: cohort_config: query: | SELECT DISTINCT entity_id FROM semantic.events WHERE event = 'booking' AND startdt <@ daterange(({as_of_date} - '3 years'::interval)::date, {as_of_date}) AND enddt < {as_of_date} LIMIT 100 name: 'booking_last_3_years_limit_100' New: cohort_config: query: | SELECT DISTINCT entity_id FROM semantic.events WHERE event = 'booking' AND startdt <@ daterange(('{as_of_date}'::date - '3 years'::interval)::date, '{as_of_date}'::date) AND enddt < '{as_of_date}' LIMIT 100 name: 'booking_last_3_years_limit_100' Upgrading the experiment config version At this point, you should be able to bump the top-level experiment config version to v6: Old: config_version: 'v5' New: config_version: 'v6'","title":"Upgrading your experiment configuration to v6"},{"location":"experiments/upgrade-to-v6/#upgrading-your-experiment-configuration-to-v6","text":"This document details the steps needed to update a triage v5 configuration to v6, mimicking the old behavior. Experiment configuration v6 includes only one change from v5: When specifying the cohort_config , if a query is given , the {af_of_date} is no longer quoted or casted by Triage. Instead, the user must perform the quoting and casting, as is done already for the label_config . Old: cohort_config: query: | SELECT DISTINCT entity_id FROM semantic.events WHERE event = 'booking' AND startdt <@ daterange(({as_of_date} - '3 years'::interval)::date, {as_of_date}) AND enddt < {as_of_date} LIMIT 100 name: 'booking_last_3_years_limit_100' New: cohort_config: query: | SELECT DISTINCT entity_id FROM semantic.events WHERE event = 'booking' AND startdt <@ daterange(('{as_of_date}'::date - '3 years'::interval)::date, '{as_of_date}'::date) AND enddt < '{as_of_date}' LIMIT 100 name: 'booking_last_3_years_limit_100'","title":"Upgrading your experiment configuration to v6"},{"location":"experiments/upgrade-to-v6/#upgrading-the-experiment-config-version","text":"At this point, you should be able to bump the top-level experiment config version to v6: Old: config_version: 'v5' New: config_version: 'v6'","title":"Upgrading the experiment config version"}]}