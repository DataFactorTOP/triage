{
    "docs": [
        {
            "location": "/",
            "text": "Triage\n\u00b6\n\n\nRisk modeling and prediction\n\n\n\n\n\n\n\n\nPredictive analytics projects require the coordination of many different tasks, such as feature generation, classifier training, evaluation, and list generation. These tasks are complicated in their own right, but in addition have to be combined in different ways throughout the course of the project. \n\n\nTriage aims to provide interfaces to these different phases of a project, such as an \nExperiment\n. Each phase is defined by configuration specific to the needs of the project, and an arrangement of core data science components that work together to produce the output of that phase.\n\n\nThe first phase implemented in triage is the \nExperiment\n. An experiment represents the initial research work of creating design matrices from source data, and training/testing/evaluating a model grid on those matrices. At the end of the experiment, a relational database with results metadata is populated, allowing for evaluation by the researcher.\n\n\nRunning an Experiment\n\u00b6\n\n\nSee the \nRunning an Experiment\n documentation.\n\n\nUpgrading an Experiment config\n\u00b6\n\n\nv3/v4 -> v5\n\n\nBackground\n\u00b6\n\n\nTriage is developed at the University of Chicago's \nCenter For Data Science and Public Policy\n. We created it in response to commonly occuring challenges we've encountered and patterns we've developed while working on projects for our partners.\n\n\nMajor Components Used by Triage\n\u00b6\n\n\nTriage makes use of many core data science components developed at DSaPP. These components can be useful in their own right, and are worth checking out if \n\n\n\n\nArchitect\n: Plan, design and build train and test matrices. Includes feature and label generation.\n\n\nCollate\n: Aggregation SQL Query Builder. This is used by the Architect to build features.\n\n\nTimechop\n: Generate temporal cross-validation time windows for matrix creation\n\n\nMetta-Data\n: Train and test matrix storage\n\n\nCatwalk\n: Training, testing, and evaluating machine learning classifier models\n\n\nResults Schema\n: Generate a database schema suitable for storing the results of modeling runs\n\n\n\n\nDesign Goals\n\u00b6\n\n\nThere are two overarching design goals for Triage:\n\n\n\n\n\n\nAll configuration necessary to run the full experiment from the external interface (ie, Experiment subclasses) from beginning to end must be easily serializable and machine-constructable, to allow the eventual development of tools for users to design experiments. \n\n\n\n\n\n\nAll core functionality must be usable outside of a specific pipeline context or workflow manager. There are many good workflow managers; everybody has their favorite, and core functionality should not be designed to work with specific execution expectations.\n\n\n\n\n\n\nFuture Plans\n\u00b6\n\n\n\n\nGeneration and Management of lists (ie for inspections) by various criteria\n\n\nIntegration of components with various workflow managers, like \nDrain\n and \nLuigi\n.\n\n\nComprehensive leakage testing of an experiment's modeling run\n\n\nFeature Generation Wizard",
            "title": "Home"
        },
        {
            "location": "/#triage",
            "text": "Risk modeling and prediction     Predictive analytics projects require the coordination of many different tasks, such as feature generation, classifier training, evaluation, and list generation. These tasks are complicated in their own right, but in addition have to be combined in different ways throughout the course of the project.   Triage aims to provide interfaces to these different phases of a project, such as an  Experiment . Each phase is defined by configuration specific to the needs of the project, and an arrangement of core data science components that work together to produce the output of that phase.  The first phase implemented in triage is the  Experiment . An experiment represents the initial research work of creating design matrices from source data, and training/testing/evaluating a model grid on those matrices. At the end of the experiment, a relational database with results metadata is populated, allowing for evaluation by the researcher.",
            "title": "Triage"
        },
        {
            "location": "/#running-an-experiment",
            "text": "See the  Running an Experiment  documentation.",
            "title": "Running an Experiment"
        },
        {
            "location": "/#upgrading-an-experiment-config",
            "text": "v3/v4 -> v5",
            "title": "Upgrading an Experiment config"
        },
        {
            "location": "/#background",
            "text": "Triage is developed at the University of Chicago's  Center For Data Science and Public Policy . We created it in response to commonly occuring challenges we've encountered and patterns we've developed while working on projects for our partners.",
            "title": "Background"
        },
        {
            "location": "/#major-components-used-by-triage",
            "text": "Triage makes use of many core data science components developed at DSaPP. These components can be useful in their own right, and are worth checking out if    Architect : Plan, design and build train and test matrices. Includes feature and label generation.  Collate : Aggregation SQL Query Builder. This is used by the Architect to build features.  Timechop : Generate temporal cross-validation time windows for matrix creation  Metta-Data : Train and test matrix storage  Catwalk : Training, testing, and evaluating machine learning classifier models  Results Schema : Generate a database schema suitable for storing the results of modeling runs",
            "title": "Major Components Used by Triage"
        },
        {
            "location": "/#design-goals",
            "text": "There are two overarching design goals for Triage:    All configuration necessary to run the full experiment from the external interface (ie, Experiment subclasses) from beginning to end must be easily serializable and machine-constructable, to allow the eventual development of tools for users to design experiments.     All core functionality must be usable outside of a specific pipeline context or workflow manager. There are many good workflow managers; everybody has their favorite, and core functionality should not be designed to work with specific execution expectations.",
            "title": "Design Goals"
        },
        {
            "location": "/#future-plans",
            "text": "Generation and Management of lists (ie for inspections) by various criteria  Integration of components with various workflow managers, like  Drain  and  Luigi .  Comprehensive leakage testing of an experiment's modeling run  Feature Generation Wizard",
            "title": "Future Plans"
        },
        {
            "location": "/experiments/defining/",
            "text": "Defining an Experiment\n\u00b6\n\n\nThis doc is coming soon. In the meantime, check out:\n\n\n\n\nExample Experiment Definition\n for an overview of the different sections of an experiment definition.\n\n\nDirty Duck\n for a beginning-to-end walkthrough to Triage, include deep dives on creating experiment configuration.",
            "title": "Defining an Experiment"
        },
        {
            "location": "/experiments/defining/#defining-an-experiment",
            "text": "This doc is coming soon. In the meantime, check out:   Example Experiment Definition  for an overview of the different sections of an experiment definition.  Dirty Duck  for a beginning-to-end walkthrough to Triage, include deep dives on creating experiment configuration.",
            "title": "Defining an Experiment"
        },
        {
            "location": "/experiments/running/",
            "text": "Running an Experiment\n\u00b6\n\n\nPrerequisites\n\u00b6\n\n\nTo use a Triage experiment, you first need:\n\n\n\n\nPython 3.5\n\n\nA PostgreSQL database with your source data (events, geographical data, etc) loaded.\n\n\nAmple space on an available disk (or S3) to store the needed matrices and models for your experiment\n\n\nAn experiment definition (see \nDefining an Experiment\n)\n\n\n\n\nInstantiating an Experiment\n\u00b6\n\n\nAn \nExperiment\n class, once instantiated, provides access to a variety of useful pieces of information about the experiment, as well as the ability to run it and get results.\n\n\nFirst, we'll look at how to instantiate the Experiment.\n\n\n    SingleThreadedExperiment(\n        config=experiment_config,\n        db_engine=triage.create_engine(...),\n        model_storage_class=FSModelStorageEngine,\n        project_path='/path/to/directory/to/save/data'\n    )\n\n\n\n\nThese lines are a bit dense: what is happening here?\n\n\n\n\nSingleThreadedExperiment\n:  There are different Experiment classes available in \ntriage.experiments\n to use, and they each represent a different way of executing the experiment, which we'll talk about in more detail later. The simplest (but slowest) is the \nSingleThreadedExperiment\n.\n\n\nconfig=experiment_config\n: The bulk of the work needed in designing an experiment will be in creating this experiment configuration. See \ndefining.md\n; more detailed instructions on each section are located in the example file. Generally these would be easiest to store as a file (or multiple files that you construct together) like that YAML file, but the configuration is passed in dict format to the Experiment constructor and you can store it however you wish.\n\n\ndb_engine=triage.create_engine(...)\n: A SQLAlchemy database engine. This will be used both for querying your source tables and writing results metadata. To create this, we recommend using \ntriage.create_engine\n. It will create a serializable version of the engine that will be fully reconstructed in multiprocess contexts. If you pass a regular SQLAlchemy engine, in these contexts the engine will be reconstructed with the \nURL only\n, which may cancel other settings you have used to configure your engine.\n\n\nmodel_storage_class=FSModelStorageEngine\n: The path to a model storage engine class. A model storage engine class is one that wraps all model storage I/O operations, allowing storage medium (e.g. filesystem, S3) to be switched easily. Triage provides a few of these in \ntriage.component.catwalk.storage\n. Shown here is the \nFSModelStorageEngine\n, which is a good place to start.\n\n\nproject_path='/path/to/directory/to/save/data'\n: The path to where you would like to store design matrices and trained models. May be an s3 path (e.g. s3://bucket-name/project-directory), in which case s3 will be used to store matrices and models.\n\n\n\n\nWith that in mind, a more full version of the experiment instantiation might look like this\n\n\n    import yaml\n    import logging\n\n    from triage import create_engine\n    from triage.component.catwalk.storage import FSModelStorageEngine\n    from triage.experiments import SingleThreadedExperiment\n\n    with open('my_experiment_config.yaml') as f:\n        experiment_config = yaml.load(f)\n    with open('my_database_creds') as f:\n        db_connection_string = yaml.load(f)['db_connection_string']\n\n    logging.basicConfig(level=logging.INFO)\n\n    experiment = SingleThreadedExperiment(\n        config=experiment_config,\n        db_engine=create_engine(db_connection_string),\n        model_storage_class=FSModelStorageEngine,\n        project_path='/home/research/myproject'\n    )\n\n\n\n\nValidating an Experiment\n\u00b6\n\n\nConfiguring an experiment is very complicated, and running an experiment can take a long time as data scales up. If there are any misconfigured values, it's going to help out a lot to figure out what they are before we run the Experiment. So we recommend running the \n.validate()\n method on the Experiment first. If any problems are detectable in your Experiment, either in configuration or the database tables referenced by it, this method will throw an exception. For instance, if I refer to the 'cat_complaints' table in a feature aggregation but it doesn't exist, I'll see something like this:\n\n\n    experiment.validate()\n\n    (Pdb) experiment.validate()\n    *** ValueError: from_obj query does not run.\n    from_obj: \"cat_complaints\"\n    Full error: (psycopg2.ProgrammingError) relation \"cat_complaints\" does not exist\n    LINE 1: explain select * from cat_complaints\n                                  ^\n     [SQL: 'explain select * from cat_complaints']\n\n\n\n\nIf the validation runs without any errors, you should see a success message (either in your log or console). At this point, the Experiment should be ready to run.\n\n\nWe'd like to add more validations for common misconfiguration problems over time. If you got an unexpected error that turned out to be related to a confusing configuration value, help us out by adding to the \nvalidation module\n and submitting a pull request!\n\n\nRunning an Experiment\n\u00b6\n\n\nOnce you're at this point, running the experiment is simple:\n\n\n    experiment.run()\n\n\n\n\nThis will run the entire experiment. This could take a while, so we recommend checking logging messages (INFO level will catch a lot of useful information) and keeping an eye on its progress.\n\n\nRunning parts of an Experiment\n\u00b6\n\n\nIf you would like to run parts of the Experiment one by one and look at their outputs, you can do so. To reproduce the entire Experiment piece by piece, you can run the following:\n\n\n\n\n\n\nexperiment.split_definitions\n will parse temporal config and create time splits. \n\n\n\n\n\n\nexperiment.generate_sparse_states()\n will use the cohort config and as of dates from the temporal config to generate an internal table keeping track of what entities are in the cohort on different dates.\n\n\n\n\n\n\nexperiment.generate_labels()\n will use the label config and as of dates from the temporal config to generate an internal labels table.\n\n\n\n\n\n\nexperiment.generate_preimputation_features()\n will use the feature aggregation config and as of dates from the temporal config to generate internal features tables.\n\n\n\n\n\n\nexperiment.generate_imputed_features()\n will use the imputation sections of the feature aggregation config and the results from the preimputed features to create internal imputed features tables\n\n\n\n\n\n\nexperiment.build_matrices()\n will use all of the internal tables generated before this point, along with feature grouping config, to generate all needed matrices.\n\n\n\n\n\n\nexperiment.train_and_test_models()\n will use the generated matrices, grid config and evaluation metric config to train and test all needed models.\n\n\n\n\n\n\nEvaluating results of an Experiment\n\u00b6\n\n\nAfter the experiment run, a variety of schemas and tables will be created and populated in the configured database:\n\n\n\n\nmodel_metadata.experiments - The experiment configuration and a hash\n\n\nmodel_metadata.models - A model describes a trained classifier; you'll have one row for each trained file that gets saved.\n\n\nmodel_metadata.model_groups - A model groups refers to all models that share parameters like classifier type, hyperparameters, etc, but \nhave different training windows\n. Look at these to see how classifiers perform over different training windows.\n\n\nmodel_metadata.matrices - Each matrix that was used for training and testing has metadata written about it such as the matrix hash, length, and time configuration.\n\n\ntrain_results.feature_importances - The sklearn feature importances results for each trained model\n\n\ntrain_results.predictions - Prediction probabilities for train matrix entities generated against trained models\n\n\ntrain_results.evaluations - Metric scores of trained models on the training data.\n\n\ntest_results.predictions - Prediction probabilities for test matrix entities generated against trained models\n\n\ntest_results.evaluations - Metric scores of trained models over given testing windows\n\n\ntest_results.individual_importances - Individual feature importance scores for test matrix entities.\n\n\n\n\nHere's an example query, which returns the top 10 model groups by precision at the top 100 entities:\n\n\n    select\n        model_groups.model_group_id,\n        model_groups.model_type,\n        model_groups.model_parameters,\n        max(test_evaluations.value) as max_precision\n    from model_metadata.model_groups\n        join model_metadata.models using (model_group_id)\n        join test_results.test_evaluations using (model_id)\n    where\n        metric = 'precision@'\n        and parameter = '100_abs'\n    group by 1,2,3\n    order by 4 desc\n    limit 10\n\n\n\n\nRestarting an Experiment\n\u00b6\n\n\nIf an experiment fails for any reason, you can restart it. Each matrix and each model file is saved with a filename matching a hash of its unique attributes, so when the experiment is rerun, it will by default reuse the matrix or model instead of rebuilding it. If you would like to change this behavior and replace existing versions of matrices and models, set \nreplace=True\n in the Experiment constructor.\n\n\nInspecting an Experiment before running\n\u00b6\n\n\nBefore you run an experiment, you can inspect properties of the Experiment object to ensure that it is configured in the way you want. Some examples:\n\n\n\n\nexperiment.all_as_of_times\n for debugging temporal config. This will show all dates that features and labels will be calculated at.\n\n\nexperiment.feature_dicts\n will output a list of feature dictionaries, representing the feature tables and columns configured in this experiment\n\n\nexperiment.matrix_build_tasks\n will output a list representing each matrix that will be built.\n\n\n\n\nExperiment Classes\n\u00b6\n\n\n\n\nSingleThreadedExperiment\n: An experiment that performs all tasks serially in a single thread. Good for simple use on small datasets, or for understanding the general flow of data through a pipeline.\n\n\nMultiCoreExperiment\n: An experiment that makes use of the multiprocessing library to parallelize various time-consuming steps. Takes an \nn_processes\n keyword argument to control how many workers to use.\n\n\nRQExperiment\n: An experiment that makes use of the python-rq library to enqueue individual tasks onto the default queue, and wait for the jobs to be finished before moving on. python-rq requires Redis and any number of worker processes running the Triage codebase. Triage does not set up any of this needed infrastructure for you. Available through the RQ extra ( \npip install triage[rq]\n )",
            "title": "Running an Experiment"
        },
        {
            "location": "/experiments/running/#running-an-experiment",
            "text": "",
            "title": "Running an Experiment"
        },
        {
            "location": "/experiments/running/#prerequisites",
            "text": "To use a Triage experiment, you first need:   Python 3.5  A PostgreSQL database with your source data (events, geographical data, etc) loaded.  Ample space on an available disk (or S3) to store the needed matrices and models for your experiment  An experiment definition (see  Defining an Experiment )",
            "title": "Prerequisites"
        },
        {
            "location": "/experiments/running/#instantiating-an-experiment",
            "text": "An  Experiment  class, once instantiated, provides access to a variety of useful pieces of information about the experiment, as well as the ability to run it and get results.  First, we'll look at how to instantiate the Experiment.      SingleThreadedExperiment(\n        config=experiment_config,\n        db_engine=triage.create_engine(...),\n        model_storage_class=FSModelStorageEngine,\n        project_path='/path/to/directory/to/save/data'\n    )  These lines are a bit dense: what is happening here?   SingleThreadedExperiment :  There are different Experiment classes available in  triage.experiments  to use, and they each represent a different way of executing the experiment, which we'll talk about in more detail later. The simplest (but slowest) is the  SingleThreadedExperiment .  config=experiment_config : The bulk of the work needed in designing an experiment will be in creating this experiment configuration. See  defining.md ; more detailed instructions on each section are located in the example file. Generally these would be easiest to store as a file (or multiple files that you construct together) like that YAML file, but the configuration is passed in dict format to the Experiment constructor and you can store it however you wish.  db_engine=triage.create_engine(...) : A SQLAlchemy database engine. This will be used both for querying your source tables and writing results metadata. To create this, we recommend using  triage.create_engine . It will create a serializable version of the engine that will be fully reconstructed in multiprocess contexts. If you pass a regular SQLAlchemy engine, in these contexts the engine will be reconstructed with the  URL only , which may cancel other settings you have used to configure your engine.  model_storage_class=FSModelStorageEngine : The path to a model storage engine class. A model storage engine class is one that wraps all model storage I/O operations, allowing storage medium (e.g. filesystem, S3) to be switched easily. Triage provides a few of these in  triage.component.catwalk.storage . Shown here is the  FSModelStorageEngine , which is a good place to start.  project_path='/path/to/directory/to/save/data' : The path to where you would like to store design matrices and trained models. May be an s3 path (e.g. s3://bucket-name/project-directory), in which case s3 will be used to store matrices and models.   With that in mind, a more full version of the experiment instantiation might look like this      import yaml\n    import logging\n\n    from triage import create_engine\n    from triage.component.catwalk.storage import FSModelStorageEngine\n    from triage.experiments import SingleThreadedExperiment\n\n    with open('my_experiment_config.yaml') as f:\n        experiment_config = yaml.load(f)\n    with open('my_database_creds') as f:\n        db_connection_string = yaml.load(f)['db_connection_string']\n\n    logging.basicConfig(level=logging.INFO)\n\n    experiment = SingleThreadedExperiment(\n        config=experiment_config,\n        db_engine=create_engine(db_connection_string),\n        model_storage_class=FSModelStorageEngine,\n        project_path='/home/research/myproject'\n    )",
            "title": "Instantiating an Experiment"
        },
        {
            "location": "/experiments/running/#validating-an-experiment",
            "text": "Configuring an experiment is very complicated, and running an experiment can take a long time as data scales up. If there are any misconfigured values, it's going to help out a lot to figure out what they are before we run the Experiment. So we recommend running the  .validate()  method on the Experiment first. If any problems are detectable in your Experiment, either in configuration or the database tables referenced by it, this method will throw an exception. For instance, if I refer to the 'cat_complaints' table in a feature aggregation but it doesn't exist, I'll see something like this:      experiment.validate()\n\n    (Pdb) experiment.validate()\n    *** ValueError: from_obj query does not run.\n    from_obj: \"cat_complaints\"\n    Full error: (psycopg2.ProgrammingError) relation \"cat_complaints\" does not exist\n    LINE 1: explain select * from cat_complaints\n                                  ^\n     [SQL: 'explain select * from cat_complaints']  If the validation runs without any errors, you should see a success message (either in your log or console). At this point, the Experiment should be ready to run.  We'd like to add more validations for common misconfiguration problems over time. If you got an unexpected error that turned out to be related to a confusing configuration value, help us out by adding to the  validation module  and submitting a pull request!",
            "title": "Validating an Experiment"
        },
        {
            "location": "/experiments/running/#running-an-experiment_1",
            "text": "Once you're at this point, running the experiment is simple:      experiment.run()  This will run the entire experiment. This could take a while, so we recommend checking logging messages (INFO level will catch a lot of useful information) and keeping an eye on its progress.",
            "title": "Running an Experiment"
        },
        {
            "location": "/experiments/running/#running-parts-of-an-experiment",
            "text": "If you would like to run parts of the Experiment one by one and look at their outputs, you can do so. To reproduce the entire Experiment piece by piece, you can run the following:    experiment.split_definitions  will parse temporal config and create time splits.     experiment.generate_sparse_states()  will use the cohort config and as of dates from the temporal config to generate an internal table keeping track of what entities are in the cohort on different dates.    experiment.generate_labels()  will use the label config and as of dates from the temporal config to generate an internal labels table.    experiment.generate_preimputation_features()  will use the feature aggregation config and as of dates from the temporal config to generate internal features tables.    experiment.generate_imputed_features()  will use the imputation sections of the feature aggregation config and the results from the preimputed features to create internal imputed features tables    experiment.build_matrices()  will use all of the internal tables generated before this point, along with feature grouping config, to generate all needed matrices.    experiment.train_and_test_models()  will use the generated matrices, grid config and evaluation metric config to train and test all needed models.",
            "title": "Running parts of an Experiment"
        },
        {
            "location": "/experiments/running/#evaluating-results-of-an-experiment",
            "text": "After the experiment run, a variety of schemas and tables will be created and populated in the configured database:   model_metadata.experiments - The experiment configuration and a hash  model_metadata.models - A model describes a trained classifier; you'll have one row for each trained file that gets saved.  model_metadata.model_groups - A model groups refers to all models that share parameters like classifier type, hyperparameters, etc, but  have different training windows . Look at these to see how classifiers perform over different training windows.  model_metadata.matrices - Each matrix that was used for training and testing has metadata written about it such as the matrix hash, length, and time configuration.  train_results.feature_importances - The sklearn feature importances results for each trained model  train_results.predictions - Prediction probabilities for train matrix entities generated against trained models  train_results.evaluations - Metric scores of trained models on the training data.  test_results.predictions - Prediction probabilities for test matrix entities generated against trained models  test_results.evaluations - Metric scores of trained models over given testing windows  test_results.individual_importances - Individual feature importance scores for test matrix entities.   Here's an example query, which returns the top 10 model groups by precision at the top 100 entities:      select\n        model_groups.model_group_id,\n        model_groups.model_type,\n        model_groups.model_parameters,\n        max(test_evaluations.value) as max_precision\n    from model_metadata.model_groups\n        join model_metadata.models using (model_group_id)\n        join test_results.test_evaluations using (model_id)\n    where\n        metric = 'precision@'\n        and parameter = '100_abs'\n    group by 1,2,3\n    order by 4 desc\n    limit 10",
            "title": "Evaluating results of an Experiment"
        },
        {
            "location": "/experiments/running/#restarting-an-experiment",
            "text": "If an experiment fails for any reason, you can restart it. Each matrix and each model file is saved with a filename matching a hash of its unique attributes, so when the experiment is rerun, it will by default reuse the matrix or model instead of rebuilding it. If you would like to change this behavior and replace existing versions of matrices and models, set  replace=True  in the Experiment constructor.",
            "title": "Restarting an Experiment"
        },
        {
            "location": "/experiments/running/#inspecting-an-experiment-before-running",
            "text": "Before you run an experiment, you can inspect properties of the Experiment object to ensure that it is configured in the way you want. Some examples:   experiment.all_as_of_times  for debugging temporal config. This will show all dates that features and labels will be calculated at.  experiment.feature_dicts  will output a list of feature dictionaries, representing the feature tables and columns configured in this experiment  experiment.matrix_build_tasks  will output a list representing each matrix that will be built.",
            "title": "Inspecting an Experiment before running"
        },
        {
            "location": "/experiments/running/#experiment-classes",
            "text": "SingleThreadedExperiment : An experiment that performs all tasks serially in a single thread. Good for simple use on small datasets, or for understanding the general flow of data through a pipeline.  MultiCoreExperiment : An experiment that makes use of the multiprocessing library to parallelize various time-consuming steps. Takes an  n_processes  keyword argument to control how many workers to use.  RQExperiment : An experiment that makes use of the python-rq library to enqueue individual tasks onto the default queue, and wait for the jobs to be finished before moving on. python-rq requires Redis and any number of worker processes running the Triage codebase. Triage does not set up any of this needed infrastructure for you. Available through the RQ extra (  pip install triage[rq]  )",
            "title": "Experiment Classes"
        },
        {
            "location": "/experiments/temporal-validation/",
            "text": "Temporal Validation Deep Dive\n\u00b6\n\n\nThis doc is coming soon.",
            "title": "Temporal Validation Deep Dive"
        },
        {
            "location": "/experiments/temporal-validation/#temporal-validation-deep-dive",
            "text": "This doc is coming soon.",
            "title": "Temporal Validation Deep Dive"
        },
        {
            "location": "/experiments/features/",
            "text": "Experiment Features Deep Dive\n\u00b6\n\n\nThis doc is coming soon",
            "title": "Feature Definition Deep Dive"
        },
        {
            "location": "/experiments/features/#experiment-features-deep-dive",
            "text": "This doc is coming soon",
            "title": "Experiment Features Deep Dive"
        },
        {
            "location": "/experiments/algorithm/",
            "text": "Experiment Algorithm Deep Dive\n\u00b6\n\n\nThis guide's purpose is to provide familiarity of the inner workings of a Triage Experiment to people with some\nexperience in data science and Python. A Triage Experiment is a highly structured way of defining the experimentation\nphase of a data science project. To those wondering whether this Experiment structure is flexible enough to fit their\nneeds, this should help.\n\n\n1. Temporal Validation Setup\n\u00b6\n\n\nFirst, the given \ntemporal_config\n section in the experiment definition is transformed into train and test splits,\nincluding \nas_of_times\n for each matrix.\n\n\nWe create these splits by figuring out the latest reasonable split time from the inputs, and moving backwards in time\nat the rate of the given \nmodel_update_frequency\n, until we get to the earliest reasonable split time.\n\n\nFor each split, we create \nas_of_times\n by moving either backwards from the split time towards the\n\nmax_training_history\n (for train matrices) or forwards from the split time towards the \ntest_duration\n (for test\nmatrices) at the provided \ndata_frequency\n.\n\n\nMany of these configured values may be lists, in which case we generate the cross-product of all the possible values\nand generate more splits.\n\n\nFor a more detailed look at the temporal validation logic, see \nTemporal Validation Deep Dive\n.\n\n\nThe train and test splits themselves are not used until the \nBuilding Matrices\n section, but a\nflat list of all computed \nas_of_times\n for all matrices needed in the experiment is used in the next section,\n\nTransforming Data\n.\n\n\n2. Transforming Data\n\u00b6\n\n\nWith all of the \nas_of_times\n for this Experiment now computed, it's now possible to transform the input data into\nfeatures and labels as of all the required times.\n\n\nLabels\n\u00b6\n\n\nThe Experiment populates a 'labels' table using the following input:\n1. A query, provided by the user in the configuration file, that generates entity_ids and outcomes for a given\nas_of_date and label_timespan.\n\n\n\n\nEach as_of_date and label_timespan defined in temporal config\n\n\n\n\nFor instance, an inspections-style query (for the given timespan, return the entity and outcome of any matching\ninspections) would look like:\n\n\n    select\n    events.entity_id,\n    bool_or(outcome::bool)::integer as outcome\n    from events\n    where '{as_of_date}' <= outcome_date\n        and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}'\n        group by entity_id\n\n\n\n\nThis binary labels table is scoped to the entire Experiment, so all \nas_of_time\n (computed in step 1) and\n\nlabel_timespan\n (taken straight from \ntemporal_config\n) combinations are present. Additionally, the 'label_name' and\n'label_type' are also recorded with each row in the table.\n\n\nAt this point, the 'labels' table may not have entries for all entities and dates that need to be in a given matrix.\nHow these rows have their labels represented is up to the configured \ninclude_missing_labels_in_train_as\n value in the\nexperiment. This value is not processed when we generate the labels table, but later on when the matrix is built (see\n'Retrieving Data and Saving Completed Matrix')\n\n\nState Table\n\u00b6\n\n\nThe Experiment keeps track of the state of the entities. Based on the configuration, certain entities can be included\nor excluded for different time periods in feature imputation, creating matrices, or both.\n\n\nIn code, it does this by computing what it calls the 'sparse' state table for an experiment. This is a table with a\nboolean flag entry for every entity, as_of_time, and state. The structure of this table allows for state filtering\nbased on SQL conditions given by the user.\n\n\nBased on configuration, it can be created through one of three code paths:\n\n\n\n\n\n\nIf the user passes what we call a 'dense states' table, with the following structure: entity id/state/start/end, and\na list of state filters, this 'dense states' table holds time ranges that entities were in for specific states.\nWhen converting this to a sparse table, we take each as_of_time present in the Experiment, and for each known state\n(that is, the distinct values found in the 'dense states' table), see if there is any entry in the dense states table\nwith this state whose range overlaps with the as_of_time. If so, the entity is considered to be in that state as of that\ndate.\n\n\n\n\n\n\nIf the user passes what we call an 'entities' table, containing an entity_id, it will simply use all distinct\nentities present in said table, and mark them as 'active' for every as_of_time in the experiment. Any other columns are\nignored.\n\n\n\n\n\n\nIf the user passes a query, parameterized with an as of date, we will populate the table by running it for each\nas_of_date.\n\n\n\n\n\n\nThis table is created and exists until matrices are built, at which point it is considered unnecessary and then dropped.\n\n\nFeatures\n\u00b6\n\n\nEach provided \nfeature_aggregation\n configures the creation and population of several feature tables in the 'features'\nschema: one for each of the groups specified in the config, one that merges the groups together into one table, and one\nthat fills in null values from the merged table with imputed values based on imputation config.\n\n\nGenerating Aggregation SQL\n\u00b6\n\n\nTo generate the SQL that creates the pre-imputation table, the Experiment assembles building blocks from the feature\naggregation config, as well as the experiment's list of \nas_of_times\n:\n\n\n\n\nfrom_obj\n represents, well, the object of the FROM clause in the SQL query. Often this is just a table, but can be\nconfigured to be a subquery. This holds all the data that we want to aggregate into features\n\n\nEach \nas_of_time\n in the experiment and \ninterval\n in the \nfeature_aggregation\n is combined with the\n\nknowledge_date_column\n to create a WHERE clause representing a valid window of events to aggregate in the \nfrom_obj\n:\ne.g (\nwhere {knowledge_date_column} >= {as_of_time} - interval {interval}\n)\n\n\nEach \naggregate\n, \ncategorical\n, or \narray_categorical\n represents a SELECT clause. For aggregates, the \nquantity\n is\na column or SQL expression representing a numeric quantity present in the \nfrom_obj\n, and the \nmetrics\n are any number\nof aggregate functions we want to use. The aggregate function is applied to the quantity.\n\n\nEach \ngroup\n is a column applied to the GROUP BY clause. Generally this is 'entity_id', but higher-level groupings\n(for instance, 'zip_code') can be used as long as they can be rolled up to 'entity_id'.\n\n\n\n\nSo a simplified version of a typical query would look like:\n\n\nSELECT {group}, {metric}({quantity})\nFROM {from_obj}\nWHERE {knowledge_date_column} >= {as_of_time} - interval {interval}\nGROUP BY {group}\n\n\n\n\nWriting Group-wide Feature Tables\n\u00b6\n\n\nFor each \nas_of_time\n, the results from the generated query are written to a table whose name is prefixed with the\n\nprefix\n, and suffixed with the \ngroup\n. For instance, if the configuration specifies zipcode-level aggregates and\nentity-level aggregates, there will be a table for each, keyed on its group plus the as_of_date.\n\n\nMerging into Aggregation-wide Feature Tables\n\u00b6\n\n\nEach generated group table is combined into one representing the whole aggregation with a left join. Given that the\ngroups originally came from the same table (the \nfrom_obj\n of the aggregation) and therefore we know the zipcode for\neach entity, what we do now is create a table that would be keyed on entity and as_of_date, and contain all\nentity-level and zipcode-level aggregates from both tables. This aggregation-level table represents all of the features\nin the aggregation, pre-imputation. Its output location is generally \n{prefix}_aggregation\n\n\nImputing Values\n\u00b6\n\n\nA table that looks similar, but with imputed values is created. The state table from above is passed into collate as\nthe comprehensive set of entities and dates for which output should be generated, regardless if they exist in the\n\nfrom_obj\n. Each feature column has an imputation rule, inherited from some level of the feature definition. The\nimputation rules that are based on data (e.g. \nmean\n) use the rows from the \nas_of_time\n to produce the imputed value.\nIts output location is generally \n{prefix}_aggregation_imputed\n\n\nRecap\n\u00b6\n\n\nAt this point, we have at least three tables that are used to populate matrices:\n\n\n\n\nlabels\n with computed labels\n\n\ntmp_states_{experiment hash}\n that tracks what \nas_of_times\n each entity was in each state.\n\n\nA \nfeatures.{prefix}_aggregation_imputed\n table for each feature aggregation present in the experiment config.\n\n\n\n\n3. Building Matrices\n\u00b6\n\n\nAt this point, we have to build actual train and test matrices that can be processed by machine learning algorithms, save at the user's specified path, either on the local filesystem or s3 depending on the scheme portion of the path (e.g. \ns3://bucket-name/project_directory\n)\n\n\nFirst we have to figure out exactly what matrices we have to build. The split definitions from step 1 are a\ngood start -- they are our train and test splits -- but sometimes we also want to test different subsets of the data,\nlike feature groups (e.g. 'how does using group of features A perform against using all features?'). So there's a layer\nof iteration we introduce for each split, that may produce many more matrices.\n\n\nWhat do we iterate over?\n\n Feature List - All subsets of features that the user wants to cycle through. This is the end result of the feature\ngroup generation and mixing process, which is described more below.\n\n States - All configured \nstate_filters\n in the experiment config. These take the form of boolean SQL clauses that are\napplied to the sparse states table, and the purpose of this is to test different cohorts against each other. Generally\nthere is just one here.\n\n Label names - In theory we can take in different labels (e.g. complaints, sustained complaints) in the same\nexperiment. Right now there is no support for multiple label names, but the label name used is configurable through the\noptional 'label_config'->'name' config value\n\n Label types - In theory we can take in different label types (e.g. binary) in the same experiment. \nRight now this\nisn't done, there is one label type and it is hardcoded as 'binary'.\n\n\nFeature Lists\n\u00b6\n\n\nHow do we arrive at the feature lists? There are two pieces of config that are used: \nfeature group_definition\n and\n\nfeature_group_strategies\n. Feature group definitions are just ways to define logical blocks of features, most often\nfeatures that come from the same source, or describing a particular type of event. These groups within the experiment\nas a list of feature names, representing some subset of all potential features for the experiment. Feature group\nstrategies are ways to take feature groups and mix them together in various ways. The feature group strategies take\nthese subsets of features and convert them into another list of subsets of features, which is the final list iterated\nover to create different matrices.\n\n\nFeature Group Definition\n\u00b6\n\n\nFeature groups, at present, can be defined as either a \nprefix\n (the prefix of the feature name), a \ntable\n (the\nfeature table that the feature resides in), or \nall\n (all features).  Each argument is passed as a list, and each entry\nin the list is interpreted as a group. So, a feature group config of \n{'table': ['complaints_aggregate_imputed',\n'incidents_aggregate_imputed']}\n would result in two feature groups: one with all the features in\n\ncomplaints_aggregate_imputed\n, and one with all the features in \nincidents_aggregate_imputed\n. Note that this requires\na bit of knowledge on the user's part of how the feature table names will be constructed.\n\n\nprefix\n works on the prefix of the feature name as it exists in the database. So this also requires some knowledge of\nhow these get created. The general format is: \n{aggregation_prefix}_{group}_{timeperiod}_{quantity}\n, so with some\nknowledge the user can create groups with the aggregation's configured prefix (common), or the aggregations configured\nprefix + group (in case they want to compare, for instance, zip-code level features versus entity level features).\n\n\nall\n, with a single value of \nTrue\n, will include a feature group with all defined features. If no feature group\ndefinition is sent, this is the default.\n\n\nEither way, at the end of this process the experiment will be aware of some list of feature groups, even if the list is\njust length 1 with all features as one group.\n\n\nFeature Group Mixing\n\u00b6\n\n\nA few basic feature group mixing strategies are implemented: \nleave-one-in\n, \nleave-one-out\n, and \nall\n. These are sent\nin the experiment definition as a list, so different strategies can be tried in the same experiment. Each included\nstrategy will be applied to the list of feature groups from the previous step, to convert them into\n\n\nFor instance, 'leave-one-in' will cycle through each feature group, and for each one create a list of features that\njust represents that feature group, so for some matrices we would only use features from that particular group.\n\nleave-one-out\n does the opposite, for each feature group creating a list of features that includes all other feature\ngroups but that one. \nall\n just creates a list of features that represents all feature groups together.\n\n\nIteration and Matrix Creation\n\u00b6\n\n\nAt this point, matrices are created by looping through all train/test splits and data subsets (e.g. feature groups,\nstate definitions), grabbing the data corresponding to each from the database, and assembling that data into a design\nmatrix that is saved along with the metadata that defines it.\n\n\nAs an example, if the experiment defines 3 train/test splits (one test per train in this example, for simplicity), 3\nfeature groups that are mixed using the 'leave-one-out' and 'all' strategies, and 1 state definition, we'll expect 18\nmatrices to be saved: 9 splits after multiplying the time splits by the feature groups, and each one creating a train\nand test matrix.\n\n\nRetrieving Data and Saving Completed Matrix\n\u00b6\n\n\nHow do we get the data for an individual matrix out of the database?\n\n\n\n\n\n\nCreate an entity-date table for this specific matrix. There is some logic applied to decide what rows show up. There\nare two possible sets of rows that could show up.\n\n\n\n\n\n\nall valid entity dates\n. These dates come from the entity-date-state table for the experiment (populated using the\nrules defined in the 'cohort_config'), filtered down to the entity-date pairs that match both \nthe state filter and the\nlist of as-of-dates for this matrix\n.\n\n\n\n\n\n\nall labeled entity dates\n. These dates consist of all the valid entity dates from above, that also have an entry in\nthe labels table.\n\n\n\n\n\n\nIf the matrix is a test matrix, all valid entity dates will be present.\n\n\nIf the matrix is a train matrix, whether or not valid but unlabeled examples show up is decided by the\n\ninclude_missing_labels_in_train_as\n configuration value. If it is present in any form, these labels will be in the\nmatrix. Otherwise, they will be filtered out.\n\n\n\n\n\n\nWrite features data from tables to disk in CSV format using a COPY command, table by table. Each table is joined\nwith the matrix-specific entity-date table to only include the desired rows.\n\n\n\n\n\n\nWrite labels data to disk in CSV format using a COPY command. These labels will consist of the rows in the\nmatrix-specific entity-date table left joined to the labels table. Rows not present in the labels table will have their\nlabel filled in (either True or False) based on the value of the \ninclude_missing_labels_in_train_as\n configuration key.\n\n\n\n\n\n\nMerge the features and labels CSV files horizontally, in pandas. They are expected to be of the same shape, which is\nenforced by the entity-date table. The resulting matrix is indexed on \nentity_id\n and \nas_of_date\n, and then saved to\ndisk (in CSV format, more formats to come) along with its metadata: time, feature, label, index, and state information.\nalong with any user metadata the experiment config specified. The filename is decided by a hash of this metadata, and\nthe metadata is saved in a YAML file with the same hash and directory. The metadata is additionally added to a database table 'matrices'.\n\n\n\n\n\n\nMatrix metadata reference:\n- \nTrain matrix temporal info\n\n- \nTest matrix temporal info\n\n- \nFeature, label, index, cohort, user\nmetadata\n\n\nRecap\n\u00b6\n\n\nAt this point, all finished matrices and metadata will be saved under the \nproject_path\n supplied by the user to the\nExperiment constructor, in the subdirectory \nmatrices\n.\n\n\n4. Running Models\n\u00b6\n\n\nThe last phase of an Experiment run uses the completed design matrices to train, test, and evaluate classifiers. This procedure writes a lot of metadata to the 3 schemas: 'model_metadata', 'train_results', and 'test_results'.\n\n\nTrain\n\u00b6\n\n\nEach matrix marked for training is sent through the configured grid in the experiment's \ngrid_config\n. This works much\nlike the scikit-learn \nParameterGrid\n (and in fact uses it on the backend). It cycles through all of the classifiers\nand hyperparameter combinations contained herein, and calls \n.fit()\n with that train matrix. Any classifier that\nadheres to the scikit-learn \n.fit/.transform\n interface and is available in the Python environment will work here,\nwhether it is a standard scikit-learn classifier, a third-party library like XGBoost, or a custom-built one in the\ncalling repository (for instance, one that implements the problem domain's baseline heuristic algorithm for\ncomparison).  Metadata about the trained classifier is written to the \nmodel_metadata.models\n Postgres table. The trained model is saved to a filename with the model hash (see Model Hash section below).\n\n\nModel Groups\n\u00b6\n\n\nEach model is assigned a 'model group'. A model group represents a number of trained classifiers that we want to treat\nas equivalent by some criteria. By default, this is aimed at defining models which are equivalent across time splits,\nto make analyzing model stability easier. This default is accomplished with a set of 'model group keys' that includes\ndata about the classifier (module, hyperparameters), temporal intervals used to create the train matrix (label\ntimespan, training history, as-of-date frequency), and metadata describing the data in the train matrix (features and\nfeature groups, label name, cohort name). The user can override this set of \nmodel_group_keys\n in the experiment\ndefinition, with all of the default information plus other matrix metadata at their disposal (See end of 'Retrieving\nData and Saving Completed Matrix' section for more about matrix metadata). This data is stored in the \nmodel_metadata.model_groups\n table, along with a \nmodel_group_id\n that is used as a foreign key in the \nmodel_metadata.models\n table.\n\n\nModel Hash\n\u00b6\n\n\nEach trained model is assigned a hash, for the purpose of uniquely defining and caching the model. This hash is based\non the training matrix metadata, classifier path, hyperparameters (except those which concern execution and do not\naffect results of the classifier, such as \nn_jobs\n), and the given project path for the Experiment. This hash can be\nfound in each row of the \nmodel_metadata.models\n table. It is enforced as a unique key in the table.\n\n\nGlobal Feature Importance\n\u00b6\n\n\nThe training phase also writes global feature importances to the database, in the \ntrain_results.feature_importances\n table.\nA few methods are queried to attempt to compute feature importances:\n\n The bulk of these are computed using the trained model's \n.feature_importances_\n attribute, if it exists.\n\n For sklearn's \nSVC\n models with a linear kernel, the model's \n.coef_.squeeze()\n is used.\n\n For sklearn's LogisticRegression models, \nnp.exp(model.coef_).squeeze()\n is used.\n\n Otherwise, no feature importances are written.\n\n\nTest Matrix\n\u00b6\n\n\nFor each test matrix, predictions, individual importances, and the user-specified testing evaluation metrics are written to the 'test_results' schema. For each train matrix, predictions and the user-specified training evaluation metrics are written to the 'train_results' schema.\n\n\nPredictions\n\u00b6\n\n\nThe trained model's prediction probabilities (\npredict_proba()\n) are computed both for the matrix it was trained on and any testing matrices. The predictions for the training matrix are saved in \ntrain_results.train_predictions\n and those for the testing matrices are saved in the \ntest_results.test_predictions\n. More specifically, \npredict_proba\n returns the probabilities for each label (false and true), but in this case only the probabilities for the true label are saved in the \n{train or test}_predictions\n table. The \nentity_id\n and \nas_of_date\n are retrieved from the matrix's index, and stored in the database table along with the probability score, label value (if it has one), as well as other metadata.\n\n\nIndividual Feature Importance\n\u00b6\n\n\nFeature importances (of a configurable number of top features, defaulting to 5) for each prediction are computed and written to the \ntest_results.individual_importances\n table. Right now, there are no sophisticated calculation methods integrated into the experiment; simply the top 5 global feature importances for the model are copied to the \nindividual_importances\n table.\n\n\nMetrics\n\u00b6\n\n\nTriage allows for the computation of both testing set and training set evaluation metrics. Evaluation metrics, such as precision and recall at various thresholds, are written to either the \ntrain_results.train_evaluations\n table or the \ntest_results.test_evaluations\n. Triage defines a number of \nEvaluation Metrics\n metrics that can be addressed by name in the experiment definition, along with a list of thresholds and/or other parameters (such as the 'beta' value for fbeta) to iterate through. Thresholding is done either via absolute value (top k) or percentile by sorting the predictions and labels by the row's predicted probability score, with ties broken at random (the random seed can be passed in the config file to make this deterministic), and assigning the predicted value as True for those above the threshold. Note that the percentile thresholds are in terms of the population percentage, not a cutoff threshold for the predicted probability.\n\n\nSometimes test matrices may not have labels for every row, so it's worth mentioning here how that is handled and interacts with thresholding. Rows with missing labels are not considered in the metric calculations, and if some of these rows are in the top k of the test matrix, no more rows are taken from the rest of the list for consideration. So if the experiment is calculating precision at the top 100 rows, and 40 of the top 100 rows are missing a label, the precision will actually be calculated on the 60 of the top 100 rows that do have a label. To make the results of this more transparent for users, a few extra pieces of metadata are written to the evaluations table for each metric score.\n\n\n\n\nnum_labeled_examples\n - The number of rows in the test matrix that have labels\n\n\nnum_labeled_above_threshold\n - The number of rows above the configured threshold for this metric score that have\nlabels\n\n\nnum_positive_labels\n - The number of positive labels in the test matrix\n\n\n\n\nRecap\n\u00b6\n\n\nAt this point, the 'model_metadata', 'train_results', and 'test_results' database schemas are fully populated with data about models, model groups, predictions, feature importances, and evaluation metrics for the researcher to query. In addition, the trained model pickle files are saved in the configured project path. The experiment is considered finished.",
            "title": "Experiment Algorithm"
        },
        {
            "location": "/experiments/algorithm/#experiment-algorithm-deep-dive",
            "text": "This guide's purpose is to provide familiarity of the inner workings of a Triage Experiment to people with some\nexperience in data science and Python. A Triage Experiment is a highly structured way of defining the experimentation\nphase of a data science project. To those wondering whether this Experiment structure is flexible enough to fit their\nneeds, this should help.",
            "title": "Experiment Algorithm Deep Dive"
        },
        {
            "location": "/experiments/algorithm/#1-temporal-validation-setup",
            "text": "First, the given  temporal_config  section in the experiment definition is transformed into train and test splits,\nincluding  as_of_times  for each matrix.  We create these splits by figuring out the latest reasonable split time from the inputs, and moving backwards in time\nat the rate of the given  model_update_frequency , until we get to the earliest reasonable split time.  For each split, we create  as_of_times  by moving either backwards from the split time towards the max_training_history  (for train matrices) or forwards from the split time towards the  test_duration  (for test\nmatrices) at the provided  data_frequency .  Many of these configured values may be lists, in which case we generate the cross-product of all the possible values\nand generate more splits.  For a more detailed look at the temporal validation logic, see  Temporal Validation Deep Dive .  The train and test splits themselves are not used until the  Building Matrices  section, but a\nflat list of all computed  as_of_times  for all matrices needed in the experiment is used in the next section, Transforming Data .",
            "title": "1. Temporal Validation Setup"
        },
        {
            "location": "/experiments/algorithm/#2-transforming-data",
            "text": "With all of the  as_of_times  for this Experiment now computed, it's now possible to transform the input data into\nfeatures and labels as of all the required times.",
            "title": "2. Transforming Data"
        },
        {
            "location": "/experiments/algorithm/#labels",
            "text": "The Experiment populates a 'labels' table using the following input:\n1. A query, provided by the user in the configuration file, that generates entity_ids and outcomes for a given\nas_of_date and label_timespan.   Each as_of_date and label_timespan defined in temporal config   For instance, an inspections-style query (for the given timespan, return the entity and outcome of any matching\ninspections) would look like:      select\n    events.entity_id,\n    bool_or(outcome::bool)::integer as outcome\n    from events\n    where '{as_of_date}' <= outcome_date\n        and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}'\n        group by entity_id  This binary labels table is scoped to the entire Experiment, so all  as_of_time  (computed in step 1) and label_timespan  (taken straight from  temporal_config ) combinations are present. Additionally, the 'label_name' and\n'label_type' are also recorded with each row in the table.  At this point, the 'labels' table may not have entries for all entities and dates that need to be in a given matrix.\nHow these rows have their labels represented is up to the configured  include_missing_labels_in_train_as  value in the\nexperiment. This value is not processed when we generate the labels table, but later on when the matrix is built (see\n'Retrieving Data and Saving Completed Matrix')",
            "title": "Labels"
        },
        {
            "location": "/experiments/algorithm/#state-table",
            "text": "The Experiment keeps track of the state of the entities. Based on the configuration, certain entities can be included\nor excluded for different time periods in feature imputation, creating matrices, or both.  In code, it does this by computing what it calls the 'sparse' state table for an experiment. This is a table with a\nboolean flag entry for every entity, as_of_time, and state. The structure of this table allows for state filtering\nbased on SQL conditions given by the user.  Based on configuration, it can be created through one of three code paths:    If the user passes what we call a 'dense states' table, with the following structure: entity id/state/start/end, and\na list of state filters, this 'dense states' table holds time ranges that entities were in for specific states.\nWhen converting this to a sparse table, we take each as_of_time present in the Experiment, and for each known state\n(that is, the distinct values found in the 'dense states' table), see if there is any entry in the dense states table\nwith this state whose range overlaps with the as_of_time. If so, the entity is considered to be in that state as of that\ndate.    If the user passes what we call an 'entities' table, containing an entity_id, it will simply use all distinct\nentities present in said table, and mark them as 'active' for every as_of_time in the experiment. Any other columns are\nignored.    If the user passes a query, parameterized with an as of date, we will populate the table by running it for each\nas_of_date.    This table is created and exists until matrices are built, at which point it is considered unnecessary and then dropped.",
            "title": "State Table"
        },
        {
            "location": "/experiments/algorithm/#features",
            "text": "Each provided  feature_aggregation  configures the creation and population of several feature tables in the 'features'\nschema: one for each of the groups specified in the config, one that merges the groups together into one table, and one\nthat fills in null values from the merged table with imputed values based on imputation config.",
            "title": "Features"
        },
        {
            "location": "/experiments/algorithm/#generating-aggregation-sql",
            "text": "To generate the SQL that creates the pre-imputation table, the Experiment assembles building blocks from the feature\naggregation config, as well as the experiment's list of  as_of_times :   from_obj  represents, well, the object of the FROM clause in the SQL query. Often this is just a table, but can be\nconfigured to be a subquery. This holds all the data that we want to aggregate into features  Each  as_of_time  in the experiment and  interval  in the  feature_aggregation  is combined with the knowledge_date_column  to create a WHERE clause representing a valid window of events to aggregate in the  from_obj :\ne.g ( where {knowledge_date_column} >= {as_of_time} - interval {interval} )  Each  aggregate ,  categorical , or  array_categorical  represents a SELECT clause. For aggregates, the  quantity  is\na column or SQL expression representing a numeric quantity present in the  from_obj , and the  metrics  are any number\nof aggregate functions we want to use. The aggregate function is applied to the quantity.  Each  group  is a column applied to the GROUP BY clause. Generally this is 'entity_id', but higher-level groupings\n(for instance, 'zip_code') can be used as long as they can be rolled up to 'entity_id'.   So a simplified version of a typical query would look like:  SELECT {group}, {metric}({quantity})\nFROM {from_obj}\nWHERE {knowledge_date_column} >= {as_of_time} - interval {interval}\nGROUP BY {group}",
            "title": "Generating Aggregation SQL"
        },
        {
            "location": "/experiments/algorithm/#writing-group-wide-feature-tables",
            "text": "For each  as_of_time , the results from the generated query are written to a table whose name is prefixed with the prefix , and suffixed with the  group . For instance, if the configuration specifies zipcode-level aggregates and\nentity-level aggregates, there will be a table for each, keyed on its group plus the as_of_date.",
            "title": "Writing Group-wide Feature Tables"
        },
        {
            "location": "/experiments/algorithm/#merging-into-aggregation-wide-feature-tables",
            "text": "Each generated group table is combined into one representing the whole aggregation with a left join. Given that the\ngroups originally came from the same table (the  from_obj  of the aggregation) and therefore we know the zipcode for\neach entity, what we do now is create a table that would be keyed on entity and as_of_date, and contain all\nentity-level and zipcode-level aggregates from both tables. This aggregation-level table represents all of the features\nin the aggregation, pre-imputation. Its output location is generally  {prefix}_aggregation",
            "title": "Merging into Aggregation-wide Feature Tables"
        },
        {
            "location": "/experiments/algorithm/#imputing-values",
            "text": "A table that looks similar, but with imputed values is created. The state table from above is passed into collate as\nthe comprehensive set of entities and dates for which output should be generated, regardless if they exist in the from_obj . Each feature column has an imputation rule, inherited from some level of the feature definition. The\nimputation rules that are based on data (e.g.  mean ) use the rows from the  as_of_time  to produce the imputed value.\nIts output location is generally  {prefix}_aggregation_imputed",
            "title": "Imputing Values"
        },
        {
            "location": "/experiments/algorithm/#recap",
            "text": "At this point, we have at least three tables that are used to populate matrices:   labels  with computed labels  tmp_states_{experiment hash}  that tracks what  as_of_times  each entity was in each state.  A  features.{prefix}_aggregation_imputed  table for each feature aggregation present in the experiment config.",
            "title": "Recap"
        },
        {
            "location": "/experiments/algorithm/#3-building-matrices",
            "text": "At this point, we have to build actual train and test matrices that can be processed by machine learning algorithms, save at the user's specified path, either on the local filesystem or s3 depending on the scheme portion of the path (e.g.  s3://bucket-name/project_directory )  First we have to figure out exactly what matrices we have to build. The split definitions from step 1 are a\ngood start -- they are our train and test splits -- but sometimes we also want to test different subsets of the data,\nlike feature groups (e.g. 'how does using group of features A perform against using all features?'). So there's a layer\nof iteration we introduce for each split, that may produce many more matrices.  What do we iterate over?  Feature List - All subsets of features that the user wants to cycle through. This is the end result of the feature\ngroup generation and mixing process, which is described more below.  States - All configured  state_filters  in the experiment config. These take the form of boolean SQL clauses that are\napplied to the sparse states table, and the purpose of this is to test different cohorts against each other. Generally\nthere is just one here.  Label names - In theory we can take in different labels (e.g. complaints, sustained complaints) in the same\nexperiment. Right now there is no support for multiple label names, but the label name used is configurable through the\noptional 'label_config'->'name' config value  Label types - In theory we can take in different label types (e.g. binary) in the same experiment.  Right now this\nisn't done, there is one label type and it is hardcoded as 'binary'.",
            "title": "3. Building Matrices"
        },
        {
            "location": "/experiments/algorithm/#feature-lists",
            "text": "How do we arrive at the feature lists? There are two pieces of config that are used:  feature group_definition  and feature_group_strategies . Feature group definitions are just ways to define logical blocks of features, most often\nfeatures that come from the same source, or describing a particular type of event. These groups within the experiment\nas a list of feature names, representing some subset of all potential features for the experiment. Feature group\nstrategies are ways to take feature groups and mix them together in various ways. The feature group strategies take\nthese subsets of features and convert them into another list of subsets of features, which is the final list iterated\nover to create different matrices.",
            "title": "Feature Lists"
        },
        {
            "location": "/experiments/algorithm/#feature-group-definition",
            "text": "Feature groups, at present, can be defined as either a  prefix  (the prefix of the feature name), a  table  (the\nfeature table that the feature resides in), or  all  (all features).  Each argument is passed as a list, and each entry\nin the list is interpreted as a group. So, a feature group config of  {'table': ['complaints_aggregate_imputed',\n'incidents_aggregate_imputed']}  would result in two feature groups: one with all the features in complaints_aggregate_imputed , and one with all the features in  incidents_aggregate_imputed . Note that this requires\na bit of knowledge on the user's part of how the feature table names will be constructed.  prefix  works on the prefix of the feature name as it exists in the database. So this also requires some knowledge of\nhow these get created. The general format is:  {aggregation_prefix}_{group}_{timeperiod}_{quantity} , so with some\nknowledge the user can create groups with the aggregation's configured prefix (common), or the aggregations configured\nprefix + group (in case they want to compare, for instance, zip-code level features versus entity level features).  all , with a single value of  True , will include a feature group with all defined features. If no feature group\ndefinition is sent, this is the default.  Either way, at the end of this process the experiment will be aware of some list of feature groups, even if the list is\njust length 1 with all features as one group.",
            "title": "Feature Group Definition"
        },
        {
            "location": "/experiments/algorithm/#feature-group-mixing",
            "text": "A few basic feature group mixing strategies are implemented:  leave-one-in ,  leave-one-out , and  all . These are sent\nin the experiment definition as a list, so different strategies can be tried in the same experiment. Each included\nstrategy will be applied to the list of feature groups from the previous step, to convert them into  For instance, 'leave-one-in' will cycle through each feature group, and for each one create a list of features that\njust represents that feature group, so for some matrices we would only use features from that particular group. leave-one-out  does the opposite, for each feature group creating a list of features that includes all other feature\ngroups but that one.  all  just creates a list of features that represents all feature groups together.",
            "title": "Feature Group Mixing"
        },
        {
            "location": "/experiments/algorithm/#iteration-and-matrix-creation",
            "text": "At this point, matrices are created by looping through all train/test splits and data subsets (e.g. feature groups,\nstate definitions), grabbing the data corresponding to each from the database, and assembling that data into a design\nmatrix that is saved along with the metadata that defines it.  As an example, if the experiment defines 3 train/test splits (one test per train in this example, for simplicity), 3\nfeature groups that are mixed using the 'leave-one-out' and 'all' strategies, and 1 state definition, we'll expect 18\nmatrices to be saved: 9 splits after multiplying the time splits by the feature groups, and each one creating a train\nand test matrix.",
            "title": "Iteration and Matrix Creation"
        },
        {
            "location": "/experiments/algorithm/#retrieving-data-and-saving-completed-matrix",
            "text": "How do we get the data for an individual matrix out of the database?    Create an entity-date table for this specific matrix. There is some logic applied to decide what rows show up. There\nare two possible sets of rows that could show up.    all valid entity dates . These dates come from the entity-date-state table for the experiment (populated using the\nrules defined in the 'cohort_config'), filtered down to the entity-date pairs that match both  the state filter and the\nlist of as-of-dates for this matrix .    all labeled entity dates . These dates consist of all the valid entity dates from above, that also have an entry in\nthe labels table.    If the matrix is a test matrix, all valid entity dates will be present.  If the matrix is a train matrix, whether or not valid but unlabeled examples show up is decided by the include_missing_labels_in_train_as  configuration value. If it is present in any form, these labels will be in the\nmatrix. Otherwise, they will be filtered out.    Write features data from tables to disk in CSV format using a COPY command, table by table. Each table is joined\nwith the matrix-specific entity-date table to only include the desired rows.    Write labels data to disk in CSV format using a COPY command. These labels will consist of the rows in the\nmatrix-specific entity-date table left joined to the labels table. Rows not present in the labels table will have their\nlabel filled in (either True or False) based on the value of the  include_missing_labels_in_train_as  configuration key.    Merge the features and labels CSV files horizontally, in pandas. They are expected to be of the same shape, which is\nenforced by the entity-date table. The resulting matrix is indexed on  entity_id  and  as_of_date , and then saved to\ndisk (in CSV format, more formats to come) along with its metadata: time, feature, label, index, and state information.\nalong with any user metadata the experiment config specified. The filename is decided by a hash of this metadata, and\nthe metadata is saved in a YAML file with the same hash and directory. The metadata is additionally added to a database table 'matrices'.    Matrix metadata reference:\n-  Train matrix temporal info \n-  Test matrix temporal info \n-  Feature, label, index, cohort, user\nmetadata",
            "title": "Retrieving Data and Saving Completed Matrix"
        },
        {
            "location": "/experiments/algorithm/#recap_1",
            "text": "At this point, all finished matrices and metadata will be saved under the  project_path  supplied by the user to the\nExperiment constructor, in the subdirectory  matrices .",
            "title": "Recap"
        },
        {
            "location": "/experiments/algorithm/#4-running-models",
            "text": "The last phase of an Experiment run uses the completed design matrices to train, test, and evaluate classifiers. This procedure writes a lot of metadata to the 3 schemas: 'model_metadata', 'train_results', and 'test_results'.",
            "title": "4. Running Models"
        },
        {
            "location": "/experiments/algorithm/#train",
            "text": "Each matrix marked for training is sent through the configured grid in the experiment's  grid_config . This works much\nlike the scikit-learn  ParameterGrid  (and in fact uses it on the backend). It cycles through all of the classifiers\nand hyperparameter combinations contained herein, and calls  .fit()  with that train matrix. Any classifier that\nadheres to the scikit-learn  .fit/.transform  interface and is available in the Python environment will work here,\nwhether it is a standard scikit-learn classifier, a third-party library like XGBoost, or a custom-built one in the\ncalling repository (for instance, one that implements the problem domain's baseline heuristic algorithm for\ncomparison).  Metadata about the trained classifier is written to the  model_metadata.models  Postgres table. The trained model is saved to a filename with the model hash (see Model Hash section below).",
            "title": "Train"
        },
        {
            "location": "/experiments/algorithm/#model-groups",
            "text": "Each model is assigned a 'model group'. A model group represents a number of trained classifiers that we want to treat\nas equivalent by some criteria. By default, this is aimed at defining models which are equivalent across time splits,\nto make analyzing model stability easier. This default is accomplished with a set of 'model group keys' that includes\ndata about the classifier (module, hyperparameters), temporal intervals used to create the train matrix (label\ntimespan, training history, as-of-date frequency), and metadata describing the data in the train matrix (features and\nfeature groups, label name, cohort name). The user can override this set of  model_group_keys  in the experiment\ndefinition, with all of the default information plus other matrix metadata at their disposal (See end of 'Retrieving\nData and Saving Completed Matrix' section for more about matrix metadata). This data is stored in the  model_metadata.model_groups  table, along with a  model_group_id  that is used as a foreign key in the  model_metadata.models  table.",
            "title": "Model Groups"
        },
        {
            "location": "/experiments/algorithm/#model-hash",
            "text": "Each trained model is assigned a hash, for the purpose of uniquely defining and caching the model. This hash is based\non the training matrix metadata, classifier path, hyperparameters (except those which concern execution and do not\naffect results of the classifier, such as  n_jobs ), and the given project path for the Experiment. This hash can be\nfound in each row of the  model_metadata.models  table. It is enforced as a unique key in the table.",
            "title": "Model Hash"
        },
        {
            "location": "/experiments/algorithm/#global-feature-importance",
            "text": "The training phase also writes global feature importances to the database, in the  train_results.feature_importances  table.\nA few methods are queried to attempt to compute feature importances:  The bulk of these are computed using the trained model's  .feature_importances_  attribute, if it exists.  For sklearn's  SVC  models with a linear kernel, the model's  .coef_.squeeze()  is used.  For sklearn's LogisticRegression models,  np.exp(model.coef_).squeeze()  is used.  Otherwise, no feature importances are written.",
            "title": "Global Feature Importance"
        },
        {
            "location": "/experiments/algorithm/#test-matrix",
            "text": "For each test matrix, predictions, individual importances, and the user-specified testing evaluation metrics are written to the 'test_results' schema. For each train matrix, predictions and the user-specified training evaluation metrics are written to the 'train_results' schema.",
            "title": "Test Matrix"
        },
        {
            "location": "/experiments/algorithm/#predictions",
            "text": "The trained model's prediction probabilities ( predict_proba() ) are computed both for the matrix it was trained on and any testing matrices. The predictions for the training matrix are saved in  train_results.train_predictions  and those for the testing matrices are saved in the  test_results.test_predictions . More specifically,  predict_proba  returns the probabilities for each label (false and true), but in this case only the probabilities for the true label are saved in the  {train or test}_predictions  table. The  entity_id  and  as_of_date  are retrieved from the matrix's index, and stored in the database table along with the probability score, label value (if it has one), as well as other metadata.",
            "title": "Predictions"
        },
        {
            "location": "/experiments/algorithm/#individual-feature-importance",
            "text": "Feature importances (of a configurable number of top features, defaulting to 5) for each prediction are computed and written to the  test_results.individual_importances  table. Right now, there are no sophisticated calculation methods integrated into the experiment; simply the top 5 global feature importances for the model are copied to the  individual_importances  table.",
            "title": "Individual Feature Importance"
        },
        {
            "location": "/experiments/algorithm/#metrics",
            "text": "Triage allows for the computation of both testing set and training set evaluation metrics. Evaluation metrics, such as precision and recall at various thresholds, are written to either the  train_results.train_evaluations  table or the  test_results.test_evaluations . Triage defines a number of  Evaluation Metrics  metrics that can be addressed by name in the experiment definition, along with a list of thresholds and/or other parameters (such as the 'beta' value for fbeta) to iterate through. Thresholding is done either via absolute value (top k) or percentile by sorting the predictions and labels by the row's predicted probability score, with ties broken at random (the random seed can be passed in the config file to make this deterministic), and assigning the predicted value as True for those above the threshold. Note that the percentile thresholds are in terms of the population percentage, not a cutoff threshold for the predicted probability.  Sometimes test matrices may not have labels for every row, so it's worth mentioning here how that is handled and interacts with thresholding. Rows with missing labels are not considered in the metric calculations, and if some of these rows are in the top k of the test matrix, no more rows are taken from the rest of the list for consideration. So if the experiment is calculating precision at the top 100 rows, and 40 of the top 100 rows are missing a label, the precision will actually be calculated on the 60 of the top 100 rows that do have a label. To make the results of this more transparent for users, a few extra pieces of metadata are written to the evaluations table for each metric score.   num_labeled_examples  - The number of rows in the test matrix that have labels  num_labeled_above_threshold  - The number of rows above the configured threshold for this metric score that have\nlabels  num_positive_labels  - The number of positive labels in the test matrix",
            "title": "Metrics"
        },
        {
            "location": "/experiments/algorithm/#recap_2",
            "text": "At this point, the 'model_metadata', 'train_results', and 'test_results' database schemas are fully populated with data about models, model groups, predictions, feature importances, and evaluation metrics for the researcher to query. In addition, the trained model pickle files are saved in the configured project path. The experiment is considered finished.",
            "title": "Recap"
        },
        {
            "location": "/experiments/architecture/",
            "text": "Experiment Architecture\n\u00b6\n\n\nThis doc is coming soon",
            "title": "Experiment Architecture"
        },
        {
            "location": "/experiments/architecture/#experiment-architecture",
            "text": "This doc is coming soon",
            "title": "Experiment Architecture"
        },
        {
            "location": "/experiments/upgrade-to-v5/",
            "text": "Upgrading your experiment configuration to v5\n\u00b6\n\n\nThis document details the steps needed to update a triage v3 or v4 configuration to v5, mimicing the old behavior (as opposed to taking advantage of new options) as much as possible.\n\n\nIn the experiment configuration v5, several things were changed:\n\n\n\n\nstate_config\n becomes \ncohort_config\n, and receives new options\n\n\nlabel_config\n is changed to take a parameterized query\n\n\nmodel_group_keys\n is changed to have more robust defaults, and values specified in the config file act as overrides for the defaults instead of additions to them.\n\n\n\n\nstate_config -> cohort_config\n\u00b6\n\n\nUpgrading the state config is fairly straightforward, as no functionality was removed. The key at which the state table-based configuration can be passed has changed. Before it resided at the top-level \nstate_config\n key, whereas now it is in the optional \ndense_states\n key within the top-level \ncohort_config\n key.\n\n\nOld:\n\n\nstate_config:\n    table_name: 'states'\n    state_filters:\n        - 'state_one AND state_two'\n        - '(state_one OR state_two) AND state_three'\n\n\n\n\nNew:\n\n\ncohort_config:\n   dense_states:\n        table_name: 'states'\n        state_filters:\n        - 'state_one AND state_two'\n        - '(state_one OR state_two) AND state_three'\n\n\n\n\nlabel_config\n\u00b6\n\n\nThe label config has had functionality changed, so there is more conversion that needs to happen. Instead of taking in an 'events' table and making assumptions suitable for inspections tasks based on that table, for transparency and flexibility this now takes a parameterized query, as well as an optional \ninclude_missing_labels_in_train_as\n boolean. Leaving out this boolean value reproduces the inspections behavior (missing labels are treated as null), so to upgrade old configurations it is not needed.\n\n\nOld:\n\n\nevents_table: 'events'\n\n\n\n\nNew:\n\n\nlabel_config:\n    query: |\n        select\n        events.entity_id,\n        bool_or(outcome::bool)::integer as outcome\n        from events\n        where '{as_of_date}' <= outcome_date\n            and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}'\n            group by entity_id\n\n\n\n\nmodel_group_keys\n\u00b6\n\n\nThe model group configuration was changed quite a bit. Before, the Experiment defined a few default grouping keys and would treat anything included in the config as additional. In practice, there were many keys that were almost always included as additional model group keys, and these are now default. There are also other keys that generally make sense if certain things are iterated on (e.g. feature groups). The goal is for most projects to simply leave out this configuration value entirely. If possible, this is the recommended route to go. But for the purposes of this guide, this change should duplicate the old behavior exactly.\n\n\nOld (empty, using defaults):\n\n\n\n\n\nNew:\n\n\nmodel_group_keys: ['class_path', 'parameters', 'feature_names']\n\n\n\n\nOld (more standard in practice, adding some temporal parameters):\n\n\nmodel_group_keys: ['label_timespan', 'as_of_date_frequency', 'max_training_history']\n\n\n\n\nNew:\n\n\nmodel_group_keys: ['class_path', 'parameters', 'feature_names', 'label_timespan', 'as_of_date_frequency', 'max_training_history']\n\n\n\n\nUpgrading the experiment config version\n\u00b6\n\n\nAt this point, you should be able to bump the top-level experiment config version to v5:\n\n\nOld:\n\n\nconfig_version: 'v4'\n\n\n\n\nNew:\n\n\nconfig_version: 'v5'",
            "title": "Upgrading Experiment to v5"
        },
        {
            "location": "/experiments/upgrade-to-v5/#upgrading-your-experiment-configuration-to-v5",
            "text": "This document details the steps needed to update a triage v3 or v4 configuration to v5, mimicing the old behavior (as opposed to taking advantage of new options) as much as possible.  In the experiment configuration v5, several things were changed:   state_config  becomes  cohort_config , and receives new options  label_config  is changed to take a parameterized query  model_group_keys  is changed to have more robust defaults, and values specified in the config file act as overrides for the defaults instead of additions to them.",
            "title": "Upgrading your experiment configuration to v5"
        },
        {
            "location": "/experiments/upgrade-to-v5/#state_config-cohort_config",
            "text": "Upgrading the state config is fairly straightforward, as no functionality was removed. The key at which the state table-based configuration can be passed has changed. Before it resided at the top-level  state_config  key, whereas now it is in the optional  dense_states  key within the top-level  cohort_config  key.  Old:  state_config:\n    table_name: 'states'\n    state_filters:\n        - 'state_one AND state_two'\n        - '(state_one OR state_two) AND state_three'  New:  cohort_config:\n   dense_states:\n        table_name: 'states'\n        state_filters:\n        - 'state_one AND state_two'\n        - '(state_one OR state_two) AND state_three'",
            "title": "state_config -&gt; cohort_config"
        },
        {
            "location": "/experiments/upgrade-to-v5/#label_config",
            "text": "The label config has had functionality changed, so there is more conversion that needs to happen. Instead of taking in an 'events' table and making assumptions suitable for inspections tasks based on that table, for transparency and flexibility this now takes a parameterized query, as well as an optional  include_missing_labels_in_train_as  boolean. Leaving out this boolean value reproduces the inspections behavior (missing labels are treated as null), so to upgrade old configurations it is not needed.  Old:  events_table: 'events'  New:  label_config:\n    query: |\n        select\n        events.entity_id,\n        bool_or(outcome::bool)::integer as outcome\n        from events\n        where '{as_of_date}' <= outcome_date\n            and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}'\n            group by entity_id",
            "title": "label_config"
        },
        {
            "location": "/experiments/upgrade-to-v5/#model_group_keys",
            "text": "The model group configuration was changed quite a bit. Before, the Experiment defined a few default grouping keys and would treat anything included in the config as additional. In practice, there were many keys that were almost always included as additional model group keys, and these are now default. There are also other keys that generally make sense if certain things are iterated on (e.g. feature groups). The goal is for most projects to simply leave out this configuration value entirely. If possible, this is the recommended route to go. But for the purposes of this guide, this change should duplicate the old behavior exactly.  Old (empty, using defaults):   New:  model_group_keys: ['class_path', 'parameters', 'feature_names']  Old (more standard in practice, adding some temporal parameters):  model_group_keys: ['label_timespan', 'as_of_date_frequency', 'max_training_history']  New:  model_group_keys: ['class_path', 'parameters', 'feature_names', 'label_timespan', 'as_of_date_frequency', 'max_training_history']",
            "title": "model_group_keys"
        },
        {
            "location": "/experiments/upgrade-to-v5/#upgrading-the-experiment-config-version",
            "text": "At this point, you should be able to bump the top-level experiment config version to v5:  Old:  config_version: 'v4'  New:  config_version: 'v5'",
            "title": "Upgrading the experiment config version"
        }
    ]
}