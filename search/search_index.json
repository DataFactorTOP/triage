{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Triage # What is Triage? # Triage is an open source machine learning toolkit to help data scientists, machine learning developers, and analysts quickly prototype, build and evaluate end-to-end predictive risk modeling systems for public policy and social good problem. While many tools (sklearn, keras, pytorch, etc.) exist to build ML models, an end-to-end project requires a lot more than just building models. Developing data science systems requires making many design decisions that need to match with how the system is going to be used. These choices then get turned into modeling choices and code. Triage lets you focus on the problem you\u2019re solving and guides you through design choices you need to make at each step of the machine learning pipeline. How to get started with Triage? # Go through a quick tutorial with sample data # Get started with your own project and data # Background # Triage was initially developed at the University of Chicago's Center For Data Science and Public Policy and is now being maintained and enhanced at Carnegie Mellon University.","title":"Home"},{"location":"#triage","text":"","title":"Triage"},{"location":"#what-is-triage","text":"Triage is an open source machine learning toolkit to help data scientists, machine learning developers, and analysts quickly prototype, build and evaluate end-to-end predictive risk modeling systems for public policy and social good problem. While many tools (sklearn, keras, pytorch, etc.) exist to build ML models, an end-to-end project requires a lot more than just building models. Developing data science systems requires making many design decisions that need to match with how the system is going to be used. These choices then get turned into modeling choices and code. Triage lets you focus on the problem you\u2019re solving and guides you through design choices you need to make at each step of the machine learning pipeline.","title":"What is Triage?"},{"location":"#how-to-get-started-with-triage","text":"","title":"How to get started with Triage?"},{"location":"#go-through-a-quick-tutorial-with-sample-data","text":"","title":"Go through a quick tutorial with sample data"},{"location":"#get-started-with-your-own-project-and-data","text":"","title":"Get started with your own project and data"},{"location":"#background","text":"Triage was initially developed at the University of Chicago's Center For Data Science and Public Policy and is now being maintained and enhanced at Carnegie Mellon University.","title":"Background"},{"location":"quickstart/","text":"Quickstart guide to using Triage # 1. Install Triage # Triage can be installed using pip or through python setup.py. It requires Python 3+ and access to a postgresql database. Ideally you have full access to a databse so triage can create additional schemas inside that it needs to store metadata, predictions, and evaluation metrics. We also recommend installing triage inside a python virtual environment for your project so you don't have any conflicts with other packages installed on the machine. You can use virutalenv or pyenv to do that. If you use pyenv (be sure your default python is 3+): $ pyenv virtualenv triage-env $ pyenv activate triage-env ( triage-env ) $ pip install triage If you use virtualenv (be sure your default python is 3+): $ virtualenv triage-env $ . triage-env/bin/activate ( triage-env ) $ pip install triage 2. Structure your data # The simplest way to start is to structure your data as a series of events connected to your entity of interest (people, organization, business, etc.) that take place at a certain time. Each row of the data will be an event . Each event will have some event_id , and an entity_id to link it to the entity it happened to, a date, as well as additional attributes about the event (type for example) and the entity ( age , gender , race , etc.). A sample row might look like: event_id, entity_id, date, event_attribute (type), entity_attribute (age), entity_attribute (gender), ... 121, 19334, 1/1/2013, Placement, 12, Male, ... Triage needs a field named entity_id (that needs to be of type integer) to refer to the primary entities of interest in our project. 3. Set up Triage configuration files # The configuration file sets up the modeling process to mirror the operational scenario the models will be used in. This involved defining the cohort to train/predict on, the outcome we're predicting, how far out we're predicting, how often will the model be updated, how often will the predicted list be used for interventions, what are the resources available to intervene to define the evaluation metric, etc. A lot of details about each section of the configration file can be found here , but for the moment we'll start with the much simplier configuration file below: config_version : 'v7' model_comment : 'quickstart_test_run' temporal_config : label_timespans : [ '<< YOUR_VALUE_HERE >>' ] label_config : query : | << YOUR_VALUE_HERE >> name : 'quickstart_label' feature_aggregations : - prefix : 'qstest' from_obj : '<< YOUR_VALUE_HERE >>' knowledge_date_column : '<< YOUR_VALUE_HERE >>' aggregates_imputation : count : type : 'zero_noflag' aggregates : - quantity : total : \"*\" metrics : - 'count' intervals : [ 'all' ] groups : - 'entity_id' model_grid_preset : 'quickstart' scoring : testing_metric_groups : - metrics : [ precision@ ] thresholds : percentiles : [ 1 ] training_metric_groups : - metrics : [ precision@ ] thresholds : percentiles : [ 1 ] Copy that code block into your text editor of choice and save it as something like quickstart-config.yaml in your working directory for your project. You'll need to fill out the sections marked << YOUR_VALUE_HERE >> with values appropriate to your project. The configuration file has a lot of sections. As a first pass, we will infer a lot of the parameters that are needed in there and use defaults for others. The primary parameters to specify (for now) are: TIMECHOP config: This sets up temporal parameters for training and testing models. The key things to set up here are your prediction horizon/timespan (how far out in the future do you want to predict?). For example, if you want to predict an outcome within one year, you would set label_timespans = '12month' . See our guide to Temporal Validation LABEL config: This is a sql query that defines what the outcome of interest is. The query must return two columns: entity_id (an integer) and outcome (with integer label values of 0 and 1 ), based on a given as_of_date and label_timespan (you can use these parameters in your query by surrounding them with curly braces as in the example below). See our guide to Labels . For example, if your data was in a table called semantic.events containing columns entity_id , event_date , and label , this query could simply be: select entity_id, max(label) as outcome from semantic.events where '{as_of_date}'::timestamp <= event_date and event_date < '{as_of_date}'::timestamp + interval '{label_timespan}' FEATURE config: This is where we define different aggregate features/attributes/variables to be created and used in our machine learning models. We need at least one feature specified here. For the purposes of the quickstart, let's just take the count of all events before the modeling date. In the template, you can simply fill in from_obj with the schema.table_name where your data can be found (but this can also be a more complex query in general) and knowledge_date_column with that table's date column. MODEL_GRID_PRESET config: Which models and hyperparameters we want to try in this run. We can start with quickstart that will run a quick model grid to test if everything works. Additionally, we will need a database credential file that contains the name of the database, server, username, and password to use to connect to it: # Connecting to the database requires a configuration file like this one but # named database.yaml host : address.of.database.server user : user_name db : database_name pass : user_password port : connection_port (often 5432) Copy this into a separate text file, fill in your values and save it as database.yaml in the working directory where you'll be running triage. Note, however, that if you have a DATABASE_URL environment variable set, triage will use this by default as well. 4. Run Triage # An overview of different steps that take place when you run Triage is here Validate the configuration files by running: triage experiment config.yaml --project-path '/project_directory' --validate-only Run triage triage experiment config.yaml --project-path '/project_directory' For this quickstart, you shouldn't need much free disk space, but note that in general your project path will contain both data matrices and trained model objects, so will need to have ample free space (you can also specify a location in S3 if you don't want to store the files locally). If you want a bit more detail or documentation, a good overview of running an experiment in triage is here . 5. Look at results generated by Triage # Once the feature/cohort/label/matrix building is done and the experiment has moved onto modeling, check out the model_metadata.models and test_results.evaluations tables as data starts to come in. Here are a couple of quick queries to help get you started: Tables in the model_metadata schema have some general information about experiments that you've run and the models they created. The quickstart model grid preset should have built 3 models. You can check that with: select model_id , model_group_id , model_type from model_metadata . models ; This should give you a result that looks something like: model_id model_group_id model_type 1 1 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression 2 2 sklearn.tree.DecisionTreeClassifier 3 3 sklearn.dummy.DummyClassifier If you want to see predictions for individual entities, you can check out test_results.predictions , for instance: select model_id , entity_id , as_of_date , score , label_value from test_results . predictions limit 5 ; This will give you something like: model_id entity_id as_of_date score label_value 1 15596 2017-09-29 00:00:00 0.21884 0 2 15596 2017-09-29 00:00:00 0.22831 0 3 15596 2017-09-29 00:00:00 0.25195 0 Finally, test_results.evaluations holds some aggregate information on model performance: select model_id , metric , parameter , stochastic_value from test_results . evaluations order by model_id , metric , parameter ; Feel free to explore some of the other tables in these schemas (note that there's also a train_results schema with performance on the training set as well as feature importances, where defined). In a more complete modeling run, you could audition with jupyter notebooks to help you select the best-performing model specifications from a wide variety of options (see the overview of model selection and tutorial audition notebook ) and postmodeling to delve deeper into understanding these models (see the README and tutorial postmodeling notebook ). 6. Iterate and Explore # Now that you have triage running, continue onto the suggested project workflow for some tips about how to iterate and tune the pipeline for your project. Alternatively, if you'd like more of a guided tour with sample data, check out our dirty duck tutorial .","title":"Quickstart guide"},{"location":"quickstart/#quickstart-guide-to-using-triage","text":"","title":"Quickstart guide to using Triage"},{"location":"quickstart/#1-install-triage","text":"Triage can be installed using pip or through python setup.py. It requires Python 3+ and access to a postgresql database. Ideally you have full access to a databse so triage can create additional schemas inside that it needs to store metadata, predictions, and evaluation metrics. We also recommend installing triage inside a python virtual environment for your project so you don't have any conflicts with other packages installed on the machine. You can use virutalenv or pyenv to do that. If you use pyenv (be sure your default python is 3+): $ pyenv virtualenv triage-env $ pyenv activate triage-env ( triage-env ) $ pip install triage If you use virtualenv (be sure your default python is 3+): $ virtualenv triage-env $ . triage-env/bin/activate ( triage-env ) $ pip install triage","title":"1. Install Triage"},{"location":"quickstart/#2-structure-your-data","text":"The simplest way to start is to structure your data as a series of events connected to your entity of interest (people, organization, business, etc.) that take place at a certain time. Each row of the data will be an event . Each event will have some event_id , and an entity_id to link it to the entity it happened to, a date, as well as additional attributes about the event (type for example) and the entity ( age , gender , race , etc.). A sample row might look like: event_id, entity_id, date, event_attribute (type), entity_attribute (age), entity_attribute (gender), ... 121, 19334, 1/1/2013, Placement, 12, Male, ... Triage needs a field named entity_id (that needs to be of type integer) to refer to the primary entities of interest in our project.","title":"2. Structure your data"},{"location":"quickstart/#3-set-up-triage-configuration-files","text":"The configuration file sets up the modeling process to mirror the operational scenario the models will be used in. This involved defining the cohort to train/predict on, the outcome we're predicting, how far out we're predicting, how often will the model be updated, how often will the predicted list be used for interventions, what are the resources available to intervene to define the evaluation metric, etc. A lot of details about each section of the configration file can be found here , but for the moment we'll start with the much simplier configuration file below: config_version : 'v7' model_comment : 'quickstart_test_run' temporal_config : label_timespans : [ '<< YOUR_VALUE_HERE >>' ] label_config : query : | << YOUR_VALUE_HERE >> name : 'quickstart_label' feature_aggregations : - prefix : 'qstest' from_obj : '<< YOUR_VALUE_HERE >>' knowledge_date_column : '<< YOUR_VALUE_HERE >>' aggregates_imputation : count : type : 'zero_noflag' aggregates : - quantity : total : \"*\" metrics : - 'count' intervals : [ 'all' ] groups : - 'entity_id' model_grid_preset : 'quickstart' scoring : testing_metric_groups : - metrics : [ precision@ ] thresholds : percentiles : [ 1 ] training_metric_groups : - metrics : [ precision@ ] thresholds : percentiles : [ 1 ] Copy that code block into your text editor of choice and save it as something like quickstart-config.yaml in your working directory for your project. You'll need to fill out the sections marked << YOUR_VALUE_HERE >> with values appropriate to your project. The configuration file has a lot of sections. As a first pass, we will infer a lot of the parameters that are needed in there and use defaults for others. The primary parameters to specify (for now) are: TIMECHOP config: This sets up temporal parameters for training and testing models. The key things to set up here are your prediction horizon/timespan (how far out in the future do you want to predict?). For example, if you want to predict an outcome within one year, you would set label_timespans = '12month' . See our guide to Temporal Validation LABEL config: This is a sql query that defines what the outcome of interest is. The query must return two columns: entity_id (an integer) and outcome (with integer label values of 0 and 1 ), based on a given as_of_date and label_timespan (you can use these parameters in your query by surrounding them with curly braces as in the example below). See our guide to Labels . For example, if your data was in a table called semantic.events containing columns entity_id , event_date , and label , this query could simply be: select entity_id, max(label) as outcome from semantic.events where '{as_of_date}'::timestamp <= event_date and event_date < '{as_of_date}'::timestamp + interval '{label_timespan}' FEATURE config: This is where we define different aggregate features/attributes/variables to be created and used in our machine learning models. We need at least one feature specified here. For the purposes of the quickstart, let's just take the count of all events before the modeling date. In the template, you can simply fill in from_obj with the schema.table_name where your data can be found (but this can also be a more complex query in general) and knowledge_date_column with that table's date column. MODEL_GRID_PRESET config: Which models and hyperparameters we want to try in this run. We can start with quickstart that will run a quick model grid to test if everything works. Additionally, we will need a database credential file that contains the name of the database, server, username, and password to use to connect to it: # Connecting to the database requires a configuration file like this one but # named database.yaml host : address.of.database.server user : user_name db : database_name pass : user_password port : connection_port (often 5432) Copy this into a separate text file, fill in your values and save it as database.yaml in the working directory where you'll be running triage. Note, however, that if you have a DATABASE_URL environment variable set, triage will use this by default as well.","title":"3. Set up Triage configuration files"},{"location":"quickstart/#4-run-triage","text":"An overview of different steps that take place when you run Triage is here Validate the configuration files by running: triage experiment config.yaml --project-path '/project_directory' --validate-only Run triage triage experiment config.yaml --project-path '/project_directory' For this quickstart, you shouldn't need much free disk space, but note that in general your project path will contain both data matrices and trained model objects, so will need to have ample free space (you can also specify a location in S3 if you don't want to store the files locally). If you want a bit more detail or documentation, a good overview of running an experiment in triage is here .","title":"4. Run Triage"},{"location":"quickstart/#5-look-at-results-generated-by-triage","text":"Once the feature/cohort/label/matrix building is done and the experiment has moved onto modeling, check out the model_metadata.models and test_results.evaluations tables as data starts to come in. Here are a couple of quick queries to help get you started: Tables in the model_metadata schema have some general information about experiments that you've run and the models they created. The quickstart model grid preset should have built 3 models. You can check that with: select model_id , model_group_id , model_type from model_metadata . models ; This should give you a result that looks something like: model_id model_group_id model_type 1 1 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression 2 2 sklearn.tree.DecisionTreeClassifier 3 3 sklearn.dummy.DummyClassifier If you want to see predictions for individual entities, you can check out test_results.predictions , for instance: select model_id , entity_id , as_of_date , score , label_value from test_results . predictions limit 5 ; This will give you something like: model_id entity_id as_of_date score label_value 1 15596 2017-09-29 00:00:00 0.21884 0 2 15596 2017-09-29 00:00:00 0.22831 0 3 15596 2017-09-29 00:00:00 0.25195 0 Finally, test_results.evaluations holds some aggregate information on model performance: select model_id , metric , parameter , stochastic_value from test_results . evaluations order by model_id , metric , parameter ; Feel free to explore some of the other tables in these schemas (note that there's also a train_results schema with performance on the training set as well as feature importances, where defined). In a more complete modeling run, you could audition with jupyter notebooks to help you select the best-performing model specifications from a wide variety of options (see the overview of model selection and tutorial audition notebook ) and postmodeling to delve deeper into understanding these models (see the README and tutorial postmodeling notebook ).","title":"5. Look at results generated by Triage"},{"location":"quickstart/#6-iterate-and-explore","text":"Now that you have triage running, continue onto the suggested project workflow for some tips about how to iterate and tune the pipeline for your project. Alternatively, if you'd like more of a guided tour with sample data, check out our dirty duck tutorial .","title":"6. Iterate and Explore"},{"location":"triage.experiments.base/","text":"Source: triage/experiments/base.py#L0 Global Variables # CONFIG_VERSION dt_from_str # dt_from_str ( dt_str ) ExperimentBase # The Base class for all Experiments. ExperimentBase.all_as_of_times All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes ExperimentBase.all_label_windows All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config ExperimentBase.collate_aggregations collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects ExperimentBase.feature_dicts Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names ExperimentBase.feature_table_tasks All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands ExperimentBase.full_matrix_definitions Full matrix definitions Returns: (list) temporal and feature information for each matrix ExperimentBase.master_feature_dictionary All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names ExperimentBase.matrix_build_tasks Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts ExperimentBase.split_definitions Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] } ExperimentBase. __init__ # __init__ ( self , config , db_engine , model_storage_class = None , project_path = None , replace = True ) Initialize self. See help(type(self)) for accurate signature. ExperimentBase.build_matrices # build_matrices ( self ) Generate labels, features, and matrices ExperimentBase.catwalk # catwalk ( self ) Train, test, and evaluate models ExperimentBase.generate_labels # generate_labels ( self ) Generate labels based on experiment configuration Results are stored in the database, not returned ExperimentBase.generate_sparse_states # generate_sparse_states ( self ) ExperimentBase.initialize_components # initialize_components ( self ) ExperimentBase.initialize_factories # initialize_factories ( self ) ExperimentBase.log_split # log_split ( self , split_num , split ) ExperimentBase.matrix_store # matrix_store ( self , matrix_uuid ) Construct a matrix store for a given matrix uuid, using the Experiment's #matrix_store_class Args: matrix_uuid (string) A uuid for a matrix ExperimentBase.run # run ( self ) ExperimentBase.update_split_definitions # update_split_definitions ( self , new_split_definitions ) Update split definitions Args: (dict) split definitions (should have matrix uuids)","title":"Triage.experiments.base"},{"location":"triage.experiments.base/#global-variables","text":"CONFIG_VERSION","title":"Global Variables"},{"location":"triage.experiments.base/#dt_from_str","text":"dt_from_str ( dt_str )","title":"dt_from_str"},{"location":"triage.experiments.base/#experimentbase","text":"The Base class for all Experiments.","title":"ExperimentBase"},{"location":"triage.experiments.base/#experimentbase__init__","text":"__init__ ( self , config , db_engine , model_storage_class = None , project_path = None , replace = True ) Initialize self. See help(type(self)) for accurate signature.","title":"ExperimentBase.__init__"},{"location":"triage.experiments.base/#experimentbasebuild_matrices","text":"build_matrices ( self ) Generate labels, features, and matrices","title":"ExperimentBase.build_matrices"},{"location":"triage.experiments.base/#experimentbasecatwalk","text":"catwalk ( self ) Train, test, and evaluate models","title":"ExperimentBase.catwalk"},{"location":"triage.experiments.base/#experimentbasegenerate_labels","text":"generate_labels ( self ) Generate labels based on experiment configuration Results are stored in the database, not returned","title":"ExperimentBase.generate_labels"},{"location":"triage.experiments.base/#experimentbasegenerate_sparse_states","text":"generate_sparse_states ( self )","title":"ExperimentBase.generate_sparse_states"},{"location":"triage.experiments.base/#experimentbaseinitialize_components","text":"initialize_components ( self )","title":"ExperimentBase.initialize_components"},{"location":"triage.experiments.base/#experimentbaseinitialize_factories","text":"initialize_factories ( self )","title":"ExperimentBase.initialize_factories"},{"location":"triage.experiments.base/#experimentbaselog_split","text":"log_split ( self , split_num , split )","title":"ExperimentBase.log_split"},{"location":"triage.experiments.base/#experimentbasematrix_store","text":"matrix_store ( self , matrix_uuid ) Construct a matrix store for a given matrix uuid, using the Experiment's #matrix_store_class Args: matrix_uuid (string) A uuid for a matrix","title":"ExperimentBase.matrix_store"},{"location":"triage.experiments.base/#experimentbaserun","text":"run ( self )","title":"ExperimentBase.run"},{"location":"triage.experiments.base/#experimentbaseupdate_split_definitions","text":"update_split_definitions ( self , new_split_definitions ) Update split definitions Args: (dict) split definitions (should have matrix uuids)","title":"ExperimentBase.update_split_definitions"},{"location":"triage.experiments.multicore/","text":"Source: triage/experiments/multicore.py#L0 insert_into_table # insert_into_table ( insert_statements , feature_generator_factory , db_connection_string ) build_matrix # build_matrix ( build_tasks , planner_factory , db_connection_string ) train_model # train_model ( train_tasks , trainer_factory , db_connection_string ) test_and_evaluate # test_and_evaluate ( model_ids , predictor_factory , evaluator_factory , indiv_importance_factory , \\ test_store , db_connection_string , split_def , train_matrix_columns , config ) MultiCoreExperiment # The Base class for all Experiments. MultiCoreExperiment.all_as_of_times All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes MultiCoreExperiment.all_label_windows All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config MultiCoreExperiment.collate_aggregations collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects MultiCoreExperiment.feature_dicts Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names MultiCoreExperiment.feature_table_tasks All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands MultiCoreExperiment.full_matrix_definitions Full matrix definitions Returns: (list) temporal and feature information for each matrix MultiCoreExperiment.master_feature_dictionary All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names MultiCoreExperiment.matrix_build_tasks Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts MultiCoreExperiment.split_definitions Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] } MultiCoreExperiment. __init__ # __init__ ( self , n_processes = 1 , n_db_processes = 1 , * args , ** kwargs ) Initialize self. See help(type(self)) for accurate signature. MultiCoreExperiment.build_matrices # build_matrices ( self ) Generate labels, features, and matrices MultiCoreExperiment.catwalk # catwalk ( self ) Train, test, and evaluate models MultiCoreExperiment.parallelize # parallelize ( self , partially_bound_function , tasks , n_processes , chunksize = 1 ) MultiCoreExperiment.parallelize_with_success_count # parallelize_with_success_count ( self , partially_bound_function , tasks , n_processes , chunksize = 1 )","title":"Triage.experiments.multicore"},{"location":"triage.experiments.multicore/#insert_into_table","text":"insert_into_table ( insert_statements , feature_generator_factory , db_connection_string )","title":"insert_into_table"},{"location":"triage.experiments.multicore/#build_matrix","text":"build_matrix ( build_tasks , planner_factory , db_connection_string )","title":"build_matrix"},{"location":"triage.experiments.multicore/#train_model","text":"train_model ( train_tasks , trainer_factory , db_connection_string )","title":"train_model"},{"location":"triage.experiments.multicore/#test_and_evaluate","text":"test_and_evaluate ( model_ids , predictor_factory , evaluator_factory , indiv_importance_factory , \\ test_store , db_connection_string , split_def , train_matrix_columns , config )","title":"test_and_evaluate"},{"location":"triage.experiments.multicore/#multicoreexperiment","text":"The Base class for all Experiments.","title":"MultiCoreExperiment"},{"location":"triage.experiments.multicore/#multicoreexperiment__init__","text":"__init__ ( self , n_processes = 1 , n_db_processes = 1 , * args , ** kwargs ) Initialize self. See help(type(self)) for accurate signature.","title":"MultiCoreExperiment.__init__"},{"location":"triage.experiments.multicore/#multicoreexperimentbuild_matrices","text":"build_matrices ( self ) Generate labels, features, and matrices","title":"MultiCoreExperiment.build_matrices"},{"location":"triage.experiments.multicore/#multicoreexperimentcatwalk","text":"catwalk ( self ) Train, test, and evaluate models","title":"MultiCoreExperiment.catwalk"},{"location":"triage.experiments.multicore/#multicoreexperimentparallelize","text":"parallelize ( self , partially_bound_function , tasks , n_processes , chunksize = 1 )","title":"MultiCoreExperiment.parallelize"},{"location":"triage.experiments.multicore/#multicoreexperimentparallelize_with_success_count","text":"parallelize_with_success_count ( self , partially_bound_function , tasks , n_processes , chunksize = 1 )","title":"MultiCoreExperiment.parallelize_with_success_count"},{"location":"triage.experiments.singlethreaded/","text":"Source: triage/experiments/singlethreaded.py#L0 SingleThreadedExperiment # The Base class for all Experiments. SingleThreadedExperiment.all_as_of_times All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes SingleThreadedExperiment.all_label_windows All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config SingleThreadedExperiment.collate_aggregations collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects SingleThreadedExperiment.feature_dicts Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names SingleThreadedExperiment.feature_table_tasks All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands SingleThreadedExperiment.full_matrix_definitions Full matrix definitions Returns: (list) temporal and feature information for each matrix SingleThreadedExperiment.master_feature_dictionary All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names SingleThreadedExperiment.matrix_build_tasks Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts SingleThreadedExperiment.split_definitions Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] } SingleThreadedExperiment. __init__ # __init__ ( self , config , db_engine , model_storage_class = None , project_path = None , replace = True ) Initialize self. See help(type(self)) for accurate signature. SingleThreadedExperiment.build_matrices # build_matrices ( self ) Generate labels, features, and matrices SingleThreadedExperiment.catwalk # catwalk ( self ) Train, test, and evaluate models","title":"Triage.experiments.singlethreaded"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperiment","text":"The Base class for all Experiments.","title":"SingleThreadedExperiment"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperiment__init__","text":"__init__ ( self , config , db_engine , model_storage_class = None , project_path = None , replace = True ) Initialize self. See help(type(self)) for accurate signature.","title":"SingleThreadedExperiment.__init__"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentbuild_matrices","text":"build_matrices ( self ) Generate labels, features, and matrices","title":"SingleThreadedExperiment.build_matrices"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentcatwalk","text":"catwalk ( self ) Train, test, and evaluate models","title":"SingleThreadedExperiment.catwalk"},{"location":"triage_project_workflow/","text":"Using triage for a Project: Workflow Tips # Getting Started... The setup and first iteration here closely follow the QuickStart Guide , so that may be a good place to start if you're new to triage . If you've already completed the QuickStart and have a working environment, you may want to jump ahead to Iteration 2 Step 1: Get your data set up # Triage needs data in a Postgresql database, with at least one table that contains events (one per row) and entities of interest (people, place, organization, etc.; identified by an integer-valued entity_id ), a timestamp (specifyinfg when the event occurred), and additional attributes of interest about the event and/or entity (demographics for example). We will need a database credentials either in a config file or as an environment variable called DATABASE_URL that contains the name of the database, server, username, and password to use to connect to it. Iteration 1: Quick Check # This set up will run a quick sanity check to make sure everything is set up correctly and that triage runs with your data and set up. Configuration # The full triage configuration file has a lot of sections allowing for extensive customization. In the first iteration, we'll set up the minimal parameters necessary to get started and make sure you have a working triage setup, but will describe how to better customize this configuration to your project in subsequent iterations. The starting point here is the QuickStart triage config found here . Define outcome/label of interest: This is a SQL query that defines outcome we want to predict and needs to return The query must return two columns: entity_id and outcome , based on a given as_of_date and label_timespan . For more detail, see our guide to Labels . Define label_timespan ( = '12month' for example for predicting one year out) Specify features_aggregations (at least one is neded to run triage -- for the QuickStart config, you need only specify an events table and we'll start with a simple count) Specify scoring (i.e. evaluation metrics) to calculate (at least one is needed) Specify model_grid_preset (i.e. model grid) for triage to run models with different hyperparameters (set to 'quickstart' at this time) Run triage # Check triage config triage experiment config.yaml --project-path '/project_directory' --validate-only Run the pipeline triage experiment config.yaml --project-path '/project_directory' Alternatively, you can also import triage as a package in your python scrips and run it that way. Learn more about that option here . Check for Results # For this quick check, we're only running a handful of models for a single time window, so triage 's tools for model selection and postmodeling analysis won't be instructive, but you can confirm that by checking out the triage -created tables in the model_metadata , test_results , and train-results schemas in your database. In particular, you should find records for all your expected models in model_metadata.models , predictions from every model for every entity in test_results.predictions , and aggregate performance metrics in test_results.evaluations for every model. If that all looks good, it's time to get started with customizing your modeling configuration to your project... Iteration 2: Refine the cohort and temporal setup # In this next iteration, we'll stick with a simplified feature configuration, but start to refine the parameters that control your modeling universe and cross-validation. We'll also introduce audition , triage 's component for model selection. However, with the limited feature set, we'll stick with the quickstart model grid for the time being. Define your cohort # For triage , the cohort represents the universe of relevant entities for a model at a given point in time. In the first iteration, we omitted the cohort_config section of our experiment configuration, in which case triage simply includes every entity it can find in your feature configuration. However, in most cases, you'll likely want to focus on a smaller set of entities, for instance: In a housing inspection project, you might want to include only houses that were built before a given modeling date and occupied at the modeling date In a project to help allocate housing subsidies, only certain individuals in your data might be eligible for the intervention at a given point in time In a recidivism prediction project, you might want to exclude individuals who are incarcerated as of the modeling date You can specify a cohort in your config file with a SQL query that returns a set of integer entity_id s for a given modeling date (parameterized as {as_of_date} in your query). For instance: cohort_config: query: \"select entity_id from events where outcome_date < '{as_of_date}'\" name: 'past_events' Configure your temporal settings # In most real-world machine learning applications, you're interested in training on past data and predicting forward into the future. triage is built with this common use-case in mind, relying on temporal cross-validation to evaluate your models' performance in a manner that best reflects how it will be used in practice. There is a lot of nuance to the temporal configuration and it can take a bit of effort to get right. If you're new to triage (or want a refresher), we highly reccomend you check out the temporal crossvalidation deep dive . In previous iteration, we used a highly simplified temporal config, with just one parameter: label_timespans , yielding a single time split to get us started. However, these default values are generally not particularly meaningful in most cases and you'll need to fill out a more detailed temporal_config . Here's what that might look like: temporal_config: feature_start_time: '1995-01-01' # earliest date included in features feature_end_time: '2015-01-01' # latest date included in features label_start_time: '2012-01-01' # earliest date for which labels are avialable label_end_time: '2015-01-01' # day AFTER last label date (all dates in any model are < this date) model_update_frequency: '6month' # how frequently to retrain models training_as_of_date_frequencies: '1day' # time between as of dates for same entity in train matrix test_as_of_date_frequencies: '3month' # time between as of dates for same entity in test matrix max_training_histories: ['6month', '3month'] # length of time included in a train matrix test_durations: ['0day', '1month', '2month'] # length of time included in a test matrix (0 days will give a single prediction immediately after training end) training_label_timespans: ['1month'] # time period across which outcomes are labeled in train matrices test_label_timespans: ['7day'] # time period across which outcomes are labeled in test matrices For more detailed guidance on how to think about each of these parameters and set them for your context, see the deep dive , but here are a couple of quick notes: The feature_start_time should reflect the earliest time available for your features, and will often be considerably earlier than the label_start_time . All of your train/test splits will be between the label_start_time and label_end_time , with splits starting from the last date and working backwards. Note that the label_end_time should be 1 day AFTER the last label date . If you're using the same label timespan in both training and testing, you can still use the single label_timespans parameter (as we did in the QuickStart config). If you need different values, you can separately configure test_label_timespans and training_label_timespans (but note in this case, you should omit label_timespans ). The parameters with plural names (e.g., test_durations ) can be given as lists, in which case, triage will run models using all possible combinations of these values. This can get complicated fast, so you're generally best off starting with a single value for each parameter, for instance: temporal_config: feature_start_time: '1980-01-01' feature_end_time: '2019-05-01' label_start_time: '2012-01-01' label_end_time: '2019-05-01' model_update_frequency: '1month' label_timespans: ['1y'] max_training_histories: ['0d'] training_as_of_date_frequencies: ['1y'] test_as_of_date_frequencies: ['1y'] test_durations: ['0d'] As you figure out your temporal parameters, you can use the triage CLI's --show-timechop parameter to visualize the resulting time splits: triage experiment config.yaml --project-path '/project_directory' --show-timechop Set a random_seed # You may want to set an integer-valued random_seed for python to use in your configuration file in order to ensure reproducibility of your results across triage runs. Run triage # Check triage config triage experiment config.yaml --project-path '/project_directory' --validate-only Run the pipeline triage experiment config.yaml --project-path '/project_directory' Alternatively, you can also import triage as a package in your python scrips and run it that way. Learn more about that option here . Check for Results # Check the database As above, you should check the triage -created tables in your database to ensure the run with your new config has trained and tested all of the expected models. A couple of things to look out for: In triage , a specification of a model algorithm, related hyperparameters, and set of features is referred to as a model_group while an instantiation of these parameters on particular set of data at a specific point in time is referred to as a model . As such, with the quickstart preset model grid, you should still have the same 3 records in model_metadata.model_groups while you should have several new records in model_metadata.models with different train_end_time s implied by your temporal config. Likewise, in test_results.predictions and test_results.evaluations , you will find an as_of_date column. In many cases, you will likely have a single as_of_date per model that lines up with the model's train_end_time , but in some situations, you may want to evaluate at several as_of_dates for each model. See the temporal crossvalidation deep dive for more details. A first look at model selection with audition Now that we have models trained across several time periods, we can use audition to take a look at each model_group 's performance over time. While the quickstart models are quite simple and there isn't much meaningful model selection to do at this point, we can start to explore how model selection works in triage . A good place to begin is with the model selection primer . We generally recommend using audition interactively with as a jupyter notebook . If you don't already have jupyter installed, you can learn more about it here . Once you have a notebook server running, you can modify the audition tutorial notebook to take a look at the data from your current experiment. The audition README is also a good resource for options available with the tool. Iteration 3: Add more data/features, models and hyperparameters, and evaluation metrics of interest # After completing iteration 2, you should now have your cohort, label, and temporal configuration well-defined for your problem and you're ready to focus on features and model specifications. We've labeled this section Iteration 3 , but in practice it's probably more like Iterations 3-n as you will likely want to do a bit of intermediate testing while adding new features and refine your model grid as you learn more about what does and doesn't seem to work well. Define some additional features # Generally speaking, the biggest determinant of the performance of many models is the quality of the underlying features, so you'll likely spend a considerable amount of time at this stage of the process. Here, you'll likely want to add additional features based on the data you've already prepared, but likely will discover that you want to structure or collect additional raw data as well where possible. The experiment configuration file provides a decent amount of flexibility for defining features, so we'll walk through some of the details here, however you may also want to refer to the relevant sections of the config README and example config file for more details. Features in triage are temporal aggregations Just as triage is built with temporal cross-validation in mind, features in triage reflect this inherent temporal nature as well. As such, all feature definitions need to be specified with an associated date reflecting when the information was known (which may or may not be the same as when an event actually happened) and a time frame before the modeling date over which to aggregate. This has two consequences which may feel unintuitive at first: - Even static features are handled in this way, so in practice, we tend to specify them as a max (or min ) taken over identical values over all time. Categorical features are also aggregated over time in this way, so in practice these are split into separate features for each value the categorical can take, each of which is expressed as a numerical value (either binary or real-valued, like a mean over time). As a result, these values will not necessarily be mutually exclusive --- that is, a given entity can have non-zero values for more than one feature derived from the same underlying categorical depending on their history of values for that feature. Unfortunately, triage has not yet implemented functionality for \"first value\" or \"most recent value\" feature aggregates, so you'll need to pre-calculate any features you want with this logic (though we do hope to add this ability). Feature definitions are specified in the feature_aggregations section of the config file, under which you should provide a list of sets of related features, and each element in this list must contain several keys (see the example config file for a detailed example of what this looks like in practice): - prefix : a simple name to identify this set of related features - all of the features defined by this feature_aggregations entry will start with this prefix. - from_obj : this may be a table name or a SQL expression that provides the source data for these features. - knowledge_date_column : the date column specifying when information in the source data was known (e.g., available to be used for predictive modeling), which may differ from when the event ocurred. - aggregates and/or categoricals : lists used to define the specific features (you must specify at least one, but may include both). See below for more detail on each. - intervals : The time intervals (as a SQL interval, such a '5 year' , '6 month' , or all for all time) over which to aggregate features. - For instance, if you specified a count of the number of events under aggregates and ['5 year', '10 year', 'all'] as intervals , triage would create features for the number of events related to an entity in the last 5 years, 10 years, and since the feature_start_time (that is, three separate features) - groups : levels at which to aggregate the features, often simply entity_id , but can also be used for other levels of analysis, such as spatial aggregations by zip codes, etc. - You also need to provide rules for how to handle missing data, which can be provided either overall under feature_aggregations to apply to all features or on a feature-by-feature basis. It's worth reading through the Feature Generation README to learn about the available options here, including options for when missingness is meaningful (e.g., in a count) or there should be no missing data. When defining features derived from numerical data, you list them under the aggregates key in your feature config, and these should include keys for: - quantity : A column or SQL expression from the from_obj yielding a number that can be aggregated - metrics : What types of aggregation to do. Namely, these are postgres aggregation functions , such as count , avg , sum , stddev , min , max , etc. - (optional) coltype : can be used to control the type of column used in the generated features table, but generally is not necessary to define. - As noted above, imputation rules can be specified at this level as well. When defining features derived from categorical data, you list them under the categoricals key in your feature config, and these should include keys for: - column : The column containing the categorical information in the from_obj (note that this must be a column, not a SQL expression). May be any type of data, but the choice values specified must be compatible for equality comparisson in SQL. - choices or choice_query : Either a hand-coded list of choices (that is, categorical values) or a choice_query that returns these distinct values from the data. - For categoricals with a very large number of possible unique values, you may want to limit the set of choices to a set of most frequently observed values. - Values in the data but not in this set of choice values will simply yield 0 s for all of these choice-set values. - metrics : As above, the postgres aggregation functions used to aggregate values across the time intervals for the feature. - If categorical values associated with an entity do not change over time, using max would give you a simple one-hot encoded categorical. - If they are changing over time, max would give you something similar to a one-hot encoding, but note that the values would no longer be mutually-exclusive. - As noted above, imputation rules can be specified at this level as well. Much more detail about defining your features can be found in the example config file and associated README . Expand, then refine, your model grid # As you develop more features, you'll want to build out your modeling grid as well. Above, we've used the very sparse quickstart grid preset, but triage offers additional model_grid_preset options of varying size: - The small model grid includes a reasonably extensive set of logistic regression and decision tree classifiers as well as a single random forest specification. This grid can be a good option as you build and refine your features, but you'll likely want to try something more extensive once you have the rest of your config set. - The medium model grid is a good starting point for general modeling, including fairly extensive logistic regressions, decision trees, and random forest grids as well as a few ada boost and extra trees specification. - The large grid adds additional specifications for these modeling types, including some very large (10,000-estimator) random forest and extra trees classifiers, so can take a bit more time and computational resources to run. These preset grids should really serve as a starting point, and as you learn what seems to be working well in your use-case, you'll likely want to explore other specifications, which you can do by specifying your own grid_config in the triage config, which looks like: grid_config: 'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression': C: [0.00001,0.0001, 0.001, 0.01, 0.1, 10] penalty: ['l2'] 'sklearn.tree.DecisionTreeClassifier': criterion: ['entropy'] max_depth: [null,2,5,10,50,100] min_samples_split: [2, 10, 50] min_samples_leaf: [0.01,0.05,0.10] Here, each top-level key is the modeling package (this needs to be a classification algorithm with a scikit-learn -style interface, but need not come from scikit-learn specifically), and the keys listed under it are hyperparameters of the algorithm with a list of values to test. triage will run the grid of all possible combinations of these hyperparameter values. Note that you can't specify both a model_grid_preset and grid_config at the same time. Check out the example config file for more details on specifying your grid. Specify evaluation metrics you care about # In the initial iterations, we simply used precision in the top 1% as the evaluation metric for our models, but this is likely not what you care about for you project! Under the scoring section of your config file, you should specify the metrics of interest: scoring: testing_metric_groups: - metrics: [precision@, recall@] thresholds: percentiles: [1,5,10] top_n: [100, 250, 500] - metrics: [accuracy, roc_auc] training_metric_groups: - metrics: [fpr@] thresholds: top_n: [100, 250, 500] You can specify any number of evaluation metrics to be calculated for your models on either the training or test sets (the set of available metrics can be found here ). For metrics that need to be calculated relative to a specific threshold in the score (e.g. precision), you must specify either percentiles or top_n (and can optionally provide both) at which to do the calculations. Additionally, you can have triage pre-calculate statistics about bias and disparities in your modeling results by specifying a bias_audit_config section, which should give details about the attributes of interest (e.g., race, age, sex) and thresholds at which to do the calculations. See the example config file and associated README for more details on setting it up. Run triage # Check triage config triage experiment config.yaml --project-path '/project_directory' --validate-only Run the pipeline triage experiment config.yaml --project-path '/project_directory' Alternatively, you can also import triage as a package in your python scrips and run it that way. Learn more about that option here . Check for Results and Select Models # Check the database As before, the first place to look to check on the results of your modeling run is in the database: - Even while the modeling is still running, you can check out test_results.evaluations to keep an eye on the progress of the run (join it to model_metadata.models using model_id if you want to see information about the model specifications). - Once the run has finished, you should see many more models in test_results.evaluations reflecting the full model grid evaluated on each of the metrics you specified above. - Information on feature importances can be found in train_results.feature_importances (note the schema is train_results since these are calculated based on the training data). Run audition Once you have a more comprehensive model run with a variety of features and modeling grid, audition can help you understand the performance of different specifications and further refine your models for future iterations. In a typical project, you'll likely run through the audition flow several times as you progressively improve your modeling configuration. When you finally settle on a configuration you're happy with, audition will also help you narrow your models down to a smaller set of well-performing options for futher analysis. Often, this might involve something like specifying a few different \"selection rules\" (e.g., best mean performance, recency-weighted performance, etc.) and exploring one or two of the best performing under each rule using postmodeling . More about using audition : - model selection primer . - audition tutorial notebook - audition README A first look at postmodeling Now that you've narrowed your grid down to a handful of model specification for a closer look, the postmodeling methods provided in triage will help you answer three avenues of investigation: Dive deeper into what\u2019s going on with each of these models, such as: score and feature distributions feature importances performance characteristics, such as stack ranking, ROC curves, and precision-recall curves Debug and improve future models look for potential leakage of future information into your training set explore patterns in the model's errors identify hyperparameter values and features to focus on in subsequent iterations Decide how to proceed with deployment compare lists and important features across models help decide on either a single \"best\" model to deploy or a strategy that combines models Like audition , our postmodeling tools are currently best used interactively with a jupyter notebook . You can read more about these tools in the postmodeling README and modify the example postmodeling notebook for your project. Iteration 4: Explore additional labels/outcomes, feature group strategies, and calculation evaluation metrics on subsets of entities that may be of special interest # Finally, in Iteration 4 , you should consider exploring additional labels, triage 's tools for understanding feature contributions, and potentially look at evaluating your models on subsets of interest in your cohort. Additional labels # In many projects, how you choose to define your outcome label can have a dramatic impact on which entities your models bring to the top, as well as disparities across protected groups. As such, we generally recommend exploring a number of options for your label definition in the course of a given project. For instance: In a project to target health and safety inspections of apartment buildings, you might consider labels that look at the presence of any violation, the presence of at least X violations, violations of a certain type or severity, violations in a certain fraction of units, etc. In a recidivism prediction project, you might consider labels that focus on subsequent arrests for any reason or only related to new criminal activity; based on either arrests, bookings, or convictions; or related to certain types or severity of offense. In a health care project, you might consider re-hospitalizations over differ time frames, certain types of complications or severity of outcomes or prognoses. Be sure to change the name parameter in your label_config with each version to ensure that triage recognizes that models built with different labels are distinct. Feature group strategies # If you want to get a better sense for the most important types of features in your models, you can specify a feature_group_strategies key in your configuration file, allowing you to run models that include subsets of your features (note that these are taken over your feature groups --- often the different prefix values you specified --- not the individual features). The strategies you can use are: all , leave-one-out , leave-one-in , all-combinations . You can specify a list of multiple strategies, for instance: feature_group_strategies: ['all', 'leave-one-out'] If you had five feature groups, this would run a total of six strategies (one including all your feature groups, and five including all but one of them) for each specification in your model grid. Before using feature group stragies... Note that model runs specifying feature_group_strategies can become quite time and resource-intensive, especially using the all-combinations option. Before making use of this functionality, it's generally smart to narrow your modeling grid considerably to at most a handful of well-performing models and do some back-of-the-envelope calculations of how many variations triage will have to run. Learn more about feature groups and strategies in the config README . Subsets # In some cases, you may be interested in your models' performance on subsets of the full cohort on which it is trained, such as certain demographics or individuals who meet a specific criteria of interest to your program (for instance, a certain level or history of need). Subsets are defined in the scoring section of the configuration file as a list of dictionaries specifying a name and query that identify the set of entities for each subset of interest using {as_of_date} as a placeholder for the modeling date. Here's a quick example: scoring: ... subsets: - name: women query: | select distinct entity_id from demographics where d.gender = 'woman' and demographic_date < '{as_of_date}'::date - name: youts query: | select distinct entity_id from demographics where extract('year' from age({as_of_date}, d.birth_date)) <= 18 and demographic_date < '{as_of_date}'::date When specify subsets, all of the model evaluation metrics will be calculated for each subset you define here, as well as the cohort overall. In the test_results.evaluations table, the subset_hash column will identify the subset for the evaluation ( NULL values indicate evaluations on the entire cohort), and can be joined to model_metadata.subsets to obtain the name and definition of the subset. Note that subsets are only used for the purposes of evaluation, while the model will still be trained and scored on the entire cohort described above. Run triage # Check triage config triage experiment config.yaml --project-path '/project_directory' --validate-only Run the pipeline triage experiment config.yaml --project-path '/project_directory' Alternatively, you can also import triage as a package in your python scrips and run it that way. Learn more about that option here . Check for Results and Select Models # As described above, once your modeling run has completed, you can explore the results in the triage -generated tables in your database, perform model selection with audition , and dig deeper into your results with postmodeling : Check the database Look for results and associated information in: - model_metadata - train_results - test_results Run audition More about using audition : - model selection primer . - audition tutorial notebook - audition README Run postmodeling More about postmodeling : - postmodeling README - example postmodeling notebook","title":"Suggested workflow"},{"location":"triage_project_workflow/#using-triage-for-a-project-workflow-tips","text":"Getting Started... The setup and first iteration here closely follow the QuickStart Guide , so that may be a good place to start if you're new to triage . If you've already completed the QuickStart and have a working environment, you may want to jump ahead to Iteration 2","title":"Using triage for a Project: Workflow Tips"},{"location":"triage_project_workflow/#step-1-get-your-data-set-up","text":"Triage needs data in a Postgresql database, with at least one table that contains events (one per row) and entities of interest (people, place, organization, etc.; identified by an integer-valued entity_id ), a timestamp (specifyinfg when the event occurred), and additional attributes of interest about the event and/or entity (demographics for example). We will need a database credentials either in a config file or as an environment variable called DATABASE_URL that contains the name of the database, server, username, and password to use to connect to it.","title":"Step 1: Get your data set up"},{"location":"triage_project_workflow/#iteration-1-quick-check","text":"This set up will run a quick sanity check to make sure everything is set up correctly and that triage runs with your data and set up.","title":"Iteration 1: Quick Check"},{"location":"triage_project_workflow/#configuration","text":"The full triage configuration file has a lot of sections allowing for extensive customization. In the first iteration, we'll set up the minimal parameters necessary to get started and make sure you have a working triage setup, but will describe how to better customize this configuration to your project in subsequent iterations. The starting point here is the QuickStart triage config found here . Define outcome/label of interest: This is a SQL query that defines outcome we want to predict and needs to return The query must return two columns: entity_id and outcome , based on a given as_of_date and label_timespan . For more detail, see our guide to Labels . Define label_timespan ( = '12month' for example for predicting one year out) Specify features_aggregations (at least one is neded to run triage -- for the QuickStart config, you need only specify an events table and we'll start with a simple count) Specify scoring (i.e. evaluation metrics) to calculate (at least one is needed) Specify model_grid_preset (i.e. model grid) for triage to run models with different hyperparameters (set to 'quickstart' at this time)","title":"Configuration"},{"location":"triage_project_workflow/#run-triage","text":"Check triage config triage experiment config.yaml --project-path '/project_directory' --validate-only Run the pipeline triage experiment config.yaml --project-path '/project_directory' Alternatively, you can also import triage as a package in your python scrips and run it that way. Learn more about that option here .","title":"Run triage"},{"location":"triage_project_workflow/#check-for-results","text":"For this quick check, we're only running a handful of models for a single time window, so triage 's tools for model selection and postmodeling analysis won't be instructive, but you can confirm that by checking out the triage -created tables in the model_metadata , test_results , and train-results schemas in your database. In particular, you should find records for all your expected models in model_metadata.models , predictions from every model for every entity in test_results.predictions , and aggregate performance metrics in test_results.evaluations for every model. If that all looks good, it's time to get started with customizing your modeling configuration to your project...","title":"Check for Results"},{"location":"triage_project_workflow/#iteration-2-refine-the-cohort-and-temporal-setup","text":"In this next iteration, we'll stick with a simplified feature configuration, but start to refine the parameters that control your modeling universe and cross-validation. We'll also introduce audition , triage 's component for model selection. However, with the limited feature set, we'll stick with the quickstart model grid for the time being.","title":"Iteration 2: Refine the cohort and temporal setup"},{"location":"triage_project_workflow/#define-your-cohort","text":"For triage , the cohort represents the universe of relevant entities for a model at a given point in time. In the first iteration, we omitted the cohort_config section of our experiment configuration, in which case triage simply includes every entity it can find in your feature configuration. However, in most cases, you'll likely want to focus on a smaller set of entities, for instance: In a housing inspection project, you might want to include only houses that were built before a given modeling date and occupied at the modeling date In a project to help allocate housing subsidies, only certain individuals in your data might be eligible for the intervention at a given point in time In a recidivism prediction project, you might want to exclude individuals who are incarcerated as of the modeling date You can specify a cohort in your config file with a SQL query that returns a set of integer entity_id s for a given modeling date (parameterized as {as_of_date} in your query). For instance: cohort_config: query: \"select entity_id from events where outcome_date < '{as_of_date}'\" name: 'past_events'","title":"Define your cohort"},{"location":"triage_project_workflow/#configure-your-temporal-settings","text":"In most real-world machine learning applications, you're interested in training on past data and predicting forward into the future. triage is built with this common use-case in mind, relying on temporal cross-validation to evaluate your models' performance in a manner that best reflects how it will be used in practice. There is a lot of nuance to the temporal configuration and it can take a bit of effort to get right. If you're new to triage (or want a refresher), we highly reccomend you check out the temporal crossvalidation deep dive . In previous iteration, we used a highly simplified temporal config, with just one parameter: label_timespans , yielding a single time split to get us started. However, these default values are generally not particularly meaningful in most cases and you'll need to fill out a more detailed temporal_config . Here's what that might look like: temporal_config: feature_start_time: '1995-01-01' # earliest date included in features feature_end_time: '2015-01-01' # latest date included in features label_start_time: '2012-01-01' # earliest date for which labels are avialable label_end_time: '2015-01-01' # day AFTER last label date (all dates in any model are < this date) model_update_frequency: '6month' # how frequently to retrain models training_as_of_date_frequencies: '1day' # time between as of dates for same entity in train matrix test_as_of_date_frequencies: '3month' # time between as of dates for same entity in test matrix max_training_histories: ['6month', '3month'] # length of time included in a train matrix test_durations: ['0day', '1month', '2month'] # length of time included in a test matrix (0 days will give a single prediction immediately after training end) training_label_timespans: ['1month'] # time period across which outcomes are labeled in train matrices test_label_timespans: ['7day'] # time period across which outcomes are labeled in test matrices For more detailed guidance on how to think about each of these parameters and set them for your context, see the deep dive , but here are a couple of quick notes: The feature_start_time should reflect the earliest time available for your features, and will often be considerably earlier than the label_start_time . All of your train/test splits will be between the label_start_time and label_end_time , with splits starting from the last date and working backwards. Note that the label_end_time should be 1 day AFTER the last label date . If you're using the same label timespan in both training and testing, you can still use the single label_timespans parameter (as we did in the QuickStart config). If you need different values, you can separately configure test_label_timespans and training_label_timespans (but note in this case, you should omit label_timespans ). The parameters with plural names (e.g., test_durations ) can be given as lists, in which case, triage will run models using all possible combinations of these values. This can get complicated fast, so you're generally best off starting with a single value for each parameter, for instance: temporal_config: feature_start_time: '1980-01-01' feature_end_time: '2019-05-01' label_start_time: '2012-01-01' label_end_time: '2019-05-01' model_update_frequency: '1month' label_timespans: ['1y'] max_training_histories: ['0d'] training_as_of_date_frequencies: ['1y'] test_as_of_date_frequencies: ['1y'] test_durations: ['0d'] As you figure out your temporal parameters, you can use the triage CLI's --show-timechop parameter to visualize the resulting time splits: triage experiment config.yaml --project-path '/project_directory' --show-timechop","title":"Configure your temporal settings"},{"location":"triage_project_workflow/#set-a-random_seed","text":"You may want to set an integer-valued random_seed for python to use in your configuration file in order to ensure reproducibility of your results across triage runs.","title":"Set a random_seed"},{"location":"triage_project_workflow/#run-triage_1","text":"Check triage config triage experiment config.yaml --project-path '/project_directory' --validate-only Run the pipeline triage experiment config.yaml --project-path '/project_directory' Alternatively, you can also import triage as a package in your python scrips and run it that way. Learn more about that option here .","title":"Run triage"},{"location":"triage_project_workflow/#check-for-results_1","text":"","title":"Check for Results"},{"location":"triage_project_workflow/#iteration-3-add-more-datafeatures-models-and-hyperparameters-and-evaluation-metrics-of-interest","text":"After completing iteration 2, you should now have your cohort, label, and temporal configuration well-defined for your problem and you're ready to focus on features and model specifications. We've labeled this section Iteration 3 , but in practice it's probably more like Iterations 3-n as you will likely want to do a bit of intermediate testing while adding new features and refine your model grid as you learn more about what does and doesn't seem to work well.","title":"Iteration 3: Add more data/features, models and hyperparameters, and evaluation metrics of interest"},{"location":"triage_project_workflow/#define-some-additional-features","text":"Generally speaking, the biggest determinant of the performance of many models is the quality of the underlying features, so you'll likely spend a considerable amount of time at this stage of the process. Here, you'll likely want to add additional features based on the data you've already prepared, but likely will discover that you want to structure or collect additional raw data as well where possible. The experiment configuration file provides a decent amount of flexibility for defining features, so we'll walk through some of the details here, however you may also want to refer to the relevant sections of the config README and example config file for more details. Features in triage are temporal aggregations Just as triage is built with temporal cross-validation in mind, features in triage reflect this inherent temporal nature as well. As such, all feature definitions need to be specified with an associated date reflecting when the information was known (which may or may not be the same as when an event actually happened) and a time frame before the modeling date over which to aggregate. This has two consequences which may feel unintuitive at first: - Even static features are handled in this way, so in practice, we tend to specify them as a max (or min ) taken over identical values over all time. Categorical features are also aggregated over time in this way, so in practice these are split into separate features for each value the categorical can take, each of which is expressed as a numerical value (either binary or real-valued, like a mean over time). As a result, these values will not necessarily be mutually exclusive --- that is, a given entity can have non-zero values for more than one feature derived from the same underlying categorical depending on their history of values for that feature. Unfortunately, triage has not yet implemented functionality for \"first value\" or \"most recent value\" feature aggregates, so you'll need to pre-calculate any features you want with this logic (though we do hope to add this ability). Feature definitions are specified in the feature_aggregations section of the config file, under which you should provide a list of sets of related features, and each element in this list must contain several keys (see the example config file for a detailed example of what this looks like in practice): - prefix : a simple name to identify this set of related features - all of the features defined by this feature_aggregations entry will start with this prefix. - from_obj : this may be a table name or a SQL expression that provides the source data for these features. - knowledge_date_column : the date column specifying when information in the source data was known (e.g., available to be used for predictive modeling), which may differ from when the event ocurred. - aggregates and/or categoricals : lists used to define the specific features (you must specify at least one, but may include both). See below for more detail on each. - intervals : The time intervals (as a SQL interval, such a '5 year' , '6 month' , or all for all time) over which to aggregate features. - For instance, if you specified a count of the number of events under aggregates and ['5 year', '10 year', 'all'] as intervals , triage would create features for the number of events related to an entity in the last 5 years, 10 years, and since the feature_start_time (that is, three separate features) - groups : levels at which to aggregate the features, often simply entity_id , but can also be used for other levels of analysis, such as spatial aggregations by zip codes, etc. - You also need to provide rules for how to handle missing data, which can be provided either overall under feature_aggregations to apply to all features or on a feature-by-feature basis. It's worth reading through the Feature Generation README to learn about the available options here, including options for when missingness is meaningful (e.g., in a count) or there should be no missing data. When defining features derived from numerical data, you list them under the aggregates key in your feature config, and these should include keys for: - quantity : A column or SQL expression from the from_obj yielding a number that can be aggregated - metrics : What types of aggregation to do. Namely, these are postgres aggregation functions , such as count , avg , sum , stddev , min , max , etc. - (optional) coltype : can be used to control the type of column used in the generated features table, but generally is not necessary to define. - As noted above, imputation rules can be specified at this level as well. When defining features derived from categorical data, you list them under the categoricals key in your feature config, and these should include keys for: - column : The column containing the categorical information in the from_obj (note that this must be a column, not a SQL expression). May be any type of data, but the choice values specified must be compatible for equality comparisson in SQL. - choices or choice_query : Either a hand-coded list of choices (that is, categorical values) or a choice_query that returns these distinct values from the data. - For categoricals with a very large number of possible unique values, you may want to limit the set of choices to a set of most frequently observed values. - Values in the data but not in this set of choice values will simply yield 0 s for all of these choice-set values. - metrics : As above, the postgres aggregation functions used to aggregate values across the time intervals for the feature. - If categorical values associated with an entity do not change over time, using max would give you a simple one-hot encoded categorical. - If they are changing over time, max would give you something similar to a one-hot encoding, but note that the values would no longer be mutually-exclusive. - As noted above, imputation rules can be specified at this level as well. Much more detail about defining your features can be found in the example config file and associated README .","title":"Define some additional features"},{"location":"triage_project_workflow/#expand-then-refine-your-model-grid","text":"As you develop more features, you'll want to build out your modeling grid as well. Above, we've used the very sparse quickstart grid preset, but triage offers additional model_grid_preset options of varying size: - The small model grid includes a reasonably extensive set of logistic regression and decision tree classifiers as well as a single random forest specification. This grid can be a good option as you build and refine your features, but you'll likely want to try something more extensive once you have the rest of your config set. - The medium model grid is a good starting point for general modeling, including fairly extensive logistic regressions, decision trees, and random forest grids as well as a few ada boost and extra trees specification. - The large grid adds additional specifications for these modeling types, including some very large (10,000-estimator) random forest and extra trees classifiers, so can take a bit more time and computational resources to run. These preset grids should really serve as a starting point, and as you learn what seems to be working well in your use-case, you'll likely want to explore other specifications, which you can do by specifying your own grid_config in the triage config, which looks like: grid_config: 'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression': C: [0.00001,0.0001, 0.001, 0.01, 0.1, 10] penalty: ['l2'] 'sklearn.tree.DecisionTreeClassifier': criterion: ['entropy'] max_depth: [null,2,5,10,50,100] min_samples_split: [2, 10, 50] min_samples_leaf: [0.01,0.05,0.10] Here, each top-level key is the modeling package (this needs to be a classification algorithm with a scikit-learn -style interface, but need not come from scikit-learn specifically), and the keys listed under it are hyperparameters of the algorithm with a list of values to test. triage will run the grid of all possible combinations of these hyperparameter values. Note that you can't specify both a model_grid_preset and grid_config at the same time. Check out the example config file for more details on specifying your grid.","title":"Expand, then refine, your model grid"},{"location":"triage_project_workflow/#specify-evaluation-metrics-you-care-about","text":"In the initial iterations, we simply used precision in the top 1% as the evaluation metric for our models, but this is likely not what you care about for you project! Under the scoring section of your config file, you should specify the metrics of interest: scoring: testing_metric_groups: - metrics: [precision@, recall@] thresholds: percentiles: [1,5,10] top_n: [100, 250, 500] - metrics: [accuracy, roc_auc] training_metric_groups: - metrics: [fpr@] thresholds: top_n: [100, 250, 500] You can specify any number of evaluation metrics to be calculated for your models on either the training or test sets (the set of available metrics can be found here ). For metrics that need to be calculated relative to a specific threshold in the score (e.g. precision), you must specify either percentiles or top_n (and can optionally provide both) at which to do the calculations. Additionally, you can have triage pre-calculate statistics about bias and disparities in your modeling results by specifying a bias_audit_config section, which should give details about the attributes of interest (e.g., race, age, sex) and thresholds at which to do the calculations. See the example config file and associated README for more details on setting it up.","title":"Specify evaluation metrics you care about"},{"location":"triage_project_workflow/#run-triage_2","text":"Check triage config triage experiment config.yaml --project-path '/project_directory' --validate-only Run the pipeline triage experiment config.yaml --project-path '/project_directory' Alternatively, you can also import triage as a package in your python scrips and run it that way. Learn more about that option here .","title":"Run triage"},{"location":"triage_project_workflow/#check-for-results-and-select-models","text":"","title":"Check for Results and Select Models"},{"location":"triage_project_workflow/#iteration-4-explore-additional-labelsoutcomes-feature-group-strategies-and-calculation-evaluation-metrics-on-subsets-of-entities-that-may-be-of-special-interest","text":"Finally, in Iteration 4 , you should consider exploring additional labels, triage 's tools for understanding feature contributions, and potentially look at evaluating your models on subsets of interest in your cohort.","title":"Iteration 4: Explore additional labels/outcomes, feature group strategies, and calculation evaluation metrics on subsets of entities that may be of special interest"},{"location":"triage_project_workflow/#additional-labels","text":"In many projects, how you choose to define your outcome label can have a dramatic impact on which entities your models bring to the top, as well as disparities across protected groups. As such, we generally recommend exploring a number of options for your label definition in the course of a given project. For instance: In a project to target health and safety inspections of apartment buildings, you might consider labels that look at the presence of any violation, the presence of at least X violations, violations of a certain type or severity, violations in a certain fraction of units, etc. In a recidivism prediction project, you might consider labels that focus on subsequent arrests for any reason or only related to new criminal activity; based on either arrests, bookings, or convictions; or related to certain types or severity of offense. In a health care project, you might consider re-hospitalizations over differ time frames, certain types of complications or severity of outcomes or prognoses. Be sure to change the name parameter in your label_config with each version to ensure that triage recognizes that models built with different labels are distinct.","title":"Additional labels"},{"location":"triage_project_workflow/#feature-group-strategies","text":"If you want to get a better sense for the most important types of features in your models, you can specify a feature_group_strategies key in your configuration file, allowing you to run models that include subsets of your features (note that these are taken over your feature groups --- often the different prefix values you specified --- not the individual features). The strategies you can use are: all , leave-one-out , leave-one-in , all-combinations . You can specify a list of multiple strategies, for instance: feature_group_strategies: ['all', 'leave-one-out'] If you had five feature groups, this would run a total of six strategies (one including all your feature groups, and five including all but one of them) for each specification in your model grid. Before using feature group stragies... Note that model runs specifying feature_group_strategies can become quite time and resource-intensive, especially using the all-combinations option. Before making use of this functionality, it's generally smart to narrow your modeling grid considerably to at most a handful of well-performing models and do some back-of-the-envelope calculations of how many variations triage will have to run. Learn more about feature groups and strategies in the config README .","title":"Feature group strategies"},{"location":"triage_project_workflow/#subsets","text":"In some cases, you may be interested in your models' performance on subsets of the full cohort on which it is trained, such as certain demographics or individuals who meet a specific criteria of interest to your program (for instance, a certain level or history of need). Subsets are defined in the scoring section of the configuration file as a list of dictionaries specifying a name and query that identify the set of entities for each subset of interest using {as_of_date} as a placeholder for the modeling date. Here's a quick example: scoring: ... subsets: - name: women query: | select distinct entity_id from demographics where d.gender = 'woman' and demographic_date < '{as_of_date}'::date - name: youts query: | select distinct entity_id from demographics where extract('year' from age({as_of_date}, d.birth_date)) <= 18 and demographic_date < '{as_of_date}'::date When specify subsets, all of the model evaluation metrics will be calculated for each subset you define here, as well as the cohort overall. In the test_results.evaluations table, the subset_hash column will identify the subset for the evaluation ( NULL values indicate evaluations on the entire cohort), and can be joined to model_metadata.subsets to obtain the name and definition of the subset. Note that subsets are only used for the purposes of evaluation, while the model will still be trained and scored on the entire cohort described above.","title":"Subsets"},{"location":"triage_project_workflow/#run-triage_3","text":"Check triage config triage experiment config.yaml --project-path '/project_directory' --validate-only Run the pipeline triage experiment config.yaml --project-path '/project_directory' Alternatively, you can also import triage as a package in your python scrips and run it that way. Learn more about that option here .","title":"Run triage"},{"location":"triage_project_workflow/#check-for-results-and-select-models_1","text":"As described above, once your modeling run has completed, you can explore the results in the triage -generated tables in your database, perform model selection with audition , and dig deeper into your results with postmodeling :","title":"Check for Results and Select Models"},{"location":"dirtyduck/","text":"This is a guide to Triage , a data science workflow tool initially developed at the Center for Data Science and Public Policy (DSaPP) at the University of Chicago and now being maintained at Carnegie Mellon University. Triage helps build models for two common applied problems : (a) Early warning systems ( EWS or EIS ), (b) resource prioritization (a.k.a \"an inspections problem\") . These problems are difficult to model because their conceptualization and and implementation are prone to error, given their multi-dimensional, multi-entity, time-series structure. Info This tutorial is in sync with the latest version of triage . At this moment v4.0.0 . How you can help to improve this tutorial If you want to contribute, please follow the suggestions in the triage\u2019s github repository . What is in the name? # There is a famous (and delicious) chinese duck restaurant in Chicago, we love that place, and as every restaurant in Chicago area, it gets inspected, so the naming is an homage to them. Who is this tutorial for? # We created this tutorial with two roles in mind: A data scientist/ML practitioner who wants to focus in the problem at his/her hands, not in the nitty-gritty detail about how to configure and setup a Machine learning pipeline, Model governance, Model selection, etc. A policy maker with a little of technical background that wants to learn how to pose his/her policy problem as a Machine Learning problem. How to use this tutorial # First, clone this repository on your laptop git clone https://github.com/dssg/triage Second, in the cloned repository's top-level directory run ./tutorial.sh up This will take several minutes the first time you do it. After this, you may decide to do the quickstart tutorial . Before you start # What you need for this tutorial # Install Docker CE and Docker Compose . That's it! Follow the links for the installation instructions. Note that if you are using GNU/Linux you should add your user to the docker group following the instructions at this link . At the moment only operative systems with *nix-type command lines are supported, such as GNU/Linux and MacOS . Recent versions of Windows may also work.","title":"Welcome!"},{"location":"dirtyduck/#what-is-in-the-name","text":"There is a famous (and delicious) chinese duck restaurant in Chicago, we love that place, and as every restaurant in Chicago area, it gets inspected, so the naming is an homage to them.","title":"What is in the name?"},{"location":"dirtyduck/#who-is-this-tutorial-for","text":"We created this tutorial with two roles in mind: A data scientist/ML practitioner who wants to focus in the problem at his/her hands, not in the nitty-gritty detail about how to configure and setup a Machine learning pipeline, Model governance, Model selection, etc. A policy maker with a little of technical background that wants to learn how to pose his/her policy problem as a Machine Learning problem.","title":"Who is this tutorial for?"},{"location":"dirtyduck/#how-to-use-this-tutorial","text":"First, clone this repository on your laptop git clone https://github.com/dssg/triage Second, in the cloned repository's top-level directory run ./tutorial.sh up This will take several minutes the first time you do it. After this, you may decide to do the quickstart tutorial .","title":"How to use this tutorial"},{"location":"dirtyduck/#before-you-start","text":"","title":"Before you start"},{"location":"dirtyduck/#what-you-need-for-this-tutorial","text":"Install Docker CE and Docker Compose . That's it! Follow the links for the installation instructions. Note that if you are using GNU/Linux you should add your user to the docker group following the instructions at this link . At the moment only operative systems with *nix-type command lines are supported, such as GNU/Linux and MacOS . Recent versions of Windows may also work.","title":"What you need for this tutorial"},{"location":"dirtyduck/audition/","text":"Model selection # How to pick the best one and use it for making predictions with new data? What do you mean by \u201cthe best\u201d? This is not as easy as it sounds, due to several factors: You can try to pick the best using a metric specified in the config file ( precision@ and recall@ ), but at what point of time? Maybe different model groups are best at different prediction times. You can just use the one that performs best on the last test set. You can value a model group that provides consistent results over time. It might not be the best on any test set, but you can feel more confident that it will continue to perform similarly. If there are several model groups that perform similarly and their lists are more or less similar, maybe it doesn't really matter which you pick. The answers to questions like these may not be obvious up front. Let\u2019s discuss an imaginary example, that will help to clarify this 1 The graphic above provides an imaginary example with three three different models groups on how they performed against the actual results of inspections in 2014, 2015, and 2016. In the x x -axis, the date of the predictions, inn the y y -axes the metric of interest (e.g. precision , recall , etc). Recall that the models groups differ in a number of ways, for instance: including or excluding different types of features , employing different algorithms or hyperparameters, or focusing on more or less recent information, this comes from timechop configuration , (e.g how much past data do you want to include). We start by building a set of models that seek to predict what will happen in 2014, using only information that would have been available to inspectors before January 1, 2014. We then evaluate the models based on what actually happened in 2014. Repeating this process for 2015 and 2016 gives us an idea of how well \u2014 and how dependably \u2014 a given model is able to predict the future. Now, for the selection you could have a reasoning process as follows: You can probably eliminate the yellow triangle model right off the bat. If we only looked at 2016, we\u2019d choose the light blue squares model, but although it does well in 2016, it performed the worst in 2015, so we don\u2019t know if we can trust its performance \u2013 what if it dips back down in 2017? Then again, what if 2015 was just some sort of anomaly? We don\u2019t know the future (which is why we need analysis like this), but we want to give ourselves the best advantage we can. To balance consistency and performance, we choose a model that reliably performs well (blue circles), even if it\u2019s not always the best. In this particular/imaginary example , the \u201cselection process\u201d was kind of easy. Of course,in real life we were choosing between more than three models; we just built and evaluated more than 46 models groups ! Remember, this is only one way of choose! You could have (or better, the organization could have) a different opinion about what consists a best model. Triage provides this functionality in audition and in postmodel . At the moment of this writing, these two modules require more interaction (i.e. they aren't integrated with the configuration file ). Audition is a tool for picking the best trained classifiers from a predictive analytics experiment. Audition introduces a structured, semi-automated way of filtering models based on what you consider important. Audition formalizes this idea through selection rules that take in the data up to a given point in time, apply some rule to choose a model group, and then evaluate the performance ( regret ) of the chosen model group in the subsequent time window. Audition predefines 7 rules: best_current_value :: Pick the model group with the best current metric Value. best_average_value :: Pick the model with the highest average metric value so far. lowest_metric_variance :: Pick the model with the lowest metric variance so far. most_frequent_best_dist :: Pick the model that is most frequently within dist_from_best_case from the best-performing model group across test sets so far. best_average_two_metrics :: Pick the model with the highest average combined value to date of two metrics weighted together using metric1_weight . best_avg_var_penalized :: Pick the model with the highest average metric value so far, penalized for relative variance as: [ =avg_value - (stdev_penalty) * (stdev - min_stdev)= ] where min_stdev is the minimum standard deviation of the metric across all model groups best_avg_recency_weight :: Pick the model with the highest average metric value so far, placing less weight in older results. You need to specify two parameters: the shape of how the weight affects points ( decay_type , linear or exponential) and the relative weight of the most recent point ( curr_weight ). Before move on, remember the two main caveats for the value of the metric in this kind of ML problems: Could be many entities with the same predicted risk score ( ties ) Could be a lot of entities without a label (Weren't inspected, so we don\u2019t know) We included a simple configuration file with some rules: # CHOOSE MODEL GROUPS model_groups : query : | select distinct(model_group_id) from model_metadata.model_groups where model_config ->> 'experiment_type' ~ 'inspection' # CHOOSE TIMESTAMPS/TRAIN END TIMES time_stamps : query : | select distinct train_end_time from model_metadata.models where model_group_id in ({}) and extract(day from train_end_time) in (1) and train_end_time >= '2015-01-01' # FILTER filter : metric : 'precision@' # metric of interest parameter : '10_pct' # parameter of interest max_from_best : 1.0 # The maximum value that the given metric can be worse than the best model for a given train end time. threshold_value : 0.0 # The worst absolute value that the given metric should be. distance_table : 'inspections_distance_table' # name of the distance table models_table : 'models' # name of the models table # RULES rules : - shared_parameters : - metric : 'precision@' parameter : '10_pct' selection_rules : - name : 'best_current_value' # Pick the model group with the best current metric value n : 3 - name : 'best_average_value' # Pick the model with the highest average metric value n : 3 - name : 'lowest_metric_variance' # Pick the model with the lowest metric variance n : 3 - name : 'most_frequent_best_dist' # Pick the model that is most frequently within `dist_from_best_case` dist_from_best_case : [ 0.05 ] n : 3 Audition will have each rule give you the best n n model groups based on the metric and parameter following that rule for the most recent time period (in all the rules shown n n = 3). We can run the simulation of the rules against the experiment as: # Run this in bastion\u2026 triage --tb audition -c inspection_audition_config.yaml --directory audition/inspections Audition will create several plots that will help you to sort out which is the best model group to use (like in a production setting or just to generate your predictions list). Footnotes # 1 The discussion was taken stolen from Data-Driven Inspections for Safer Housing in San Jose, California (Kit Rodolfa, Jane Zanzig 2016) Great read by the way!","title":"Model selection"},{"location":"dirtyduck/audition/#model-selection","text":"How to pick the best one and use it for making predictions with new data? What do you mean by \u201cthe best\u201d? This is not as easy as it sounds, due to several factors: You can try to pick the best using a metric specified in the config file ( precision@ and recall@ ), but at what point of time? Maybe different model groups are best at different prediction times. You can just use the one that performs best on the last test set. You can value a model group that provides consistent results over time. It might not be the best on any test set, but you can feel more confident that it will continue to perform similarly. If there are several model groups that perform similarly and their lists are more or less similar, maybe it doesn't really matter which you pick. The answers to questions like these may not be obvious up front. Let\u2019s discuss an imaginary example, that will help to clarify this 1 The graphic above provides an imaginary example with three three different models groups on how they performed against the actual results of inspections in 2014, 2015, and 2016. In the x x -axis, the date of the predictions, inn the y y -axes the metric of interest (e.g. precision , recall , etc). Recall that the models groups differ in a number of ways, for instance: including or excluding different types of features , employing different algorithms or hyperparameters, or focusing on more or less recent information, this comes from timechop configuration , (e.g how much past data do you want to include). We start by building a set of models that seek to predict what will happen in 2014, using only information that would have been available to inspectors before January 1, 2014. We then evaluate the models based on what actually happened in 2014. Repeating this process for 2015 and 2016 gives us an idea of how well \u2014 and how dependably \u2014 a given model is able to predict the future. Now, for the selection you could have a reasoning process as follows: You can probably eliminate the yellow triangle model right off the bat. If we only looked at 2016, we\u2019d choose the light blue squares model, but although it does well in 2016, it performed the worst in 2015, so we don\u2019t know if we can trust its performance \u2013 what if it dips back down in 2017? Then again, what if 2015 was just some sort of anomaly? We don\u2019t know the future (which is why we need analysis like this), but we want to give ourselves the best advantage we can. To balance consistency and performance, we choose a model that reliably performs well (blue circles), even if it\u2019s not always the best. In this particular/imaginary example , the \u201cselection process\u201d was kind of easy. Of course,in real life we were choosing between more than three models; we just built and evaluated more than 46 models groups ! Remember, this is only one way of choose! You could have (or better, the organization could have) a different opinion about what consists a best model. Triage provides this functionality in audition and in postmodel . At the moment of this writing, these two modules require more interaction (i.e. they aren't integrated with the configuration file ). Audition is a tool for picking the best trained classifiers from a predictive analytics experiment. Audition introduces a structured, semi-automated way of filtering models based on what you consider important. Audition formalizes this idea through selection rules that take in the data up to a given point in time, apply some rule to choose a model group, and then evaluate the performance ( regret ) of the chosen model group in the subsequent time window. Audition predefines 7 rules: best_current_value :: Pick the model group with the best current metric Value. best_average_value :: Pick the model with the highest average metric value so far. lowest_metric_variance :: Pick the model with the lowest metric variance so far. most_frequent_best_dist :: Pick the model that is most frequently within dist_from_best_case from the best-performing model group across test sets so far. best_average_two_metrics :: Pick the model with the highest average combined value to date of two metrics weighted together using metric1_weight . best_avg_var_penalized :: Pick the model with the highest average metric value so far, penalized for relative variance as: [ =avg_value - (stdev_penalty) * (stdev - min_stdev)= ] where min_stdev is the minimum standard deviation of the metric across all model groups best_avg_recency_weight :: Pick the model with the highest average metric value so far, placing less weight in older results. You need to specify two parameters: the shape of how the weight affects points ( decay_type , linear or exponential) and the relative weight of the most recent point ( curr_weight ). Before move on, remember the two main caveats for the value of the metric in this kind of ML problems: Could be many entities with the same predicted risk score ( ties ) Could be a lot of entities without a label (Weren't inspected, so we don\u2019t know) We included a simple configuration file with some rules: # CHOOSE MODEL GROUPS model_groups : query : | select distinct(model_group_id) from model_metadata.model_groups where model_config ->> 'experiment_type' ~ 'inspection' # CHOOSE TIMESTAMPS/TRAIN END TIMES time_stamps : query : | select distinct train_end_time from model_metadata.models where model_group_id in ({}) and extract(day from train_end_time) in (1) and train_end_time >= '2015-01-01' # FILTER filter : metric : 'precision@' # metric of interest parameter : '10_pct' # parameter of interest max_from_best : 1.0 # The maximum value that the given metric can be worse than the best model for a given train end time. threshold_value : 0.0 # The worst absolute value that the given metric should be. distance_table : 'inspections_distance_table' # name of the distance table models_table : 'models' # name of the models table # RULES rules : - shared_parameters : - metric : 'precision@' parameter : '10_pct' selection_rules : - name : 'best_current_value' # Pick the model group with the best current metric value n : 3 - name : 'best_average_value' # Pick the model with the highest average metric value n : 3 - name : 'lowest_metric_variance' # Pick the model with the lowest metric variance n : 3 - name : 'most_frequent_best_dist' # Pick the model that is most frequently within `dist_from_best_case` dist_from_best_case : [ 0.05 ] n : 3 Audition will have each rule give you the best n n model groups based on the metric and parameter following that rule for the most recent time period (in all the rules shown n n = 3). We can run the simulation of the rules against the experiment as: # Run this in bastion\u2026 triage --tb audition -c inspection_audition_config.yaml --directory audition/inspections Audition will create several plots that will help you to sort out which is the best model group to use (like in a production setting or just to generate your predictions list).","title":"Model selection"},{"location":"dirtyduck/audition/#footnotes","text":"1 The discussion was taken stolen from Data-Driven Inspections for Safer Housing in San Jose, California (Kit Rodolfa, Jane Zanzig 2016) Great read by the way!","title":"Footnotes"},{"location":"dirtyduck/aws_batch/","text":"Scaling out: AWS Batch # If your laptop choked in the previous sections or if you can't afford to look your laptop just lagging forever, you should read this section\u2026 For bigger experiment, one option is use [[https://aws.amazon.com/batch/][AWS Batch]] . AWS Batch dynamically provisions the optimal quantity and type of compute resources based on the specific resource requirements of the tasks submitted. AWS Batch will manage (i.e. plans, schedules, and executes) the resources (CPU, Memory) that we need to run the pipeline. In other words, AWS Batch will provide you with a computer (an AWS EC2 machine) that satisfies your computing requirements, and then it will execute the software that you intend to run. AWS Batch dependes in other two technologies in order to work: Elastic Container Registry (Amazon ECR) as the Docker image registry (allowing AWS Batch to fetch the task images), and Elastic Compute Cloud (Amazon EC2) instances located in the cluster as the docker host (allowing AWS Batch to execute the task). An AWS ECS task will be executed by an EC2 instance belonging to the ECS cluster (if there are resources available). The EC2 machine operates as a Docker host: it will run the task definition, download the appropriate image from the ECS registry, and execute the container. What do you need to setup? # AWS Batch requires setup the following infrastructure: An AWS S3 bucket for storing the original data and the successive transformations of it made by the pipeline. A PostgreSQL database (provided by AWS RDS ) for storing the data in a relational form. An Elastic Container Registry ( AWS ECR ) for storing the triage's Docker image used in the pipeline. AWS Batch Job Queue configured and ready to go. Assumptions # You have [[https://stedolan.github.io/jq/download/][jq]] installed You have IAM credentials with permissions to run AWS Batch, read AWS S3 and create AWS EC2 machines. You installed awscli and configure your credentials following the standard instructions. You have access to a S3 bucket: You have a AWS ECR repository with the following form: dsapp/triage-cli You have a AWS Batch job queue configured and have permissions for adding, running, canceling jobs. You can check if you have the AWS S3 permissions like: [ AWS_PROFILE = your_profile ] aws ls [ your-bucket ] # (dont't forget the last backslash) And for the AWS Batch part: [ AWS_PROFILE = your_profile ] aws batch describe-job-queues [ AWS_PROFILE = your_profile ] aws batch describe-job-definitions Configuration # First we need to customize the file .aws_env (yes, another environment file). Copy the file aws_env.example to .aws_env and fill the blanks NOTE : Don't include the s3:// protocol prefix in the S3_BUCKET (Local) Environment variables # #!/usr/bin/env bash PROJECT_NAME=dirtyduck TRIAGE_VERSION=3.3.0 ENV=development AWS_REGISTRY={your-ecr-registry} AWS_JOB_QUEUE={your-job-queue} POSTGRES_DB={postgresql://user:password@db_server/dbname} S3_BUCKET={your-bucket} To check if everything is correct you can run: [ AWS_PROFILE = your_profile ] ./deploy.sh -h Next, we need 3 files for running in AWS Batch, copy the files and remove the .example extension and adapt them to your case: Job definition # Change the PROJECT_NAME and AWS_ACCOUNT for their real values { \"containerProperties\" : { \"command\" : [ \"--tb\" , \"Ref::experiment_file\" , \"--project-path\" , \"Ref::output_path\" , \"Ref::replace\" , \"Ref::save_predictions\" , \"Ref::profile\" , \"Ref::validate\" ], \"image\" : \"AWS_ACCOUNT.dkr.ecr.us-west-2.amazonaws.com/YOUR_TRIAGE_IMAGE\" , \"jobRoleArn\" : \"arn:aws:iam::AWS_ACCOUNT:role/dsappBatchJobRole\" , \"memory\" : 16000 , \"vcpus\" : 1 }, \"jobDefinitionName\" : \"triage-cli-experiment\" , \"retryStrategy\" : { \"attempts\" : 1 }, \"type\" : \"container\" } Environment variables overrides (for docker container inside the AWS EC2) # Fill out the missing values { \"environment\" : [ { \"name\" : \"AWS_DEFAULT_REGION\" , \"value\" : \"us-west-2\" }, { \"name\" : \"AWS_JOB_QUEUE\" , \"value\" : \"\" }, { \"name\" : \"POSTGRES_PASSWORD\" , \"value\" : \"\" }, { \"name\" : \"POSTGRES_USER\" , \"value\" : \"\" }, { \"name\" : \"POSTGRES_DB\" , \"value\" : \"\" }, { \"name\" : \"POSTGRES_PORT\" , \"value\" : \"\" }, { \"name\" : \"POSTGRES_HOST\" , \"value\" : \"\" } ] } credentials-filter # Leave this file as is (We will use it for storing the temporal token in deploy.sh ) { \"environment\" : [ { \"name\" : \"AWS_ACCESS_KEY_ID\" , \"value\" : .Credentials.AccessKeyId }, { \"name\" : \"AWS_SECRET_ACCESS_KEY\" , \"value\" : .Credentials.SecretAccessKey }, { \"name\" : \"AWS_SESSION_TOKEN\" , \"value\" : .Credentials.SessionToken } ] } Running an experiment # We provided a simple bash file for creating the image, uploading/updating the job definition and running the experiment: ./deploy.sh -h Usage: ./deploy.sh ( -h | -i | -u | -b | -r | -a | --sync_ { to,from } _s3 ) OPTIONS: -h | --help Show this message -i | --info Show information about the environment -b | --update-images Build the triage image and push it to the AWS ECR -u | --update-jobs Update the triage job definition in AWS Batch -r | --run-experiment Run experiments on chile-dt data --sync-to-s3 Uploads the experiments and configuration files to s3://your_project --sync-from-s3 Gets the experiments and configuration files from s3://your_project EXAMPLES: Build and push the images to your AWS ECR: $ ./deploy.sh -b Update the job ' s definitions: $ ./deploy.sh -u Run triage experiments: $ ./deploy.sh -r --experiment_file = s3://your_project/experiments/test.yaml,project_path = s3://your_project/triage,replace = --replace If you have multiple AWS profiles use deploy.sh as follows: [ AWS_PROFILE = your_profile ] ./deploy.sh -r [ job-run-name ] experiment_file = s3:// { your_bucket } /experiments/simple_test_skeleton.yaml,output_path = s3:// { your_bucket } /triage,replace = --no-replace,save_predictions = --no-save-predictions,profile = --profile,validate = --validate Where your_profile is the name of the profile in ~/.aws/credentials Suggested workflow # The workflow now is: At the beginning of the project Set a docker image and publish it to the AWS ECR (if needed, or you can use the triage official one). You could create different images if you want to run something more tailored to you (like not using the cli interface) Create a job definition and publish it: [ AWS_PROFILE = your_profile ] ./deploy.sh -u You could create different jobs if, for example, you want to have different resources (maybe small resources for testing or a lot of resources for a big experiment) Every time that you have an idea about how to improve the results Create experiment files and publish them to the s3 bucket: [ AWS_PROFILE = your_profile ] ./deploy.sh --synt-to-s3 Run the experiments [ AWS_PROFILE = your_profile ] ./deploy.sh -r [ job-run-name ] experiment_file = s3:// { your_bucket } /experiments/simple_test_skeleton.yaml,output_path = s3:// { your_bucket } /triage,replace = --no-replace,save_predictions = --no-save-predictions,profile = --profile,validate = --validate","title":"Scaling up"},{"location":"dirtyduck/aws_batch/#scaling-out-aws-batch","text":"If your laptop choked in the previous sections or if you can't afford to look your laptop just lagging forever, you should read this section\u2026 For bigger experiment, one option is use [[https://aws.amazon.com/batch/][AWS Batch]] . AWS Batch dynamically provisions the optimal quantity and type of compute resources based on the specific resource requirements of the tasks submitted. AWS Batch will manage (i.e. plans, schedules, and executes) the resources (CPU, Memory) that we need to run the pipeline. In other words, AWS Batch will provide you with a computer (an AWS EC2 machine) that satisfies your computing requirements, and then it will execute the software that you intend to run. AWS Batch dependes in other two technologies in order to work: Elastic Container Registry (Amazon ECR) as the Docker image registry (allowing AWS Batch to fetch the task images), and Elastic Compute Cloud (Amazon EC2) instances located in the cluster as the docker host (allowing AWS Batch to execute the task). An AWS ECS task will be executed by an EC2 instance belonging to the ECS cluster (if there are resources available). The EC2 machine operates as a Docker host: it will run the task definition, download the appropriate image from the ECS registry, and execute the container.","title":"Scaling out: AWS Batch"},{"location":"dirtyduck/aws_batch/#what-do-you-need-to-setup","text":"AWS Batch requires setup the following infrastructure: An AWS S3 bucket for storing the original data and the successive transformations of it made by the pipeline. A PostgreSQL database (provided by AWS RDS ) for storing the data in a relational form. An Elastic Container Registry ( AWS ECR ) for storing the triage's Docker image used in the pipeline. AWS Batch Job Queue configured and ready to go.","title":"What do you need to setup?"},{"location":"dirtyduck/aws_batch/#assumptions","text":"You have [[https://stedolan.github.io/jq/download/][jq]] installed You have IAM credentials with permissions to run AWS Batch, read AWS S3 and create AWS EC2 machines. You installed awscli and configure your credentials following the standard instructions. You have access to a S3 bucket: You have a AWS ECR repository with the following form: dsapp/triage-cli You have a AWS Batch job queue configured and have permissions for adding, running, canceling jobs. You can check if you have the AWS S3 permissions like: [ AWS_PROFILE = your_profile ] aws ls [ your-bucket ] # (dont't forget the last backslash) And for the AWS Batch part: [ AWS_PROFILE = your_profile ] aws batch describe-job-queues [ AWS_PROFILE = your_profile ] aws batch describe-job-definitions","title":"Assumptions"},{"location":"dirtyduck/aws_batch/#configuration","text":"First we need to customize the file .aws_env (yes, another environment file). Copy the file aws_env.example to .aws_env and fill the blanks NOTE : Don't include the s3:// protocol prefix in the S3_BUCKET","title":"Configuration"},{"location":"dirtyduck/aws_batch/#local-environment-variables","text":"#!/usr/bin/env bash PROJECT_NAME=dirtyduck TRIAGE_VERSION=3.3.0 ENV=development AWS_REGISTRY={your-ecr-registry} AWS_JOB_QUEUE={your-job-queue} POSTGRES_DB={postgresql://user:password@db_server/dbname} S3_BUCKET={your-bucket} To check if everything is correct you can run: [ AWS_PROFILE = your_profile ] ./deploy.sh -h Next, we need 3 files for running in AWS Batch, copy the files and remove the .example extension and adapt them to your case:","title":"(Local) Environment variables"},{"location":"dirtyduck/aws_batch/#job-definition","text":"Change the PROJECT_NAME and AWS_ACCOUNT for their real values { \"containerProperties\" : { \"command\" : [ \"--tb\" , \"Ref::experiment_file\" , \"--project-path\" , \"Ref::output_path\" , \"Ref::replace\" , \"Ref::save_predictions\" , \"Ref::profile\" , \"Ref::validate\" ], \"image\" : \"AWS_ACCOUNT.dkr.ecr.us-west-2.amazonaws.com/YOUR_TRIAGE_IMAGE\" , \"jobRoleArn\" : \"arn:aws:iam::AWS_ACCOUNT:role/dsappBatchJobRole\" , \"memory\" : 16000 , \"vcpus\" : 1 }, \"jobDefinitionName\" : \"triage-cli-experiment\" , \"retryStrategy\" : { \"attempts\" : 1 }, \"type\" : \"container\" }","title":"Job definition"},{"location":"dirtyduck/aws_batch/#environment-variables-overrides-for-docker-container-inside-the-aws-ec2","text":"Fill out the missing values { \"environment\" : [ { \"name\" : \"AWS_DEFAULT_REGION\" , \"value\" : \"us-west-2\" }, { \"name\" : \"AWS_JOB_QUEUE\" , \"value\" : \"\" }, { \"name\" : \"POSTGRES_PASSWORD\" , \"value\" : \"\" }, { \"name\" : \"POSTGRES_USER\" , \"value\" : \"\" }, { \"name\" : \"POSTGRES_DB\" , \"value\" : \"\" }, { \"name\" : \"POSTGRES_PORT\" , \"value\" : \"\" }, { \"name\" : \"POSTGRES_HOST\" , \"value\" : \"\" } ] }","title":"Environment variables overrides (for docker container inside the AWS EC2)"},{"location":"dirtyduck/aws_batch/#credentials-filter","text":"Leave this file as is (We will use it for storing the temporal token in deploy.sh ) { \"environment\" : [ { \"name\" : \"AWS_ACCESS_KEY_ID\" , \"value\" : .Credentials.AccessKeyId }, { \"name\" : \"AWS_SECRET_ACCESS_KEY\" , \"value\" : .Credentials.SecretAccessKey }, { \"name\" : \"AWS_SESSION_TOKEN\" , \"value\" : .Credentials.SessionToken } ] }","title":"credentials-filter"},{"location":"dirtyduck/aws_batch/#running-an-experiment","text":"We provided a simple bash file for creating the image, uploading/updating the job definition and running the experiment: ./deploy.sh -h Usage: ./deploy.sh ( -h | -i | -u | -b | -r | -a | --sync_ { to,from } _s3 ) OPTIONS: -h | --help Show this message -i | --info Show information about the environment -b | --update-images Build the triage image and push it to the AWS ECR -u | --update-jobs Update the triage job definition in AWS Batch -r | --run-experiment Run experiments on chile-dt data --sync-to-s3 Uploads the experiments and configuration files to s3://your_project --sync-from-s3 Gets the experiments and configuration files from s3://your_project EXAMPLES: Build and push the images to your AWS ECR: $ ./deploy.sh -b Update the job ' s definitions: $ ./deploy.sh -u Run triage experiments: $ ./deploy.sh -r --experiment_file = s3://your_project/experiments/test.yaml,project_path = s3://your_project/triage,replace = --replace If you have multiple AWS profiles use deploy.sh as follows: [ AWS_PROFILE = your_profile ] ./deploy.sh -r [ job-run-name ] experiment_file = s3:// { your_bucket } /experiments/simple_test_skeleton.yaml,output_path = s3:// { your_bucket } /triage,replace = --no-replace,save_predictions = --no-save-predictions,profile = --profile,validate = --validate Where your_profile is the name of the profile in ~/.aws/credentials","title":"Running an experiment"},{"location":"dirtyduck/aws_batch/#suggested-workflow","text":"The workflow now is: At the beginning of the project Set a docker image and publish it to the AWS ECR (if needed, or you can use the triage official one). You could create different images if you want to run something more tailored to you (like not using the cli interface) Create a job definition and publish it: [ AWS_PROFILE = your_profile ] ./deploy.sh -u You could create different jobs if, for example, you want to have different resources (maybe small resources for testing or a lot of resources for a big experiment) Every time that you have an idea about how to improve the results Create experiment files and publish them to the s3 bucket: [ AWS_PROFILE = your_profile ] ./deploy.sh --synt-to-s3 Run the experiments [ AWS_PROFILE = your_profile ] ./deploy.sh -r [ job-run-name ] experiment_file = s3:// { your_bucket } /experiments/simple_test_skeleton.yaml,output_path = s3:// { your_bucket } /triage,replace = --no-replace,save_predictions = --no-save-predictions,profile = --profile,validate = --validate","title":"Suggested workflow"},{"location":"dirtyduck/choose_your_own_adventure/","text":"How to use this tutorial? # You are interested in the learn how to use triage and have a lot of time: Problem description Infrastructure Data preparation Resource prioritization Early warning systems A deeper look into triage Scaling up You want to know about triage A deeper look into triage Model governance Model selection You want to learn about case studies Quick setup Resource prioritization and/or Early warning systems You already know triage but want to use it on the cloud Scaling up You just want to use the database for your own purposes Quick setup","title":"How to use this tutorial?"},{"location":"dirtyduck/choose_your_own_adventure/#how-to-use-this-tutorial","text":"You are interested in the learn how to use triage and have a lot of time: Problem description Infrastructure Data preparation Resource prioritization Early warning systems A deeper look into triage Scaling up You want to know about triage A deeper look into triage Model governance Model selection You want to learn about case studies Quick setup Resource prioritization and/or Early warning systems You already know triage but want to use it on the cloud Scaling up You just want to use the database for your own purposes Quick setup","title":"How to use this tutorial?"},{"location":"dirtyduck/data_preparation/","text":"Data preparation # We need to get the data and transform it into a shape that is suitable for the analysis. NOTE: Unless we say otherwise, you should run all the following commands inside bastion . Load the data # Before loading the data into the database, verify that the database table is empty by running the following code: select count ( * ) from raw . inspections ; count 0 We will use some postgresql magic in order to get the data in our database. In particular we will use the powerful copy command and the City of Chicago's data API: \\ copy raw . inspections from program 'curl \"https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD\"' HEADER CSV Now, you should have some data: select to_char ( count ( * ), '999,999' ) as \"facilities inspected\" from raw . inspections ; facilities inspected 186,426 You'll probably get a different number because the data are updated every day. Let's peek inside the table 1 select inspection , dba_name , risk , results from raw . inspections limit 1 ; inspection dba_name risk results 2268241 ANTOJITOS PUEBLITA INC Risk 1 (High) Pass w/ Conditions Ok, now you have some data loaded! But we still need to munge it to use it in our machine learning task. Transforming (and cleaning) the data # Rationale # To tackle a machine learning problem, you need to identify the entities for your problem domain. Also, if your problem involves time, you will need to understand how those entities change, either what events happen to the entity or what events the entity affects. We will encode this information into two tables, one named entities and the other named events . The entity, in this example, is the food facility , and the events are the inspections on the facility. The entities table should contain a unique identifier for the entity and some data about that entity (like name, age and status). The events table will include data related to the inspection, including the two most important attributes: its spatial and temporal positions 2 . Before we start the data cleaning, make your life easier by following this rule: Important Do not change the original data The reason is, if you make a mistake or want to try a different data transformation, you can always can go back to the raw data and start over. Data road # The transformation \"road\" that we will take in this tutorial is as follows: Put a copy of the data in the raw schema. (We just did that.) Apply some simple transformations and store the resulting data in the cleaned schema. Organize the data into two unnormalized 3 tables in the semantic schema: events and entities . Run triage . It will create several schemas ( model_metadata , test_results , train_results ). Dataset documentation # Info For an updated version of the documentation of this dataset see Food Protection Services . Info The Food Code Rules (effective 2/1/2019) could be consulted here . The Chicago Food Inspection dataset has documentation here . We can use this documentation to better understand each column's meaning, and the process that generates the data. Most columns are self-explanatory, but some are not 4 : Risk category of facility ( risk ) Each establishment is categorized by its risk of adversely affecting the public\u2019s health, with 1 being the highest and 3 the lowest. The frequency of inspection is tied to this risk, with risk = 1 establishments inspected most frequently and risk = 3 least frequently. Inspection type ( type ) An inspection can be one of the following types: Canvass , the most common type of inspection performed at a frequency relative to the risk of the establishment; Consultation , when the inspection is done at the request of the owner prior to the opening of the establishment; Complaint , when the inspection is done in response to a complaint against the establishment License , when the inspection is done as a requirement for the establishment to receive its license to operate; Suspect food poisoning , when the inspection is done in response to one or more persons claiming to have gotten ill as a result of eating at the establishment (a specific type of complaint-based inspection); Task-force inspection , when an inspection of a bar or tavern is done. Re-inspections can occur for most types of these inspections and are indicated as such. Results ( results ) An inspection can pass, pass with conditions, or fail. Establishments receiving a \u2018pass\u2019 were found to have no critical or serious violations (violation number 1-14 and 15-29, respectively). Establishments receiving a \u2018pass with conditions\u2019 were found to have critical or serious violations, but these were corrected during the inspection. Establishments receiving a \u2018fail\u2019 were found to have critical or serious violations that were not correctable during the inspection. An establishment receiving a \u2018fail\u2019 does not necessarily mean the establishment\u2019s licensed is suspended. Establishments found to be out of business or not located are indicated as such. Important! The result of the inspections (pass, pass with conditions or fail) as well as the violations noted are based on the findings identified and reported by the inspector at the time of the inspection, and may not reflect the findings noted at other times. Violations ( violations ) An establishment can receive one or more of 45 distinct violations (violation numbers 1-44 and 70). For each violation number listed for a given establishment, the requirement the establishment must meet in order for it to NOT receive a violation is noted, followed by a specific description of the findings that caused the violation to be issued . Data Changes On 7/1/2018 the Chicago Department of Public Health\u2019s Food Protection unit changed the definition of violations. The changes don\u2019t affect structurally the dataset (e.g. how the violations are inputted to the database), but the redefinition will change the distribution and interpretation of the violation codes. See here . Data Changes On 2/1/2019 the Chicago Department of Public Health\u2019s Food Protection unit changed the requirements that the facilities must follow. See here We added emphasis to the last one. From these definitions, we can infer the following: risk is related to the frequency of inspections of type canvass . consultation is an inspection before the facility opens (so we can remove it from the data). The same happens with license . complaint and suspected food poisoning are triggered by people. consultation is triggered by the owner of the facility. task-force occurs at bars or taverns. Critical violations are coded between 1-14 , serious violations between 15-29 . We can assume that the violations code 30 and higher are minor violations. (see below) violation describes the problems found, and the comment section describes the steps the facility should take to fix the problem. There are only three possible results of the inspection. (Also, an inspection may not happen if the facility was not located or went out of business). There can be several violations per inspection . Data Changes On 7/1/2018 Critical violation changed to Priority (P) Violation , Serious violation changed to Priority Foundation (PF) Violation and Minor violation changed to Core (C) Violation . Data Changes On 7/1/2018 the number of potential violations has increased from 45 to 63 . Data Changes On 7/1/2018 Corrected Dduring Inspection (CDI) has been changed to Corrected on Site (COS) . Potentially Hazardous Foods (PHF) changed to Time/Temperature Control for Safety Foods (TCS Foods) . Reality check # It is important to verify that the documentation is correct. Let's start by checking that the risk column only has three classifications: NOTE Execute this in psql inside the container bastion . select risk , to_char ( count ( * ), '999,999' ) as \"number of inspections\" from raw . inspections group by risk order by count ( * ) desc ; risk number of inspections Risk 1 (High) 133,170 Risk 2 (Medium) 36,597 Risk 3 (Low) 16,556 \u00a4 75 All 28 Ok, there are two extra risk types, All and NULL , for a grand total of 5 . What about types of inspections? select count ( distinct type ) as \"types of inspections\" from raw . inspections ; types of inspections 108 Wow, there are 108 types of inspections instead of the expected 5 ! What are those types? How bad is it? select type , to_char ( count ( * ), '999,999' ) as \"number of inspections\" from raw . inspections group by type order by count ( * ) desc limit 10 ; type number of inspections Canvass 99,792 License 24,385 Canvass Re-Inspection 19,380 Complaint 17,289 License Re-Inspection 8,572 Complaint Re-Inspection 7,060 Short Form Complaint 6,534 Suspected Food Poisoning 834 Consultation 671 License-Task Force 605 This column will require also cleaning. Finally, let's look results (should be 3) select results , to_char ( count ( * ), '999,999' ) as \"number of inspections\" from raw . inspections group by results order by count ( * ) desc ; results number of inspections Pass 103,528 Fail 35,948 Pass w/ Conditions 23,258 Out of Business 16,212 No Entry 5,784 Not Ready 1,630 Business Not Located 66 Ok, disheartening. But that's the reality of real data. We'll try to clean this mess. Cleaning # Let's look at the data to figure out how we need to transform it. We'll start with all the columns except violations . We'll deal with that one later because it's more complex. First, we'll remove superfluous spaces; convert the columns type, results, dba_name, aka_name, facility_type, address, city to lower case; and clean risk , keeping only the description (e.g. high instead of Risk 1 (High) ). We still need to clean further the column type (which contains more values than the seven mentioned in the documentation: canvass , complaint , license , re-inspection , task-force , consultation , and suspected food poisoning ). For simplicity, we will use regular expressions and ignore re-inspection . For the column risk , we will impute as high all the NULL and All values 5 . As we have seen (and will continue see) through this tutorial, real data are messy ; for example, the column dba_name has several spellings for the same thing: SUBWAY and Subway , MCDONALDS and MC DONALD'S , DUNKIN DONUTS/BASKIN ROBBINS and DUNKIN DONUTS / BASKIN ROBBINS , etc. We could use soundex or machine learning deduplication 6 to clean these names, but we'll go with a very simple cleaning strategy: convert all the names to lowercase, remove the trailing spaces, remove the apostrophe, and remove the spaces around \" / \". It won't completely clean those names, but it's good enough for this example project. Let's review the status of the spatial columns ( state, city, zip, latitude, longitude ). Beginning with state , all the facilities in the data should be located in Illinois : select state , to_char ( count ( * ), '999,999' ) as \"number of inspections\" from raw . inspections group by state ; state number of inspections IL 186,392 \u00a4 34 Ok, almost correct, there are some NULL values. We will assume that the NULL values are actually IL (i.e. we will impute them). Moving to the next spatial column, we expect that all the values in the column city are Chicago 7 : select lower ( city ) as city , to_char ( count ( * ), '999,999' ) as \"number of inspections\" from raw . inspections group by lower ( city ) order by count ( * ) desc limit 10 ; city number of inspections chicago 186,009 \u00a4 161 cchicago 44 schaumburg 23 maywood 16 elk grove village 13 evanston 10 chestnut street 9 cicero 9 inactive 8 Oh boy. There are 150-ish rows with NULL values and forty-ish rows with the value cchicago . Farther down the list (if you dare), we even have chicagochicago . All the values are near Chicago, even if they're in different counties, so we will ignore this column (or equivalently, we will assume that all the records are from Chicago). Zip code has a similar NULL problem: select count ( * ) as \"number of inspections w/o zip code\" from raw . inspections where zip is null or btrim ( zip ) = '' ; number of inspections w/o zip code 75 We could attempt to replace these NULL values using the location point or using similar names of restaurants, but for this tutorial we will remove them. Also, we will convert the coordinates latitude and longitude to a Postgres Point 8 9 10 . We will drop the columns state , latitude , and longitude because the Point contains all that information. We also will remove the column city because almost everything happens in Chicago. If you're keeping count, we are only keeping two columns related to the spatial location of the events: the location of the facility ( location ) and one related to inspection assignments ( zip_code ). Additionally, we will keep the columns wards, historical_wards, census_tracts and community_areas . Each inspection can have multiple violations. To handle that as simply as possible, we'll put violations in their own table. Decisions regarding data We will inspections that occurred before 2018-07-01 . This is due the changes in the types and definition of the violations. See here Finally, we will improve the names of the columns (e.g. results -> result, dba_name -> facility , etc). We will create a new schema called cleaned . The objective of this schema is twofold: to keep our raw data as is 11 and to store our assumptions and cleaning decisions separate from the raw data in a schema that semantically transmits that \"this is our cleaned data.\" The cleaned schema will contain two tables: cleaned.inspections and cleaned.violations . create schema if not exists cleaned ; Then, we will create our mini ETL with our cleaning decisions: Data changes At least from May 2019 the dataset contains news columns: zip_codes, historical_wards, wards, community_areas and census_tracts . The most recent code reflects those changes. create schema if not exists cleaned ; drop table if exists cleaned . inspections cascade ; create table cleaned . inspections as ( with cleaned as ( select inspection :: integer , btrim ( lower ( results )) as result , license_num :: integer , btrim ( lower ( dba_name )) as facility , btrim ( lower ( aka_name )) as facility_aka , case when facility_type is null then 'unknown' else btrim ( lower ( facility_type )) end as facility_type , lower ( substring ( risk from '\\((.+)\\)' )) as risk , btrim ( lower ( address )) as address , zip as zip_code , community_areas as community_area , census_tracts as census_tracts , historical_wards as historical_ward , wards as ward , substring ( btrim ( lower ( regexp_replace ( type , 'liquor' , 'task force' , 'gi' ))) from 'canvass|task force|complaint|food poisoning|consultation|license|tag removal' ) as type , date , -- point(longitude, latitude) as location ST_SetSRID ( ST_MakePoint ( longitude , latitude ), 4326 ):: geography as location -- We use geography so the measurements are in meters from raw . inspections where zip is not null -- removing NULL zip codes and date < '2018-07-01' ) select * from cleaned where type is not null ); The number of inspections now is: select to_char ( count ( inspection ), '999,999,999' ) as \"number of inspections (until 07/01/2018)\" from cleaned . inspections ; number of inspections (until 07/01/2018) 172,052 Note that quantity is smaller than the one from raw.inspections , since we throw away some inspections. With the cleaned.inspections table created, let's take a closer look at the violations column to figure out how to clean it. The first thing to note is that the column violation has a lot of information: it describes the code violation, what's required to address it (see Dataset documentation ), and the inspector's comments. The comments are free text, which means that they can contain line breaks, mispellings, etc. In particular, note that pipes ( | ) seperate multiple violations. The following sql code removes line breaks and multiple spaces and creates an array with all the violations for inspection number 2145736 : select unnest ( string_to_array ( regexp_replace ( violations , '[\\n\\r]+' , ' ' , 'g' ), '|' )) as violations_array from raw . inspections where inspection = '2145736' ; violations array 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: FIRST FLOOR GIRL'S WASHROOM,MIDDLE WASHBOWL SINK FAUCET NOT IN GOOD REPAIR, MUST REPAIR AND MAINTAIN. ONE OUT OF TWO HAND DRYER NOT WORKING IN THE FOLLOWING WASHROOM: FIRST FLOOR BOY'S AND GIRL'S WASHROOM, AND BOY'S AND GIRL'S WASHROOM 2ND FLOOR. MUST REPAIR AND MAINTAIN. 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: DAMAGED FLOOR INSIDE THE BOY'S AND GIRL'S WASHROOM 2ND FLOOR. MUST REPAIR, MAKE THE FLOOR SMOOTH EASILY CLEANABLE. 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: MISSING PART OF THE COVING(BASEBOARD) BY THE EXPOSED HAND SINK IN THE KITCHEN. MUST REPAIR AND MAINTAIN. WATER STAINED CEILING TILES IN THE LUNCH ROOM. MUST REPLACE CEILING TILES AND MAINTAIN. PEELING PAINT ON THE CEILING AND WALLS THROUGHOUT THE SCHOOL. HALLWAYS, INSIDE THE CLASSROOMS, INSIDE THE WASHROOMS IN ALL FLOORS. INSTRUCTED TO SCRAPE PEELING PAINT AND RE PAINT. This little piece of code is doing a lot: first it replaces all the line breaks [\\n\\r]+ with spaces, then, it splits the string using the pipe and stores it in an array ( string_to_array ), finally it returns every violation description in a row ( unnest ). From this, we can learn that the structure of the violations column follows: If there are several violations reported, those violations will be separated by '|' Every violation begins with a code and a description Every violation can have comments , which appear after the string - Comments: We will create a new table called cleaned.violations to store inspection code description comments drop table if exists cleaned . violations cascade ; create table cleaned . violations as ( select inspection :: integer , license_num :: integer , date :: date , btrim ( tuple [ 1 ]) as code , lower ( btrim ( tuple [ 2 ])) as description , lower ( btrim ( tuple [ 3 ])) as comment , ( case when btrim ( tuple [ 1 ]) = '' then NULL when btrim ( tuple [ 1 ]):: int between 1 and 14 then 'critical' -- From the documentation when btrim ( tuple [ 1 ]):: int between 15 and 29 then 'serious' else 'minor' end ) as severity from ( select inspection , license_num , date , regexp_split_to_array ( -- Create an array we will split the code, description, comment regexp_split_to_table ( -- Create a row per each comment we split by | coalesce ( -- If there isn't a violation add '- Comments:' regexp_replace ( violations , '[\\n\\r]+' , '' , 'g' ) -- Remove line breaks , '- Comments:' ) , '\\|' ) -- Split the violations , '(?<=\\d+)\\.\\s*|\\s*-\\s*Comments:' ) -- Split each violation in three -- , '\\.\\s*|\\s*-\\s*Comments:') -- Split each violation in three (Use this if your postgresql is kind off old as tuple from raw . inspections where results in ( 'Fail' , 'Pass' , 'Pass w/ Conditions' ) and license_num is not null ) as t ); This code is in /sql/create_violations_table.sql . We can verify the result of the previous script select inspection , date , code , description from cleaned . violations where inspection = 2145736 order by code asc ; inspection date code description 2145736 2018-03-01 32 food and non-food contact surfaces properly designed, constructed and maintained 2145736 2018-03-01 34 floors: constructed per code, cleaned, good repair, coving installed, dust-less cleaning methods used 2145736 2018-03-01 35 walls, ceilings, attached equipment constructed per code: good repair, surfaces clean and dust-less cleaning methods If everything worked correctly you should be able to run the following code 12 : select case when grouping ( severity ) = 1 then 'TOTAL' else severity end as severity , to_char ( count ( * ), '999,999,999' ) as \"number of inspections\" from cleaned . violations group by rollup ( severity ) order by severity nulls first ; severity number of inspections \u00a4 26,415 critical 51,486 minor 478,340 serious 55,583 TOTAL 611,824 As a last step, we should create from the cleaned tables the entities and events tables. Semantic tables # Entities table # The entities table should uniquely identify each facility and contain descriptive attributes. First, we should investigate how we can uniquely identify a facility. Let's hope it's easy 13 . Let's start with the obvious option. Perhaps license_num is a unique identifier. Let's confirm our hypothesis with some queries. We will begin with the following query: What are 5 licenses with the most inspections? select license_num , to_char ( count ( * ), '999,999,999' ) as \"number of inspections\" , coalesce ( count ( * ) filter ( where result = 'fail' ), 0 ) as \"number of failed inspections\" from cleaned . inspections group by license_num order by count ( * ) desc limit 5 ; license num number of inspections number of failed inspections 0 442 111 1354323 192 1 14616 174 31 1574001 82 4 1974745 59 3 This looks weird. There are three license numbers, in particular license number 0 , that have many more inspections than the rest. Let's investigate license_num = 0 . select facility_type , count ( * ) as \"number of inspections\" , coalesce ( count ( * ) filter ( where result = 'fail' ), 0 ) as \"number of failed inspections\" from cleaned . inspections where license_num = 0 group by facility_type order by \"number of inspections\" desc limit 10 ; facility type number of inspections number of failed inspections restaurant 103 43 special event 70 8 unknown 44 10 shelter 31 6 navy pier kiosk 30 4 church 30 3 grocery store 16 7 school 13 1 long term care 11 2 church kitchen 11 4 It seems that license_number 0 is a generic placeholder: Most of these are related to special events , churches , festivals , etc. But what about the restaurants that have license_num = 0 ? Are those the same restaurant? select license_num , facility , address , count ( * ) as \"number of inspections\" , coalesce ( count ( * ) filter ( where result = 'fail' ), 0 ) as \"number of failed inspections\" from cleaned . inspections where license_num = 0 and facility_type = 'restaurant' group by license_num , facility , address order by \"number of inspections\" desc limit 10 ; license num facility address number of inspections number of failed inspections 0 british airways 11601 w touhy ave 5 1 0 rib lady 2 4203 w cermak rd 4 3 0 taqueria la capital 3508 w 63 rd st 3 1 0 nutricion familiar 3000 w 59 th st 3 1 0 salvation army 506 n des plaines st 3 1 0 herbalife 6214 w diversey ave 3 2 0 la michoacana 4346 s california ave 3 1 0 las quecas 2500 s christiana ave 3 1 0 mrs ts southern fried chicken 3343 n broadway 3 1 0 unlicensed 7559 n ridge blvd 3 1 Nope. Unfortunately, license_num is not a unique identifier. Perhaps license_num and address are a unique identifier. select to_char ( count ( distinct license_num ), '999,999' ) as \"number of licenses\" , to_char ( count ( distinct facility ), '999,999' ) as \"number of facilities\" , to_char ( count ( distinct address ), '999,999' ) as \"number of addresses\" from cleaned . inspections ; number of licenses number of facilities number of addresses 34,364 25,371 17,252 We were expecting (naively) that we should get one license_num per facility per address , but that isn't the case. Perhaps several facilities share a name (e.g. \"Subway\" or \"McDonalds\") or license, or perhaps several facilities share the same address, such as facilities at the stadium or the airport. We will try to use the combination of license_num , facility , facility_aka , facility_type , and address to identify a facility: select license_num , facility , facility_type , facility_aka , address , count ( * ) as \"number of inspections\" from cleaned . inspections group by license_num , facility , facility_type , facility_aka , address order by count ( * ) desc , facility , facility_aka , address , license_num , facility_type limit 10 ; license_num facility facility_type facility_aka address number of inspections 1490035 mcdonald's restaurant mcdonald's 6900 s lafayette ave 46 1142451 jewel food store # 3345 grocery store jewel food store # 3345 1224 s wabash ave 45 1596210 food 4 less midwest #552 grocery store food 4 less 7030 s ashland ave 44 2083833 mariano's fresh market #8503 grocery store mariano's fresh market 333 e benton pl 41 1302136 mcdonald's restaurant mcdonald's 70 e garfield blvd 40 1476553 pete's produce grocery store pete's produce 1543 e 87 th st 40 1000572 jewel food store #3030 grocery store jewel food store #3030 7530 s stony island ave 39 1094 one stop food & liquor store grocery store one stop food & liquor store 4301-4323 s lake park ave 39 60184 taqueria el ranchito restaurant taqueria el ranchito 2829 n milwaukee ave 39 9154 jimmy g's restaurant jimmy g's 307 s kedzie ave 37 Yay, it looks like these columns enable us to identify a facility! 14 The entities table should store two other types of attributes. The first type describe the entity no matter the time. If the entity were a person, date of birth would be an example but age would not because the latter changes but the former does not. We'll include zip_code and location as two facility attributes. The second type describes when the entity is available for action (e.g. inspection). In this case, the columns start_time, end_time describe the interval in which the facility is in business or active . These columns are important because we don't want to make predictions for inactive entities. The data don't contain active/inactive date columns, so we will use the date of the facility's first inspection as start_time , and either NULL or the date of inspection if the result was out of business or business not located as end_time . create schema if not exists semantic ; drop table if exists semantic . entities cascade ; create table semantic . entities as ( with entities as ( select distinct on ( license_num , facility , facility_aka , facility_type , address ) license_num , facility , facility_aka , facility_type , address , zip_code , location , min ( date ) over ( partition by license_num , facility , facility_aka , facility_type , address ) as start_time , max ( case when result in ( 'out of business' , 'business not located' ) then date else NULL end ) over ( partition by license_num , facility , facility_aka , address ) as end_time from cleaned . inspections order by license_num asc , facility asc , facility_aka asc , facility_type asc , address asc , date asc -- IMPORTANT!! ) select row_number () over ( order by start_time asc , license_num asc , facility asc , facility_aka asc , facility_type asc , address asc ) as entity_id , license_num , facility , facility_aka , facility_type , address , zip_code , location , start_time , end_time , daterange ( start_time , end_time ) as activity_period from entities ); Note that we added a unique identifier ( entity_id ) to this table. This identifier was assigned using a PostgreSQL idiom: distinct on() . DISTINCT ON keeps the \"first\" row of each group. If you are interested in this powerful technique see this blogpost . select to_char ( count ( entity_id ), '999,999' ) as entities from semantic . entities ; entities 35,668 We will add some indexes to this table 15 : create index entities_ix on semantic . entities ( entity_id ); create index entities_license_num_ix on semantic . entities ( license_num ); create index entities_facility_ix on semantic . entities ( facility ); create index entities_facility_type_ix on semantic . entities ( facility_type ); create index entities_zip_code_ix on semantic . entities ( zip_code ); -- Spatial index create index entities_location_gix on semantic . entities using gist ( location ); create index entities_full_key_ix on semantic . entities ( license_num , facility , facility_aka , facility_type , address ); Events table # We are ready to create the events table. This table will describe the inspection, like the type of inspection, when and where the inspection happened, and the inspection result . We will add the violations as a JSONB column 16 . Finally, we'll rename inspection as event_id 17 . drop table if exists semantic . events cascade ; create table semantic . events as ( with entities as ( select * from semantic . entities ), inspections as ( select i . inspection , i . type , i . date , i . risk , i . result , i . license_num , i . facility , i . facility_aka , i . facility_type , i . address , i . zip_code , i . location , jsonb_agg ( jsonb_build_object ( 'code' , v . code , 'severity' , v . severity , 'description' , v . description , 'comment' , v . comment ) order by code ) as violations from cleaned . inspections as i inner join cleaned . violations as v on i . inspection = v . inspection group by i . inspection , i . type , i . license_num , i . facility , i . facility_aka , i . facility_type , i . address , i . zip_code , i . location , i . date , i . risk , i . result ) select i . inspection as event_id , e . entity_id , i . type , i . date , i . risk , i . result , e . facility_type , e . zip_code , e . location , i . violations from entities as e inner join inspections as i using ( license_num , facility , facility_aka , facility_type , address , zip_code ) ); -- Add some indices create index events_entity_ix on semantic . events ( entity_id asc nulls last ); create index events_event_ix on semantic . events ( event_id asc nulls last ); create index events_type_ix on semantic . events ( type ); create index events_date_ix on semantic . events ( date asc nulls last ); create index events_facility_type_ix on semantic . events ( facility_type ); create index events_zip_code_ix on semantic . events ( zip_code ); -- Spatial index create index events_location_gix on semantic . events using gist ( location ); -- JSONB indices create index events_violations on semantic . events using gin ( violations ); create index events_violations_json_path on semantic . events using gin ( violations jsonb_path_ops ); create index events_event_entity_zip_code_date on semantic . events ( event_id asc nulls last , entity_id asc nulls last , zip_code , date desc nulls last ); Success! We have one row per event 18 Our semantic data looks like: select event_id , entity_id , type , date , risk , result , facility_type , zip_code from semantic . events limit 1 ; event_id entity_id type date risk result facility_type zip_code 1343315 22054 canvass 2013-06-06 low fail newsstand 60623 We omitted violations and location for brevity. The total number of inspections is select to_char ( count ( event_id ), '999,999,999' ) as events from semantic . events ; events 148,724 Now that we have our data in a good shape, we are ready to use Triage . Let's EDA \u2026 # It is always a good idea to do some Exploratory Data Analysis 19 or EDA for short. This will help us to learn more about the dynamics of the entities or the inspections. We will generate a few plots, just to know: how many entities/events are every month? how many entities/events ended in a failed state every month? and, how many entities/events have in a critical violation in a failed inspection? Inspections over time # First, we will try the answer the question: how many inspections are realized every month? Number of facilities inspected over time # The previous plot was about the number of events every month, now we will plot how many entities are acted every month. One question, that is useful to answer is: Are there facilities that are inspected more than once in a month? Note We are doing an emphasis in inspected since our data set doesn't contain all the facilities in Chicago. This will have an effect on the modeling stage. Number of failed inspections over time # What is the proportion of inspections every month that actually end in a failed inspection? Number of facilities with failed inspections over time # Now let's see the behavior of the outcomes of the inspection across time. First just if the inspection failed. Number of severe violations found in a failed inspection over time # Finally let's analyze the evolution of failed inspections with severe violations (violation code in 15-29) Number of facilities with severe violations found in a failed inspection over time # This few plots give us a sense of how the data behaves and will help us in detect weird bugs or model-behavior later. What\u2019s next? # Learn more about triage Learn more about early warning systems Learn more about resource prioritization systems If you want to try different columns (or you don't remember which columns try \\d raw.inspectios first \u21a9 We are following the event's definition from physics : \"an event is the instantaneous physical situation or occurrence associated with a point in spacetime\" \u21a9 It will make your life easier and most of the Machine Learning algorithms only accept data in matrix form (i.e. one big table) \u21a9 Verbatim from the datasource documentation. \u21a9 A controversial decision, I know. \u21a9 This problem is related to the process of deduplication and there is another DSaPP tool for that: matching-tool . \u21a9 It is the Chicago Food Inspections dataset, after all. \u21a9 We could also use the default geometric data type from postgresql: point ( https://www.postgresql.org/docs/10/datatype-geometric.html ) \u21a9 We will store the Point as a geography object. As a result, spatial database operations (like calculating the distances between two facilities) will return answers in meters instead of degrees. See this . \u21a9 As a real geographical object check the PostGIS documentation \u21a9 Remember our tip at the beginning of this section! \u21a9 If the code looks funny to you, it is because we are using grouping sets , in particular rollup . See the docs. \u21a9 Yeah, you wish \u21a9 Almost. At least good for this tutorial. Look carefully. \u21a9 ALWAYS add indexes to your tables! \u21a9 If you want to have a deep explanation about why is this good check this blog post \u21a9 As a general rule I hate to add the suffix _id , I would rather prefer to name them as event and entity instead of event_id and entity_id . But triage named those columns in that way and for that we are stuck with that nomenclature. \u21a9 This will simplify the creation of features for our machine learning models. \u21a9 Defined by John Tukey as: Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. \u21a9","title":"Data preparation"},{"location":"dirtyduck/data_preparation/#data-preparation","text":"We need to get the data and transform it into a shape that is suitable for the analysis. NOTE: Unless we say otherwise, you should run all the following commands inside bastion .","title":"Data preparation"},{"location":"dirtyduck/data_preparation/#load-the-data","text":"Before loading the data into the database, verify that the database table is empty by running the following code: select count ( * ) from raw . inspections ; count 0 We will use some postgresql magic in order to get the data in our database. In particular we will use the powerful copy command and the City of Chicago's data API: \\ copy raw . inspections from program 'curl \"https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD\"' HEADER CSV Now, you should have some data: select to_char ( count ( * ), '999,999' ) as \"facilities inspected\" from raw . inspections ; facilities inspected 186,426 You'll probably get a different number because the data are updated every day. Let's peek inside the table 1 select inspection , dba_name , risk , results from raw . inspections limit 1 ; inspection dba_name risk results 2268241 ANTOJITOS PUEBLITA INC Risk 1 (High) Pass w/ Conditions Ok, now you have some data loaded! But we still need to munge it to use it in our machine learning task.","title":"Load the data"},{"location":"dirtyduck/data_preparation/#transforming-and-cleaning-the-data","text":"","title":"Transforming (and cleaning) the data"},{"location":"dirtyduck/data_preparation/#rationale","text":"To tackle a machine learning problem, you need to identify the entities for your problem domain. Also, if your problem involves time, you will need to understand how those entities change, either what events happen to the entity or what events the entity affects. We will encode this information into two tables, one named entities and the other named events . The entity, in this example, is the food facility , and the events are the inspections on the facility. The entities table should contain a unique identifier for the entity and some data about that entity (like name, age and status). The events table will include data related to the inspection, including the two most important attributes: its spatial and temporal positions 2 . Before we start the data cleaning, make your life easier by following this rule: Important Do not change the original data The reason is, if you make a mistake or want to try a different data transformation, you can always can go back to the raw data and start over.","title":"Rationale"},{"location":"dirtyduck/data_preparation/#data-road","text":"The transformation \"road\" that we will take in this tutorial is as follows: Put a copy of the data in the raw schema. (We just did that.) Apply some simple transformations and store the resulting data in the cleaned schema. Organize the data into two unnormalized 3 tables in the semantic schema: events and entities . Run triage . It will create several schemas ( model_metadata , test_results , train_results ).","title":"Data road"},{"location":"dirtyduck/data_preparation/#dataset-documentation","text":"Info For an updated version of the documentation of this dataset see Food Protection Services . Info The Food Code Rules (effective 2/1/2019) could be consulted here . The Chicago Food Inspection dataset has documentation here . We can use this documentation to better understand each column's meaning, and the process that generates the data. Most columns are self-explanatory, but some are not 4 : Risk category of facility ( risk ) Each establishment is categorized by its risk of adversely affecting the public\u2019s health, with 1 being the highest and 3 the lowest. The frequency of inspection is tied to this risk, with risk = 1 establishments inspected most frequently and risk = 3 least frequently. Inspection type ( type ) An inspection can be one of the following types: Canvass , the most common type of inspection performed at a frequency relative to the risk of the establishment; Consultation , when the inspection is done at the request of the owner prior to the opening of the establishment; Complaint , when the inspection is done in response to a complaint against the establishment License , when the inspection is done as a requirement for the establishment to receive its license to operate; Suspect food poisoning , when the inspection is done in response to one or more persons claiming to have gotten ill as a result of eating at the establishment (a specific type of complaint-based inspection); Task-force inspection , when an inspection of a bar or tavern is done. Re-inspections can occur for most types of these inspections and are indicated as such. Results ( results ) An inspection can pass, pass with conditions, or fail. Establishments receiving a \u2018pass\u2019 were found to have no critical or serious violations (violation number 1-14 and 15-29, respectively). Establishments receiving a \u2018pass with conditions\u2019 were found to have critical or serious violations, but these were corrected during the inspection. Establishments receiving a \u2018fail\u2019 were found to have critical or serious violations that were not correctable during the inspection. An establishment receiving a \u2018fail\u2019 does not necessarily mean the establishment\u2019s licensed is suspended. Establishments found to be out of business or not located are indicated as such. Important! The result of the inspections (pass, pass with conditions or fail) as well as the violations noted are based on the findings identified and reported by the inspector at the time of the inspection, and may not reflect the findings noted at other times. Violations ( violations ) An establishment can receive one or more of 45 distinct violations (violation numbers 1-44 and 70). For each violation number listed for a given establishment, the requirement the establishment must meet in order for it to NOT receive a violation is noted, followed by a specific description of the findings that caused the violation to be issued . Data Changes On 7/1/2018 the Chicago Department of Public Health\u2019s Food Protection unit changed the definition of violations. The changes don\u2019t affect structurally the dataset (e.g. how the violations are inputted to the database), but the redefinition will change the distribution and interpretation of the violation codes. See here . Data Changes On 2/1/2019 the Chicago Department of Public Health\u2019s Food Protection unit changed the requirements that the facilities must follow. See here We added emphasis to the last one. From these definitions, we can infer the following: risk is related to the frequency of inspections of type canvass . consultation is an inspection before the facility opens (so we can remove it from the data). The same happens with license . complaint and suspected food poisoning are triggered by people. consultation is triggered by the owner of the facility. task-force occurs at bars or taverns. Critical violations are coded between 1-14 , serious violations between 15-29 . We can assume that the violations code 30 and higher are minor violations. (see below) violation describes the problems found, and the comment section describes the steps the facility should take to fix the problem. There are only three possible results of the inspection. (Also, an inspection may not happen if the facility was not located or went out of business). There can be several violations per inspection . Data Changes On 7/1/2018 Critical violation changed to Priority (P) Violation , Serious violation changed to Priority Foundation (PF) Violation and Minor violation changed to Core (C) Violation . Data Changes On 7/1/2018 the number of potential violations has increased from 45 to 63 . Data Changes On 7/1/2018 Corrected Dduring Inspection (CDI) has been changed to Corrected on Site (COS) . Potentially Hazardous Foods (PHF) changed to Time/Temperature Control for Safety Foods (TCS Foods) .","title":"Dataset documentation"},{"location":"dirtyduck/data_preparation/#reality-check","text":"It is important to verify that the documentation is correct. Let's start by checking that the risk column only has three classifications: NOTE Execute this in psql inside the container bastion . select risk , to_char ( count ( * ), '999,999' ) as \"number of inspections\" from raw . inspections group by risk order by count ( * ) desc ; risk number of inspections Risk 1 (High) 133,170 Risk 2 (Medium) 36,597 Risk 3 (Low) 16,556 \u00a4 75 All 28 Ok, there are two extra risk types, All and NULL , for a grand total of 5 . What about types of inspections? select count ( distinct type ) as \"types of inspections\" from raw . inspections ; types of inspections 108 Wow, there are 108 types of inspections instead of the expected 5 ! What are those types? How bad is it? select type , to_char ( count ( * ), '999,999' ) as \"number of inspections\" from raw . inspections group by type order by count ( * ) desc limit 10 ; type number of inspections Canvass 99,792 License 24,385 Canvass Re-Inspection 19,380 Complaint 17,289 License Re-Inspection 8,572 Complaint Re-Inspection 7,060 Short Form Complaint 6,534 Suspected Food Poisoning 834 Consultation 671 License-Task Force 605 This column will require also cleaning. Finally, let's look results (should be 3) select results , to_char ( count ( * ), '999,999' ) as \"number of inspections\" from raw . inspections group by results order by count ( * ) desc ; results number of inspections Pass 103,528 Fail 35,948 Pass w/ Conditions 23,258 Out of Business 16,212 No Entry 5,784 Not Ready 1,630 Business Not Located 66 Ok, disheartening. But that's the reality of real data. We'll try to clean this mess.","title":"Reality check"},{"location":"dirtyduck/data_preparation/#cleaning","text":"Let's look at the data to figure out how we need to transform it. We'll start with all the columns except violations . We'll deal with that one later because it's more complex. First, we'll remove superfluous spaces; convert the columns type, results, dba_name, aka_name, facility_type, address, city to lower case; and clean risk , keeping only the description (e.g. high instead of Risk 1 (High) ). We still need to clean further the column type (which contains more values than the seven mentioned in the documentation: canvass , complaint , license , re-inspection , task-force , consultation , and suspected food poisoning ). For simplicity, we will use regular expressions and ignore re-inspection . For the column risk , we will impute as high all the NULL and All values 5 . As we have seen (and will continue see) through this tutorial, real data are messy ; for example, the column dba_name has several spellings for the same thing: SUBWAY and Subway , MCDONALDS and MC DONALD'S , DUNKIN DONUTS/BASKIN ROBBINS and DUNKIN DONUTS / BASKIN ROBBINS , etc. We could use soundex or machine learning deduplication 6 to clean these names, but we'll go with a very simple cleaning strategy: convert all the names to lowercase, remove the trailing spaces, remove the apostrophe, and remove the spaces around \" / \". It won't completely clean those names, but it's good enough for this example project. Let's review the status of the spatial columns ( state, city, zip, latitude, longitude ). Beginning with state , all the facilities in the data should be located in Illinois : select state , to_char ( count ( * ), '999,999' ) as \"number of inspections\" from raw . inspections group by state ; state number of inspections IL 186,392 \u00a4 34 Ok, almost correct, there are some NULL values. We will assume that the NULL values are actually IL (i.e. we will impute them). Moving to the next spatial column, we expect that all the values in the column city are Chicago 7 : select lower ( city ) as city , to_char ( count ( * ), '999,999' ) as \"number of inspections\" from raw . inspections group by lower ( city ) order by count ( * ) desc limit 10 ; city number of inspections chicago 186,009 \u00a4 161 cchicago 44 schaumburg 23 maywood 16 elk grove village 13 evanston 10 chestnut street 9 cicero 9 inactive 8 Oh boy. There are 150-ish rows with NULL values and forty-ish rows with the value cchicago . Farther down the list (if you dare), we even have chicagochicago . All the values are near Chicago, even if they're in different counties, so we will ignore this column (or equivalently, we will assume that all the records are from Chicago). Zip code has a similar NULL problem: select count ( * ) as \"number of inspections w/o zip code\" from raw . inspections where zip is null or btrim ( zip ) = '' ; number of inspections w/o zip code 75 We could attempt to replace these NULL values using the location point or using similar names of restaurants, but for this tutorial we will remove them. Also, we will convert the coordinates latitude and longitude to a Postgres Point 8 9 10 . We will drop the columns state , latitude , and longitude because the Point contains all that information. We also will remove the column city because almost everything happens in Chicago. If you're keeping count, we are only keeping two columns related to the spatial location of the events: the location of the facility ( location ) and one related to inspection assignments ( zip_code ). Additionally, we will keep the columns wards, historical_wards, census_tracts and community_areas . Each inspection can have multiple violations. To handle that as simply as possible, we'll put violations in their own table. Decisions regarding data We will inspections that occurred before 2018-07-01 . This is due the changes in the types and definition of the violations. See here Finally, we will improve the names of the columns (e.g. results -> result, dba_name -> facility , etc). We will create a new schema called cleaned . The objective of this schema is twofold: to keep our raw data as is 11 and to store our assumptions and cleaning decisions separate from the raw data in a schema that semantically transmits that \"this is our cleaned data.\" The cleaned schema will contain two tables: cleaned.inspections and cleaned.violations . create schema if not exists cleaned ; Then, we will create our mini ETL with our cleaning decisions: Data changes At least from May 2019 the dataset contains news columns: zip_codes, historical_wards, wards, community_areas and census_tracts . The most recent code reflects those changes. create schema if not exists cleaned ; drop table if exists cleaned . inspections cascade ; create table cleaned . inspections as ( with cleaned as ( select inspection :: integer , btrim ( lower ( results )) as result , license_num :: integer , btrim ( lower ( dba_name )) as facility , btrim ( lower ( aka_name )) as facility_aka , case when facility_type is null then 'unknown' else btrim ( lower ( facility_type )) end as facility_type , lower ( substring ( risk from '\\((.+)\\)' )) as risk , btrim ( lower ( address )) as address , zip as zip_code , community_areas as community_area , census_tracts as census_tracts , historical_wards as historical_ward , wards as ward , substring ( btrim ( lower ( regexp_replace ( type , 'liquor' , 'task force' , 'gi' ))) from 'canvass|task force|complaint|food poisoning|consultation|license|tag removal' ) as type , date , -- point(longitude, latitude) as location ST_SetSRID ( ST_MakePoint ( longitude , latitude ), 4326 ):: geography as location -- We use geography so the measurements are in meters from raw . inspections where zip is not null -- removing NULL zip codes and date < '2018-07-01' ) select * from cleaned where type is not null ); The number of inspections now is: select to_char ( count ( inspection ), '999,999,999' ) as \"number of inspections (until 07/01/2018)\" from cleaned . inspections ; number of inspections (until 07/01/2018) 172,052 Note that quantity is smaller than the one from raw.inspections , since we throw away some inspections. With the cleaned.inspections table created, let's take a closer look at the violations column to figure out how to clean it. The first thing to note is that the column violation has a lot of information: it describes the code violation, what's required to address it (see Dataset documentation ), and the inspector's comments. The comments are free text, which means that they can contain line breaks, mispellings, etc. In particular, note that pipes ( | ) seperate multiple violations. The following sql code removes line breaks and multiple spaces and creates an array with all the violations for inspection number 2145736 : select unnest ( string_to_array ( regexp_replace ( violations , '[\\n\\r]+' , ' ' , 'g' ), '|' )) as violations_array from raw . inspections where inspection = '2145736' ; violations array 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: FIRST FLOOR GIRL'S WASHROOM,MIDDLE WASHBOWL SINK FAUCET NOT IN GOOD REPAIR, MUST REPAIR AND MAINTAIN. ONE OUT OF TWO HAND DRYER NOT WORKING IN THE FOLLOWING WASHROOM: FIRST FLOOR BOY'S AND GIRL'S WASHROOM, AND BOY'S AND GIRL'S WASHROOM 2ND FLOOR. MUST REPAIR AND MAINTAIN. 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: DAMAGED FLOOR INSIDE THE BOY'S AND GIRL'S WASHROOM 2ND FLOOR. MUST REPAIR, MAKE THE FLOOR SMOOTH EASILY CLEANABLE. 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: MISSING PART OF THE COVING(BASEBOARD) BY THE EXPOSED HAND SINK IN THE KITCHEN. MUST REPAIR AND MAINTAIN. WATER STAINED CEILING TILES IN THE LUNCH ROOM. MUST REPLACE CEILING TILES AND MAINTAIN. PEELING PAINT ON THE CEILING AND WALLS THROUGHOUT THE SCHOOL. HALLWAYS, INSIDE THE CLASSROOMS, INSIDE THE WASHROOMS IN ALL FLOORS. INSTRUCTED TO SCRAPE PEELING PAINT AND RE PAINT. This little piece of code is doing a lot: first it replaces all the line breaks [\\n\\r]+ with spaces, then, it splits the string using the pipe and stores it in an array ( string_to_array ), finally it returns every violation description in a row ( unnest ). From this, we can learn that the structure of the violations column follows: If there are several violations reported, those violations will be separated by '|' Every violation begins with a code and a description Every violation can have comments , which appear after the string - Comments: We will create a new table called cleaned.violations to store inspection code description comments drop table if exists cleaned . violations cascade ; create table cleaned . violations as ( select inspection :: integer , license_num :: integer , date :: date , btrim ( tuple [ 1 ]) as code , lower ( btrim ( tuple [ 2 ])) as description , lower ( btrim ( tuple [ 3 ])) as comment , ( case when btrim ( tuple [ 1 ]) = '' then NULL when btrim ( tuple [ 1 ]):: int between 1 and 14 then 'critical' -- From the documentation when btrim ( tuple [ 1 ]):: int between 15 and 29 then 'serious' else 'minor' end ) as severity from ( select inspection , license_num , date , regexp_split_to_array ( -- Create an array we will split the code, description, comment regexp_split_to_table ( -- Create a row per each comment we split by | coalesce ( -- If there isn't a violation add '- Comments:' regexp_replace ( violations , '[\\n\\r]+' , '' , 'g' ) -- Remove line breaks , '- Comments:' ) , '\\|' ) -- Split the violations , '(?<=\\d+)\\.\\s*|\\s*-\\s*Comments:' ) -- Split each violation in three -- , '\\.\\s*|\\s*-\\s*Comments:') -- Split each violation in three (Use this if your postgresql is kind off old as tuple from raw . inspections where results in ( 'Fail' , 'Pass' , 'Pass w/ Conditions' ) and license_num is not null ) as t ); This code is in /sql/create_violations_table.sql . We can verify the result of the previous script select inspection , date , code , description from cleaned . violations where inspection = 2145736 order by code asc ; inspection date code description 2145736 2018-03-01 32 food and non-food contact surfaces properly designed, constructed and maintained 2145736 2018-03-01 34 floors: constructed per code, cleaned, good repair, coving installed, dust-less cleaning methods used 2145736 2018-03-01 35 walls, ceilings, attached equipment constructed per code: good repair, surfaces clean and dust-less cleaning methods If everything worked correctly you should be able to run the following code 12 : select case when grouping ( severity ) = 1 then 'TOTAL' else severity end as severity , to_char ( count ( * ), '999,999,999' ) as \"number of inspections\" from cleaned . violations group by rollup ( severity ) order by severity nulls first ; severity number of inspections \u00a4 26,415 critical 51,486 minor 478,340 serious 55,583 TOTAL 611,824 As a last step, we should create from the cleaned tables the entities and events tables.","title":"Cleaning"},{"location":"dirtyduck/data_preparation/#semantic-tables","text":"","title":"Semantic tables"},{"location":"dirtyduck/data_preparation/#entities-table","text":"The entities table should uniquely identify each facility and contain descriptive attributes. First, we should investigate how we can uniquely identify a facility. Let's hope it's easy 13 . Let's start with the obvious option. Perhaps license_num is a unique identifier. Let's confirm our hypothesis with some queries. We will begin with the following query: What are 5 licenses with the most inspections? select license_num , to_char ( count ( * ), '999,999,999' ) as \"number of inspections\" , coalesce ( count ( * ) filter ( where result = 'fail' ), 0 ) as \"number of failed inspections\" from cleaned . inspections group by license_num order by count ( * ) desc limit 5 ; license num number of inspections number of failed inspections 0 442 111 1354323 192 1 14616 174 31 1574001 82 4 1974745 59 3 This looks weird. There are three license numbers, in particular license number 0 , that have many more inspections than the rest. Let's investigate license_num = 0 . select facility_type , count ( * ) as \"number of inspections\" , coalesce ( count ( * ) filter ( where result = 'fail' ), 0 ) as \"number of failed inspections\" from cleaned . inspections where license_num = 0 group by facility_type order by \"number of inspections\" desc limit 10 ; facility type number of inspections number of failed inspections restaurant 103 43 special event 70 8 unknown 44 10 shelter 31 6 navy pier kiosk 30 4 church 30 3 grocery store 16 7 school 13 1 long term care 11 2 church kitchen 11 4 It seems that license_number 0 is a generic placeholder: Most of these are related to special events , churches , festivals , etc. But what about the restaurants that have license_num = 0 ? Are those the same restaurant? select license_num , facility , address , count ( * ) as \"number of inspections\" , coalesce ( count ( * ) filter ( where result = 'fail' ), 0 ) as \"number of failed inspections\" from cleaned . inspections where license_num = 0 and facility_type = 'restaurant' group by license_num , facility , address order by \"number of inspections\" desc limit 10 ; license num facility address number of inspections number of failed inspections 0 british airways 11601 w touhy ave 5 1 0 rib lady 2 4203 w cermak rd 4 3 0 taqueria la capital 3508 w 63 rd st 3 1 0 nutricion familiar 3000 w 59 th st 3 1 0 salvation army 506 n des plaines st 3 1 0 herbalife 6214 w diversey ave 3 2 0 la michoacana 4346 s california ave 3 1 0 las quecas 2500 s christiana ave 3 1 0 mrs ts southern fried chicken 3343 n broadway 3 1 0 unlicensed 7559 n ridge blvd 3 1 Nope. Unfortunately, license_num is not a unique identifier. Perhaps license_num and address are a unique identifier. select to_char ( count ( distinct license_num ), '999,999' ) as \"number of licenses\" , to_char ( count ( distinct facility ), '999,999' ) as \"number of facilities\" , to_char ( count ( distinct address ), '999,999' ) as \"number of addresses\" from cleaned . inspections ; number of licenses number of facilities number of addresses 34,364 25,371 17,252 We were expecting (naively) that we should get one license_num per facility per address , but that isn't the case. Perhaps several facilities share a name (e.g. \"Subway\" or \"McDonalds\") or license, or perhaps several facilities share the same address, such as facilities at the stadium or the airport. We will try to use the combination of license_num , facility , facility_aka , facility_type , and address to identify a facility: select license_num , facility , facility_type , facility_aka , address , count ( * ) as \"number of inspections\" from cleaned . inspections group by license_num , facility , facility_type , facility_aka , address order by count ( * ) desc , facility , facility_aka , address , license_num , facility_type limit 10 ; license_num facility facility_type facility_aka address number of inspections 1490035 mcdonald's restaurant mcdonald's 6900 s lafayette ave 46 1142451 jewel food store # 3345 grocery store jewel food store # 3345 1224 s wabash ave 45 1596210 food 4 less midwest #552 grocery store food 4 less 7030 s ashland ave 44 2083833 mariano's fresh market #8503 grocery store mariano's fresh market 333 e benton pl 41 1302136 mcdonald's restaurant mcdonald's 70 e garfield blvd 40 1476553 pete's produce grocery store pete's produce 1543 e 87 th st 40 1000572 jewel food store #3030 grocery store jewel food store #3030 7530 s stony island ave 39 1094 one stop food & liquor store grocery store one stop food & liquor store 4301-4323 s lake park ave 39 60184 taqueria el ranchito restaurant taqueria el ranchito 2829 n milwaukee ave 39 9154 jimmy g's restaurant jimmy g's 307 s kedzie ave 37 Yay, it looks like these columns enable us to identify a facility! 14 The entities table should store two other types of attributes. The first type describe the entity no matter the time. If the entity were a person, date of birth would be an example but age would not because the latter changes but the former does not. We'll include zip_code and location as two facility attributes. The second type describes when the entity is available for action (e.g. inspection). In this case, the columns start_time, end_time describe the interval in which the facility is in business or active . These columns are important because we don't want to make predictions for inactive entities. The data don't contain active/inactive date columns, so we will use the date of the facility's first inspection as start_time , and either NULL or the date of inspection if the result was out of business or business not located as end_time . create schema if not exists semantic ; drop table if exists semantic . entities cascade ; create table semantic . entities as ( with entities as ( select distinct on ( license_num , facility , facility_aka , facility_type , address ) license_num , facility , facility_aka , facility_type , address , zip_code , location , min ( date ) over ( partition by license_num , facility , facility_aka , facility_type , address ) as start_time , max ( case when result in ( 'out of business' , 'business not located' ) then date else NULL end ) over ( partition by license_num , facility , facility_aka , address ) as end_time from cleaned . inspections order by license_num asc , facility asc , facility_aka asc , facility_type asc , address asc , date asc -- IMPORTANT!! ) select row_number () over ( order by start_time asc , license_num asc , facility asc , facility_aka asc , facility_type asc , address asc ) as entity_id , license_num , facility , facility_aka , facility_type , address , zip_code , location , start_time , end_time , daterange ( start_time , end_time ) as activity_period from entities ); Note that we added a unique identifier ( entity_id ) to this table. This identifier was assigned using a PostgreSQL idiom: distinct on() . DISTINCT ON keeps the \"first\" row of each group. If you are interested in this powerful technique see this blogpost . select to_char ( count ( entity_id ), '999,999' ) as entities from semantic . entities ; entities 35,668 We will add some indexes to this table 15 : create index entities_ix on semantic . entities ( entity_id ); create index entities_license_num_ix on semantic . entities ( license_num ); create index entities_facility_ix on semantic . entities ( facility ); create index entities_facility_type_ix on semantic . entities ( facility_type ); create index entities_zip_code_ix on semantic . entities ( zip_code ); -- Spatial index create index entities_location_gix on semantic . entities using gist ( location ); create index entities_full_key_ix on semantic . entities ( license_num , facility , facility_aka , facility_type , address );","title":"Entities table"},{"location":"dirtyduck/data_preparation/#events-table","text":"We are ready to create the events table. This table will describe the inspection, like the type of inspection, when and where the inspection happened, and the inspection result . We will add the violations as a JSONB column 16 . Finally, we'll rename inspection as event_id 17 . drop table if exists semantic . events cascade ; create table semantic . events as ( with entities as ( select * from semantic . entities ), inspections as ( select i . inspection , i . type , i . date , i . risk , i . result , i . license_num , i . facility , i . facility_aka , i . facility_type , i . address , i . zip_code , i . location , jsonb_agg ( jsonb_build_object ( 'code' , v . code , 'severity' , v . severity , 'description' , v . description , 'comment' , v . comment ) order by code ) as violations from cleaned . inspections as i inner join cleaned . violations as v on i . inspection = v . inspection group by i . inspection , i . type , i . license_num , i . facility , i . facility_aka , i . facility_type , i . address , i . zip_code , i . location , i . date , i . risk , i . result ) select i . inspection as event_id , e . entity_id , i . type , i . date , i . risk , i . result , e . facility_type , e . zip_code , e . location , i . violations from entities as e inner join inspections as i using ( license_num , facility , facility_aka , facility_type , address , zip_code ) ); -- Add some indices create index events_entity_ix on semantic . events ( entity_id asc nulls last ); create index events_event_ix on semantic . events ( event_id asc nulls last ); create index events_type_ix on semantic . events ( type ); create index events_date_ix on semantic . events ( date asc nulls last ); create index events_facility_type_ix on semantic . events ( facility_type ); create index events_zip_code_ix on semantic . events ( zip_code ); -- Spatial index create index events_location_gix on semantic . events using gist ( location ); -- JSONB indices create index events_violations on semantic . events using gin ( violations ); create index events_violations_json_path on semantic . events using gin ( violations jsonb_path_ops ); create index events_event_entity_zip_code_date on semantic . events ( event_id asc nulls last , entity_id asc nulls last , zip_code , date desc nulls last ); Success! We have one row per event 18 Our semantic data looks like: select event_id , entity_id , type , date , risk , result , facility_type , zip_code from semantic . events limit 1 ; event_id entity_id type date risk result facility_type zip_code 1343315 22054 canvass 2013-06-06 low fail newsstand 60623 We omitted violations and location for brevity. The total number of inspections is select to_char ( count ( event_id ), '999,999,999' ) as events from semantic . events ; events 148,724 Now that we have our data in a good shape, we are ready to use Triage .","title":"Events table"},{"location":"dirtyduck/data_preparation/#lets-eda","text":"It is always a good idea to do some Exploratory Data Analysis 19 or EDA for short. This will help us to learn more about the dynamics of the entities or the inspections. We will generate a few plots, just to know: how many entities/events are every month? how many entities/events ended in a failed state every month? and, how many entities/events have in a critical violation in a failed inspection?","title":"Let's EDA &#x2026;"},{"location":"dirtyduck/data_preparation/#inspections-over-time","text":"First, we will try the answer the question: how many inspections are realized every month?","title":"Inspections over time"},{"location":"dirtyduck/data_preparation/#number-of-facilities-inspected-over-time","text":"The previous plot was about the number of events every month, now we will plot how many entities are acted every month. One question, that is useful to answer is: Are there facilities that are inspected more than once in a month? Note We are doing an emphasis in inspected since our data set doesn't contain all the facilities in Chicago. This will have an effect on the modeling stage.","title":"Number of facilities inspected over time"},{"location":"dirtyduck/data_preparation/#number-of-failed-inspections-over-time","text":"What is the proportion of inspections every month that actually end in a failed inspection?","title":"Number of failed inspections over time"},{"location":"dirtyduck/data_preparation/#number-of-facilities-with-failed-inspections-over-time","text":"Now let's see the behavior of the outcomes of the inspection across time. First just if the inspection failed.","title":"Number of facilities with failed inspections over time"},{"location":"dirtyduck/data_preparation/#number-of-severe-violations-found-in-a-failed-inspection-over-time","text":"Finally let's analyze the evolution of failed inspections with severe violations (violation code in 15-29)","title":"Number of severe violations found in a failed inspection over time"},{"location":"dirtyduck/data_preparation/#number-of-facilities-with-severe-violations-found-in-a-failed-inspection-over-time","text":"This few plots give us a sense of how the data behaves and will help us in detect weird bugs or model-behavior later.","title":"Number of facilities with severe violations found in a failed inspection over time"},{"location":"dirtyduck/data_preparation/#whats-next","text":"Learn more about triage Learn more about early warning systems Learn more about resource prioritization systems If you want to try different columns (or you don't remember which columns try \\d raw.inspectios first \u21a9 We are following the event's definition from physics : \"an event is the instantaneous physical situation or occurrence associated with a point in spacetime\" \u21a9 It will make your life easier and most of the Machine Learning algorithms only accept data in matrix form (i.e. one big table) \u21a9 Verbatim from the datasource documentation. \u21a9 A controversial decision, I know. \u21a9 This problem is related to the process of deduplication and there is another DSaPP tool for that: matching-tool . \u21a9 It is the Chicago Food Inspections dataset, after all. \u21a9 We could also use the default geometric data type from postgresql: point ( https://www.postgresql.org/docs/10/datatype-geometric.html ) \u21a9 We will store the Point as a geography object. As a result, spatial database operations (like calculating the distances between two facilities) will return answers in meters instead of degrees. See this . \u21a9 As a real geographical object check the PostGIS documentation \u21a9 Remember our tip at the beginning of this section! \u21a9 If the code looks funny to you, it is because we are using grouping sets , in particular rollup . See the docs. \u21a9 Yeah, you wish \u21a9 Almost. At least good for this tutorial. Look carefully. \u21a9 ALWAYS add indexes to your tables! \u21a9 If you want to have a deep explanation about why is this good check this blog post \u21a9 As a general rule I hate to add the suffix _id , I would rather prefer to name them as event and entity instead of event_id and entity_id . But triage named those columns in that way and for that we are stuck with that nomenclature. \u21a9 This will simplify the creation of features for our machine learning models. \u21a9 Defined by John Tukey as: Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. \u21a9","title":"What\u2019s next?"},{"location":"dirtyduck/dirty_duckling/","text":"Dirty duckling: the quick start guide # This quickstart guide follows the workflow explained here . The goal is to show you an instance of that workflow using the Chicago Food Inspections dataset data source. We packed this a sample of Chicago Food Inspections data source as part of the dirty duck tutorial. Just run in the folder that contains the triage local repository: ./tutorial.sh up from you command line. This will start the database. 1. Install Triage: Check! # We also containerized triage , so, in this tutorial it is already installed for you! Just run ./tutorial.sh bastion The prompt in your command line should change to something like [dirtyduck@bastion$:/triage] Type triage , if no error. You completed this step! Now you have triage installed, with all its power at the point of your fingers. 2. Structure your data: Events (and entities) # As mentioned in the quickstart workflow , at least you need one table that contains events , i.e. something that happened to your entities of interest somewhere at sometime. So you need at least three columns in your data: entity_id , event_id , date (and location if you have it would be a nice addition). In dirtyduck, we provide you with two tables: semantic.entities and semantic.events . The latter is the required minimum. We added the semantic.entities table as a good practice. This is the simplest way to structure your data: as a series of events connected to your entity of interest (people, organization, business, etc.) that take place at a certain time. Each row of the data will be an event. For this quickstart tutorial, you don't need to interact manually with the database, but, if you are curious you can peek inside it, and verify how the events table look like. Inside bastion you can connect to the database typing psql $DATABASE_URL This will change the prompt one more time to food=# Now, type (or copy-paste) the following select event_id entity_id , date , zip_code , type from semantic . events where random () < 0 . 001 limit 5 ; entity_id date zip_code type 1092838 2014-02-27 60657 license 1325036 2014-05-19 60612 canvass 1385431 2014-06-25 60651 complaint 1395315 2014-01-08 60707 canvass 1395916 2014-02-03 60641 canvass Each row in this table is an event with event_id and entity_id (which links to the entity it happened to), a date (when it happened), as well a location (the zip_code column). The event will have attributes that describe it in its particularity, in this case we are just showing one of those attributes: the type of the inspection ( type ) And, if you also want to see the entities in your data select entity_id , license_num , facility , facility_type , activity_period from semantic . entities where random () < 0 . 001 limit 5 ; entity_id license_num facility facility_type activity_period 2218 1223576 loretto hospital hospital [2014-02-27,) 2353 1804587 subway restaurant [2014-03-05,) 636 2002788 duck walk restaurant [2014-01-17,2016-02-29) 3748 1904141 zaragoza restaurant restaurant [2014-04-03,) 5118 2224978 saint cajetan school [2014-05-06,) Triage needs a field named entity_id (that needs to be of type integer) to refer to the primary entities of interest in our project. When you're done exploring the database, you can exit the postgres command line interface by typing \\q 3. Set up Dirty duck's triage configuration file # The configuration file sets up the modeling process to mirror the operational scenario the models will be used in. This involves defining the cohort to train/predict on, the outcome we're predicting, how far in the future we're predicting, how often will the model be updated, how often will the predicted list be used for interventions, what are the resources available to intervene to define the evaluation metric, etc. Here's the sample configuration file called dirty-duckling.yaml config_version : 'v7' model_comment : 'dirtyduck-quickstart' temporal_config : label_timespans : [ '3months' ] label_config : query : | select entity_id, bool_or(result = 'fail')::integer as outcome from semantic.events where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id name : 'failed_inspections' feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates_imputation : count : type : 'zero_noflag' aggregates : - quantity : total : \"*\" metrics : - 'count' intervals : [ 'all' ] groups : - 'entity_id' model_grid_preset : 'quickstart' scoring : testing_metric_groups : - metrics : [ precision@ ] thresholds : percentiles : [ 10 ] training_metric_groups : - metrics : [ precision@ ] thresholds : percentiles : [ 10 ] This is the minimum configuration file, and it still has a lot of sections (ML is a complex business!). Warning If you use the minimum configuration file several parameters will fill up using defaults. Most of the time those defaults are not the values that your modeling of the problem needs! Please check here to see which values are being used and act accordingly. triage uses/needs a data connection in order to work. The connection will be created using the database credentials information (name of the database, server, username, and password). You could use a database configuration file here's an example database configuation file or you can setup an environment variable named $DATABASE_URL , this is the approach taken in the dirtyduck tutorial, its value inside bastion is postgresql://food_user:some_password@food_db/food For the quick explanation of the sections check the quickstart workflow guide . For a detailed explanation about each section of the configuration file look here 4. Run triage # Now we are ready for run something! First we will validate the configuration files by running: triage experiment experiments/dirty-duckling.yaml --validate-only If everything was OK (it should!), you can run the experiment with: triage experiment experiments/dirty-duckling.yaml That's it! If you see this message in your screen: INFO:root:Experiment complete INFO:root:All models that were supposed to be trained were trained. Awesome! INFO:root:All matrices that were supposed to be build were built. Awesome! it would mean that triage actually built (in this order) cohort (table cohort_all_entities... ), labels (table labels_failed_inspections... ), features (schema features ), matrices (table model_metdata.matrices and folder matrices ), models (tables model_metadata.models and model_metadata.model_groups ; folder trained_models ), predictions (table test_results.predictions ) and evaluations (table test_results.evaluations ). 5. Look at results of your duckling! # Next, let's quickly check the tables in the schemas model_metadata and test_results to make sure everything worked. There you will find a lot of information related to the performance of your models. Still connected to the bastion docker container, you can connect to the database by typing: psql $DATABASE_URL Again, you should see the postgreSQL prompt: food=# Tables in the model_metadata schema have some general information about experiments that you've run and the models they created. The quickstart model grid preset should have built 3 models. Let's check with: select model_id , model_group_id , model_type from model_metadata . models ; This should give you a result that looks something like: model_id model_group_id model_type 1 1 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression 2 2 sklearn.tree.DecisionTreeClassifier 3 3 sklearn.dummy.DummyClassifier If you want to see predictions for individual entities, you can check out test_results.predictions , for instance: select model_id , entity_id , as_of_date , score , label_value from test_results . predictions where entity_id = 15596 order by model_id ; This will give you something like: model_id entity_id as_of_date score label_value 1 15596 2017-09-29 00:00:00 0.21884 0 2 15596 2017-09-29 00:00:00 0.22831 0 3 15596 2017-09-29 00:00:00 0.25195 0 Finally, test_results.evaluations holds some aggregate information on model performance. In our config above, we only focused on precision in the top ten percent, so let's see how the models are doing based on this: select model_id , metric , parameter , stochastic_value from test_results . evaluations where metric = 'precision@' and parameter = '10_pct' order by model_id ; model_id metric parameter stochastic_value 1 precision@ 10_pct 0.2865853658536585 2 precision@ 10_pct 0.0 3 precision@ 10_pct 0.0 Not great! But then again, these were just a couple of overly simple model specifications to get things up and running... Feel free to explore some of the other tables in these schemas (note that there's also a train_results schema with performance on the training set as well as feature importances, where defined). When you're done exploring the database, you can exit the postgres command line interface by typing \\q With a real modeling run you could ( should ) do model selection, postmodeling, bias audit, etc. triage provides tools for doing all of that, but we the purpose of this little experiment was just to get things up and running. If you have successfully arrived to this point, you are all set to do your own modeling ( here's a good place to start ), but if you want to go deeper in this example and learn about these other triage functions, continue reading our in-depth tutorial . 6. What's next? # Take a deeper look at triage through this example # Get started with your own project and data #","title":"Dirty duckling"},{"location":"dirtyduck/dirty_duckling/#dirty-duckling-the-quick-start-guide","text":"This quickstart guide follows the workflow explained here . The goal is to show you an instance of that workflow using the Chicago Food Inspections dataset data source. We packed this a sample of Chicago Food Inspections data source as part of the dirty duck tutorial. Just run in the folder that contains the triage local repository: ./tutorial.sh up from you command line. This will start the database.","title":"Dirty duckling: the quick start guide"},{"location":"dirtyduck/dirty_duckling/#1-install-triage-check","text":"We also containerized triage , so, in this tutorial it is already installed for you! Just run ./tutorial.sh bastion The prompt in your command line should change to something like [dirtyduck@bastion$:/triage] Type triage , if no error. You completed this step! Now you have triage installed, with all its power at the point of your fingers.","title":"1. Install Triage: Check!"},{"location":"dirtyduck/dirty_duckling/#2-structure-your-data-events-and-entities","text":"As mentioned in the quickstart workflow , at least you need one table that contains events , i.e. something that happened to your entities of interest somewhere at sometime. So you need at least three columns in your data: entity_id , event_id , date (and location if you have it would be a nice addition). In dirtyduck, we provide you with two tables: semantic.entities and semantic.events . The latter is the required minimum. We added the semantic.entities table as a good practice. This is the simplest way to structure your data: as a series of events connected to your entity of interest (people, organization, business, etc.) that take place at a certain time. Each row of the data will be an event. For this quickstart tutorial, you don't need to interact manually with the database, but, if you are curious you can peek inside it, and verify how the events table look like. Inside bastion you can connect to the database typing psql $DATABASE_URL This will change the prompt one more time to food=# Now, type (or copy-paste) the following select event_id entity_id , date , zip_code , type from semantic . events where random () < 0 . 001 limit 5 ; entity_id date zip_code type 1092838 2014-02-27 60657 license 1325036 2014-05-19 60612 canvass 1385431 2014-06-25 60651 complaint 1395315 2014-01-08 60707 canvass 1395916 2014-02-03 60641 canvass Each row in this table is an event with event_id and entity_id (which links to the entity it happened to), a date (when it happened), as well a location (the zip_code column). The event will have attributes that describe it in its particularity, in this case we are just showing one of those attributes: the type of the inspection ( type ) And, if you also want to see the entities in your data select entity_id , license_num , facility , facility_type , activity_period from semantic . entities where random () < 0 . 001 limit 5 ; entity_id license_num facility facility_type activity_period 2218 1223576 loretto hospital hospital [2014-02-27,) 2353 1804587 subway restaurant [2014-03-05,) 636 2002788 duck walk restaurant [2014-01-17,2016-02-29) 3748 1904141 zaragoza restaurant restaurant [2014-04-03,) 5118 2224978 saint cajetan school [2014-05-06,) Triage needs a field named entity_id (that needs to be of type integer) to refer to the primary entities of interest in our project. When you're done exploring the database, you can exit the postgres command line interface by typing \\q","title":"2. Structure your data: Events (and entities)"},{"location":"dirtyduck/dirty_duckling/#3-set-up-dirty-ducks-triage-configuration-file","text":"The configuration file sets up the modeling process to mirror the operational scenario the models will be used in. This involves defining the cohort to train/predict on, the outcome we're predicting, how far in the future we're predicting, how often will the model be updated, how often will the predicted list be used for interventions, what are the resources available to intervene to define the evaluation metric, etc. Here's the sample configuration file called dirty-duckling.yaml config_version : 'v7' model_comment : 'dirtyduck-quickstart' temporal_config : label_timespans : [ '3months' ] label_config : query : | select entity_id, bool_or(result = 'fail')::integer as outcome from semantic.events where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id name : 'failed_inspections' feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates_imputation : count : type : 'zero_noflag' aggregates : - quantity : total : \"*\" metrics : - 'count' intervals : [ 'all' ] groups : - 'entity_id' model_grid_preset : 'quickstart' scoring : testing_metric_groups : - metrics : [ precision@ ] thresholds : percentiles : [ 10 ] training_metric_groups : - metrics : [ precision@ ] thresholds : percentiles : [ 10 ] This is the minimum configuration file, and it still has a lot of sections (ML is a complex business!). Warning If you use the minimum configuration file several parameters will fill up using defaults. Most of the time those defaults are not the values that your modeling of the problem needs! Please check here to see which values are being used and act accordingly. triage uses/needs a data connection in order to work. The connection will be created using the database credentials information (name of the database, server, username, and password). You could use a database configuration file here's an example database configuation file or you can setup an environment variable named $DATABASE_URL , this is the approach taken in the dirtyduck tutorial, its value inside bastion is postgresql://food_user:some_password@food_db/food For the quick explanation of the sections check the quickstart workflow guide . For a detailed explanation about each section of the configuration file look here","title":"3. Set up Dirty duck's triage configuration file"},{"location":"dirtyduck/dirty_duckling/#4-run-triage","text":"Now we are ready for run something! First we will validate the configuration files by running: triage experiment experiments/dirty-duckling.yaml --validate-only If everything was OK (it should!), you can run the experiment with: triage experiment experiments/dirty-duckling.yaml That's it! If you see this message in your screen: INFO:root:Experiment complete INFO:root:All models that were supposed to be trained were trained. Awesome! INFO:root:All matrices that were supposed to be build were built. Awesome! it would mean that triage actually built (in this order) cohort (table cohort_all_entities... ), labels (table labels_failed_inspections... ), features (schema features ), matrices (table model_metdata.matrices and folder matrices ), models (tables model_metadata.models and model_metadata.model_groups ; folder trained_models ), predictions (table test_results.predictions ) and evaluations (table test_results.evaluations ).","title":"4. Run triage"},{"location":"dirtyduck/dirty_duckling/#5-look-at-results-of-your-duckling","text":"Next, let's quickly check the tables in the schemas model_metadata and test_results to make sure everything worked. There you will find a lot of information related to the performance of your models. Still connected to the bastion docker container, you can connect to the database by typing: psql $DATABASE_URL Again, you should see the postgreSQL prompt: food=# Tables in the model_metadata schema have some general information about experiments that you've run and the models they created. The quickstart model grid preset should have built 3 models. Let's check with: select model_id , model_group_id , model_type from model_metadata . models ; This should give you a result that looks something like: model_id model_group_id model_type 1 1 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression 2 2 sklearn.tree.DecisionTreeClassifier 3 3 sklearn.dummy.DummyClassifier If you want to see predictions for individual entities, you can check out test_results.predictions , for instance: select model_id , entity_id , as_of_date , score , label_value from test_results . predictions where entity_id = 15596 order by model_id ; This will give you something like: model_id entity_id as_of_date score label_value 1 15596 2017-09-29 00:00:00 0.21884 0 2 15596 2017-09-29 00:00:00 0.22831 0 3 15596 2017-09-29 00:00:00 0.25195 0 Finally, test_results.evaluations holds some aggregate information on model performance. In our config above, we only focused on precision in the top ten percent, so let's see how the models are doing based on this: select model_id , metric , parameter , stochastic_value from test_results . evaluations where metric = 'precision@' and parameter = '10_pct' order by model_id ; model_id metric parameter stochastic_value 1 precision@ 10_pct 0.2865853658536585 2 precision@ 10_pct 0.0 3 precision@ 10_pct 0.0 Not great! But then again, these were just a couple of overly simple model specifications to get things up and running... Feel free to explore some of the other tables in these schemas (note that there's also a train_results schema with performance on the training set as well as feature importances, where defined). When you're done exploring the database, you can exit the postgres command line interface by typing \\q With a real modeling run you could ( should ) do model selection, postmodeling, bias audit, etc. triage provides tools for doing all of that, but we the purpose of this little experiment was just to get things up and running. If you have successfully arrived to this point, you are all set to do your own modeling ( here's a good place to start ), but if you want to go deeper in this example and learn about these other triage functions, continue reading our in-depth tutorial .","title":"5. Look at results of your duckling!"},{"location":"dirtyduck/dirty_duckling/#6-whats-next","text":"","title":"6. What's next?"},{"location":"dirtyduck/dirty_duckling/#take-a-deeper-look-at-triage-through-this-example","text":"","title":"Take a deeper look at triage through this example"},{"location":"dirtyduck/dirty_duckling/#get-started-with-your-own-project-and-data","text":"","title":"Get started with your own project and data"},{"location":"dirtyduck/eis/","text":"An Early Intervention System: Chicago food inspections # Before continue, Did you\u2026? This case study, part of the dirtyduck tutorial, assumes that you already setup the tutorial\u2019s infrastructure and load the dataset. If you didn\u2019t setup the infrastructure go here , If you didn't load the data, you can do it very quickly or you can follow all the steps and explanations about the data . Problem description # Triage is designed to build, among other things, early warning systems (also called early intervention, EIS). While there are several differences between modeling early warnings and inspection prioritization, perhaps the biggest differences is that the entity is active (i.e. it is doing stuff for which an outcome will happen) in EIS, but passive (e.g it is inspected) in resource prioritization . Among other things, this difference affects the way the outcome is built. Saying that, here's the question we want to answer: Will my restaurant be inspected in the next Y Y period of time? Where X X could be 3 days, 2 months, 1 year, etc. We will translate that problem to Will my restaurant be at the top- X X facilities most likely to be inspected in the next Y Y period of time? Knowing the answer to this question enables you (as the restaurant owner or manager) to prepare for the inspection. What are the outcomes? # The trick to note is that on any given day there are two possible outcomes: the facility was inspected and the facility wasn't inspected . Our outcomes table will be larger than in the resource prioritization example because we need an outcome for every active facility on every date. The following image tries to exemplify this reasoning: Figure. The image shows three facilities (blue, red and orange), and next to each, a temporal line with 6 days (0-5). Each dot represents the event (whether an inspection happened). Yellow means the inspection happened ( TRUE outcome) and blue means it didn't ( FALSE outcome). Each facility in the image had two inspections, six in total. Fortunately, triage will help us to create this table. What are the entities of interest? The cohort # We are interested in predict only in active facilities (remember, in this case study, you own a restaurant, What is the point on predict if your restaurant is already closed for good?). This is the same cohort as the cohort table in the resource prioritization case study Experiment description file You could check the meaning about experiment description files (or configuration files) in A deeper look into triage . First the usual stuff. Note that we are changing model_comment and label_definition (remember that this is used for generating the hash that differentiates models and model groups). config_version : 'v7' model_comment : 'eis: 01' random_seed : 23895478 user_metadata : label_definition : 'inspected' experiment_type : 'eis' description : | EIS 01 purpose : 'model creation' org : 'DSaPP' team : 'Tutorial' author : 'Your name here' etl_date : '2019-05-07' model_group_keys : - 'class_path' - 'parameters' - 'feature_names' - 'feature_groups' - 'cohort_name' - 'state' - 'label_name' - 'label_timespan' - 'training_as_of_date_frequency' - 'max_training_history' - 'label_definition' - 'experiment_type' - 'org' - 'team' - 'author' - 'etl_date' For the labels the query is pretty simple, if the facility showed in the data, it will get a positive outcome, if not they will get a negative outcome label_config : query : | select entity_id, True::integer as outcome from semantic.events where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id include_missing_labels_in_train_as : False name : 'inspected' Note the two introduced changes in this block, first, the outcome is True , because all our observations represent inspected facilities (see discussion above and in particular previous image), second, we added the line include_missing_labels_in_train_as: False . This line tells triage to incorporate all the missing facilities in the training matrices with False as the label . As stated we will use the same configuration block for cohorts that we used in inspections: cohort_config : query : | select e.entity_id from semantic.entities as e where daterange(start_time, end_time, '[]') @> '{as_of_date}'::date name : 'active_facilities' Modeling Using Machine Learning # We need to specify the temporal configuration, this section should reflect the operationalization of the model. Let\u2019s assume that every facility owner needs 6 months to prepare for an inspection. So, the model needs to answer the question: Will my restaurant be inspected in the next 6 months? Temporal configuration # temporal_config : feature_start_time : '2010-01-04' feature_end_time : '2018-06-01' label_start_time : '2014-06-01' label_end_time : '2018-06-01' model_update_frequency : '6month' training_label_timespans : [ '6month' ] training_as_of_date_frequencies : '6month' test_durations : '6month' test_label_timespans : [ '6month' ] test_as_of_date_frequencies : '6month' max_training_histories : '5y' As before, you can generate the image of the temporal blocks: # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/eis_01.yaml --show-timechop What? \u2026 Bastion? bastion is the docker container that contains all the setup required to run this tutorial, if this is the first time that you see this word, you should stop and revisit setup infrastructure . Figure. Temporal blocks for the Early Warning System. We want to predict the most likely facilities to be inspected in the following 6 months Features # Regarding the features, we will use the same ones that were used in inspections prioritization : feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates_imputation : count : type : 'zero_noflag' aggregates : - quantity : total : \"*\" metrics : - 'count' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'risks' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero' avg : type : 'zero' categoricals : - column : 'risk' choices : [ 'low' , 'medium' , 'high' ] metrics : - 'sum' - 'avg' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - 'zip_code' - prefix : 'results' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : all : type : 'zero' categoricals : - column : 'result' choice_query : 'select distinct result from semantic.events' metrics : - 'sum' - 'avg' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'inspection_types' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero_noflag' categoricals : - column : 'type' choice_query : 'select distinct type from semantic.events where type is not null' metrics : - 'sum' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - 'zip_code' We specify that we want to use all possible feature-group combinations for training: feature_group_definition : prefix : - 'inspections' - 'results' - 'risks' - 'inspection_types' feature_group_strategies : [ 'all' ] i.e. all will train models with all the features groups, leave-one-in will use only one of the feature groups for traning, and lastly, leave-one-out will train the model with all the features except one. Algorithm and hyperparameters # We will begin defining some basic models as baselines. 'triage.component.catwalk.baselines.thresholders.SimpleThresholder' : rules : - [ 'inspections_entity_id_1month_total_count > 0' ] - [ 'results_entity_id_1month_result_fail_sum > 0' ] - [ 'risks_entity_id_1month_risk_high_sum > 0' ] 'triage.component.catwalk.baselines.rankers.PercentileRankOneFeature' : feature : [ 'risks_entity_id_all_risk_high_sum' , 'inspections_entity_id_all_total_count' , 'results_entity_id_all_result_fail_sum' ] descend : [ True ] 'sklearn.dummy.DummyClassifier' : strategy : [ 'prior' , 'stratified' ] 'sklearn.tree.DecisionTreeClassifier' : criterion : [ 'gini' ] max_features : [ 'sqrt' ] max_depth : [ 1 , 2 , 5 , ~ ] min_samples_split : [ 2 ] 'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression' : penalty : [ 'l1' , 'l2' ] C : [ 0.000001 , 0.0001 , 0.01 , 1.0 ] How did I know the name of the features? triage has a very useful utility called featuretest triage featuretest experiments/eis_01.yaml 2018 -01-01 You can use for testing the definition of your features and also to see if the way that the features are calculated is actually what do you expect. Here we are using it just to check the name of the generated features. triage will create 20 model groups : algorithms and hyperparameters (4 DecisionTreeClassifier , 8 ScaledLogisticRegression , 2 DummyClassifier , 3 SimpleThresholder and 3 PercentileRankOneFeature ) \u00d7 1 features sets (1 all ). The total number of models is three times that (we have 6 time blocks, so 120 models). scoring : testing_metric_groups : - metrics : [ precision@ , recall@ ] thresholds : percentiles : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 95 , 100 ] top_n : [ 1 , 5 , 10 , 25 , 50 , 100 , 250 , 500 , 1000 ] training_metric_groups : - metrics : [ accuracy ] - metrics : [ precision@ , recall@ ] thresholds : percentiles : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 95 , 100 ] top_n : [ 1 , 5 , 10 , 25 , 50 , 100 , 250 , 500 , 1000 ] As a last step, we validate that the configuration file is correct: # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/eis_01.yaml --validate-only And then just run it: # Remember to run this in bastion NOT in your laptop shell! time triage experiment experiments/eis_01.yaml Protip We are including the command time in order to get the total running time of the experiment. You can remove it, if you like. This will take a lot amount of time (on my computer took 3h 42m), so, grab your coffee, chat with your coworkers, check your email, or read the DSSG blog . It's taking that long for several reasons: There are a lot of models, parameters, etc. We are running in serial mode (i.e. not in parallel). The database is running on your laptop. You can solve 2 and 3. For the second point you could use the docker container that has the multicore option enabled. For 3, I recommed you to use a PostgreSQL database in the cloud, such as Amazon's PostgreSQL RDS (we will explore this later in running triage in AWS Batch). After the experiment finishes, we can create the following table: with features_groups as ( select model_group_id , split_part ( unnest ( feature_list ), '_' , 1 ) as feature_groups from model_metadata . model_groups ), features_arrays as ( select model_group_id , array_agg ( distinct feature_groups ) as feature_groups from features_groups group by model_group_id ) select model_group_id , regexp_replace ( model_type , '^.*\\.' , '' ) as model_type , hyperparameters , feature_groups , array_agg ( model_id order by train_end_time asc ) as models , array_agg ( train_end_time :: date order by train_end_time asc ) as times , array_agg ( to_char ( stochastic_value , '0.999' ) order by train_end_time asc ) as \"precision@10% (stochastic)\" from model_metadata . models join features_arrays using ( model_group_id ) join test_results . evaluations using ( model_id ) where model_comment ~ 'eis' and metric || parameter = 'precision@10_pct' group by model_group_id , model_type , hyperparameters , feature_groups order by model_group_id ; model_group_id model_type hyperparameters feature_groups models times precision@10% (stochastic) 1 SimpleThresholder {\"rules\": [\"inspections_entity_id_1month_total_count > 0\"]} {inspection,inspections,results,risks} {1,19,37,55,73,91} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.358\",\" 0.231\",\" 0.321\",\" 0.267\",\" 0.355\",\" 0.239\"} 2 SimpleThresholder {\"rules\": [\"results_entity_id_1month_result_fail_sum > 0\"]} {inspection,inspections,results,risks} {2,20,38,56,74,92} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.316\",\" 0.316\",\" 0.323\",\" 0.344\",\" 0.330\",\" 0.312\"} 3 SimpleThresholder {\"rules\": [\"risks_entity_id_1month_risk_high_sum > 0\"]} {inspection,inspections,results,risks} {3,21,39,57,75,93} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.364\",\" 0.248\",\" 0.355\",\" 0.286\",\" 0.371\",\" 0.257\"} 4 PercentileRankOneFeature {\"descend\": true, \"feature\": \"risks_entity_id_all_risk_high_sum\"} {inspection,inspections,results,risks} {4,22,40,58,76,94} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.121\",\" 0.193\",\" 0.124\",\" 0.230\",\" 0.112\",\" 0.161\"} 5 PercentileRankOneFeature {\"descend\": true, \"feature\": \"inspections_entity_id_all_total_count\"} {inspection,inspections,results,risks} {5,23,41,59,77,95} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.076\",\" 0.133\",\" 0.098\",\" 0.101\",\" 0.086\",\" 0.082\"} 6 PercentileRankOneFeature {\"descend\": true, \"feature\": \"results_entity_id_all_result_fail_sum\"} {inspection,inspections,results,risks} {6,24,42,60,78,96} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.237\",\" 0.274\",\" 0.250\",\" 0.275\",\" 0.225\",\" 0.221\"} 7 DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 1, \"max_features\": \"sqrt\", \"min_samples_split\": 2} {inspection,inspections,results,risks} {7,25,43,61,79,97} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.284\",\" 0.441\",\" 0.559\",\" 0.479\",\" 0.463\",\" 0.412\"} 8 DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"min_samples_split\": 2} {inspection,inspections,results,risks} {8,26,44,62,80,98} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.401\",\" 0.388\",\" 0.533\",\" 0.594\",\" 0.519\",\" 0.649\"} 9 DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 5, \"max_features\": \"sqrt\", \"min_samples_split\": 2} {inspection,inspections,results,risks} {9,27,45,63,81,99} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.594\",\" 0.876\",\" 0.764\",\" 0.843\",\" 0.669\",\" 0.890\"} 10 DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"sqrt\", \"min_samples_split\": 2} {inspection,inspections,results,risks} {10,28,46,64,82,100} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.484\",\" 0.542\",\" 0.566\",\" 0.589\",\" 0.565\",\" 0.546\"} 11 ScaledLogisticRegression {\"C\": 0.000001, \"penalty\": \"l1\"} {inspection,inspections,results,risks} {11,29,47,65,83,101} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.272\",\" 0.318\",\" 0.306\",\" 0.328\",\" 0.292\",\" 0.281\"} 12 ScaledLogisticRegression {\"C\": 0.000001, \"penalty\": \"l2\"} {inspection,inspections,results,risks} {12,30,48,66,84,102} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.382\",\" 0.187\",\" 0.375\",\" 0.261\",\" 0.419\",\" 0.233\"} 13 ScaledLogisticRegression {\"C\": 0.0001, \"penalty\": \"l1\"} {inspection,inspections,results,risks} {13,31,49,67,85,103} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.275\",\" 0.314\",\" 0.306\",\" 0.329\",\" 0.462\",\" 0.421\"} 14 ScaledLogisticRegression {\"C\": 0.0001, \"penalty\": \"l2\"} {inspection,inspections,results,risks} {14,32,50,68,86,104} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.562\",\" 0.454\",\" 0.765\",\" 0.821\",\" 0.758\",\" 0.828\"} 15 ScaledLogisticRegression {\"C\": 0.01, \"penalty\": \"l1\"} {inspection,inspections,results,risks} {15,33,51,69,87,105} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.745\",\" 0.863\",\" 0.807\",\" 0.867\",\" 0.826\",\" 0.873\"} 16 ScaledLogisticRegression {\"C\": 0.01, \"penalty\": \"l2\"} {inspection,inspections,results,risks} {16,34,52,70,88,106} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.739\",\" 0.863\",\" 0.793\",\" 0.870\",\" 0.822\",\" 0.874\"} 17 ScaledLogisticRegression {\"C\": 1.0, \"penalty\": \"l1\"} {inspection,inspections,results,risks} {17,35,53,71,89,107} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.706\",\" 0.769\",\" 0.796\",\" 0.846\",\" 0.822\",\" 0.868\"} 18 ScaledLogisticRegression {\"C\": 1.0, \"penalty\": \"l2\"} {inspection,inspections,results,risks} {18,36,54,72,90,108} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.694\",\" 0.779\",\" 0.793\",\" 0.845\",\" 0.823\",\" 0.867\"} 19 DummyClassifier {\"strategy\": \"prior\"} {inspection,inspections,results,risks} {109,111,113,115,117,119} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.273\",\" 0.316\",\" 0.306\",\" 0.332\",\" 0.295\",\" 0.282\"} 20 DummyClassifier {\"strategy\": \"stratified\"} {inspection,inspections,results,risks} {110,112,114,116,118,120} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.272\",\" 0.314\",\" 0.301\",\" 0.343\",\" 0.292\",\" 0.287\"} Protip You could have a \"real time\" version of the previous query while you are running the experiment config file with triage. Just execute \\watch n in the psql console and it will be refreshed every n n seconds Let\u2019s explore more: second grid # After the baseline we will explore a more robust set of algorithms. We will use a different experiment config file: eis_02.yaml . The only differences between this experiment config file and the previous are in the user_metadata section: config_version : 'v7' model_comment : 'eis: 02' random_seed : 23895478 user_metadata : label_definition : 'inspected' experiment_type : 'eis' description : | EIS 02 purpose : 'model creation' org : 'DSaPP' team : 'Tutorial' author : 'Your name here' etl_date : '2019-05-07' and in the grid_config : grid_config : ## Boosting 'sklearn.ensemble.AdaBoostClassifier' : n_estimators : [ 1000 , 2000 ] 'sklearn.ensemble.GradientBoostingClassifier' : n_estimators : [ 1000 , 2000 ] learning_rate : [ 0.01 , 1.0 ] subsample : [ 0.5 , 1.0 ] min_samples_split : [ 2 ] max_depth : [ 2 , 5 ] ## Forest 'sklearn.tree.DecisionTreeClassifier' : criterion : [ 'gini' ] max_depth : [ 2 , 5 , 10 ] min_samples_split : [ 2 , 10 , 50 ] 'sklearn.ensemble.RandomForestClassifier' : n_estimators : [ 10000 ] criterion : [ 'gini' ] max_depth : [ 2 , 5 , 10 ] max_features : [ 'sqrt' ] min_samples_split : [ 2 , 10 , 50 ] n_jobs : [ -1 ] 'sklearn.ensemble.ExtraTreesClassifier' : n_estimators : [ 10000 ] criterion : [ 'gini' ] max_depth : [ 2 , 5 , 10 ] max_features : [ 'sqrt' ] min_samples_split : [ 2 , 10 , 50 ] n_jobs : [ -1 ] This new experiment configuration file will add 37 models groups to our experiment. You can run this experiment with: # Remember to run this in bastion NOT in your laptop shell! time triage experiment experiments/eis_02.yaml Audition: So many models, how can I choose the best one? # Let\u2019s select the best model groups, using Audition. We need to make small changes to the /triage/audition/eis_audition_config.yaml compared to the inspection\u2019s one: # CHOOSE MODEL GROUPS model_groups : query : | select distinct(model_group_id) from model_metadata.model_groups where model_config ->> 'experiment_type' ~ 'eis' # CHOOSE TIMESTAMPS/TRAIN END TIMES time_stamps : query : | select distinct train_end_time from model_metadata.models where model_group_id in ({}) and extract(day from train_end_time) in (1) and train_end_time >= '2014-01-01' # FILTER filter : metric : 'precision@' # metric of interest parameter : '10_pct' # parameter of interest max_from_best : 1.0 # The maximum value that the given metric can be worse than the best model for a given train end time. threshold_value : 0.0 # The worst absolute value that the given metric should be. distance_table : 'eis_distance_table' # name of the distance table models_table : 'models' # name of the models table # RULES rules : - shared_parameters : - metric : 'precision@' parameter : '10_pct' selection_rules : - name : 'best_current_value' # Pick the model group with the best current metric value n : 5 - name : 'best_average_value' # Pick the model with the highest average metric value n : 5 - name : 'lowest_metric_variance' # Pick the model with the lowest metric variance n : 5 - name : 'most_frequent_best_dist' # Pick the model that is most frequently within `dist_from_best_case` dist_from_best_case : [ 0.05 ] n : 5 And then we run the simulation of the rules againts the experiment as: triage audition -c ./audition/eis_audition_config.yaml --directory audition/eis Audition will create several plots that will help you to sort out which is the best model group to use (like in a production setting or just to generate your list). Filtering model groups # Audition will generate two plots that are meant to be used together: model performance over time and distance from best . Figure. Model group performance over time. In this case the metric show is precision@10% . The black dashed line represents the (theoretical) system's performance if we select the best performant model in a every evaluation date. The colored lines represents different model groups. All the model groups that share an algorithm will be colored the same. Figure. Proportion of all the models in a model group that are separated from the best model. The distance is measured in percentual points, i.e. How much less precision at 10 percent of the population compared to the best model in that date. Selecting the best rule or strategy for choosing model groups # In this phase of the audition, you will see what will happen in the next time if you choose your model group with an specific strategy or rule. You then, can calculate the regret . Regret is defined as the difference between the performance of the best model evaluated on the \"next time\" and the performance of the model selected by a particular rule. Figure. Given a strategy for selecting model groups (in the plot 4 are shown), What will be the performace of the model group chosen by that strategy in the next evaluation date? Figure. Given a strategy for selecting model groups (in the plot 4 are shown). What will be the distance (*regret ) to the best theoretical model in the following evaluation date.* Figure. Expected regret for the strategies. The less the better. It seems that the worst strategy (the one with the bigger \u201cregret\u201d) for selecting a model_group is lowest_metric_variance_precision . The other three seem almost indistinguishable. We will dig in using Postmodeling. And afterwards instead of using the feature importance to characterize the facilities, we will explore how the model is splitting the facilities using crosstabs . As before, the best 3 model groups per strategy will be stored in the file /triage/audition/eis/results_model_group_ids.json { \"most_frequent_best_dist_precision@_10_pct_0.05\" : [ 15 , 16 , 17 , 9 , 18 ], \"lowest_metric_variance_precision@_10_pct\" : [ 2 , 5 , 19 , 11 , 6 ], \"best_average_value_precision@_10_pct\" : [ 15 , 16 , 17 , 18 , 9 ], \"best_current_value_precision@_10_pct\" : [ 9 , 16 , 15 , 17 , 18 ] } Postmodeling: Inspecting the best models closely # Given that almost all the strategies perform well, we will change the parameter model_group_id in the postmodeling's configuration file and we will use the complete set of model groups selected by audition: # Postmodeling Configuration File project_path : '/triage' # Project path defined in triage with matrices and models audition_output_path : '/triage/audition/eis/results_model_group_ids.json' thresholds : # Thresholds for2 defining positive predictions rank_abs : [ 50 , 100 , 250 ] rank_pct : [ 5 , 10 , 25 ] baseline_query : | # SQL query for defining a baseline for comparison in plots. It needs a metric and parameter select g.model_group_id, m.model_id, extract('year' from m.evaluation_end_time) as as_of_date_year, m.metric, m.parameter, m.stochastic_value, m.num_labeled_examples, m.num_labeled_above_threshold, m.num_positive_labels from test_results.evaluations m left join model_metadata.models g using(model_id) where g.model_group_id = 20 and metric = 'precision@' and parameter = '10_pct' max_depth_error_tree : 5 # For error trees, how depth the decision trees should go? n_features_plots : 10 # Number of features for importances figsize : [ 12 , 12 ] # Default size for plots fontsize : 20 # Default fontsize for plots Launch jupyter in bastion : jupyter-notebook \u2013-ip = 0 .0.0.0 --port = 56406 --allow-root And then in your browser location bar type: http://0.0.0.0:56406 Setup # %matplotlib inline import pandas as pd import numpy as np from collections import OrderedDict from triage.component.postmodeling.contrast.utils.aux_funcs import create_pgconn, get_models_ids from triage.component.catwalk.storage import ProjectStorage, ModelStorageEngine, MatrixStorageEngine from triage.component.postmodeling.contrast.parameters import PostmodelParameters from triage.component.postmodeling.contrast.model_evaluator import ModelEvaluator from triage.component.postmodeling.contrast.model_group_evaluator import ModelGroupEvaluator params = PostmodelParameters('../triage/eis_postmodeling_config.yaml') engine = create_pgconn('database.yaml') # Model group object (useful to compare across model_groups and models in time) audited_models_class = ModelGroupEvaluator(tuple(params.model_group_id), engine) Model groups # Let\u2019s start with the behavior in time of the selected model groups audited_models_class.plot_prec_across_time(param_type='rank_pct', param=10, baseline=True, baseline_query=params.baseline_query, metric='precision@', figsize=params.figsize) Every model selected by audition has a very similar performance across time, and they are ~2.5 times above the baseline in precision@10%. We could also check the recall of the model groups. audited_models_class.plot_prec_across_time(param_type='rank_pct', param=10, metric='recall@', figsize=params.figsize) That behavior is similar for the recall@10%, except for the model group 69 audited_models_class.plot_jaccard_preds(param_type='rank_pct', param=10, temporal_comparison=True) There are a high jaccard similarity between some model groups across time. This could be an indicator that they are so similar that you can choose any and it won\u2019t matter. Going deeper with a model # We will choose the model group 64 as the winner. select mg . model_group_id , mg . model_type , mg . hyperparameters , array_agg ( model_id order by train_end_time ) as models from model_metadata . model_groups as mg inner join model_metadata . models using ( model_group_id ) where model_group_id = 76 group by 1 , 2 , 3 model group id model type hyperparameters models 64 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max features \": \"sqrt\", \"n estimators \": 500, \"min samples leaf \": 1, \"min samples split \": 50} {190,208,226} But before going to production and start making predictions in unseen data, let\u2019s see what the particular models are doing. Postmodeling created a ModelEvaluator (similar to the ModelGroupEvaluator ) to do this exploration: models_76 = { f'{model}': ModelEvaluator(76, model, engine) for model in [198,216,234] } In this tutorial, we will just show some parts of the analysis in the most recent model, but feel free of exploring the behavior of all the models in this model group, and check if you can detect any pattern. Feature importances models_76['234'].plot_feature_importances(path=params.project_path, n_features_plots=params.n_features_plots, figsize=params.figsize) models_76['234'].plot_feature_group_average_importances() Crosstabs: How are the entities classified? # Model interpretation is a huge topic nowadays, the most obvious path is using the features importance from the model. This could be useful, but we could do a lot better. Triage uses crosstabs as a different approach that complements the list of features importance . crosstabs will run statistical tests to compare the predicted positive and the predicted false facilities in each feature. output : schema : 'test_results' table : 'eis_crosstabs' thresholds : rank_abs : [ 50 ] rank_pct : [ 5 ] #(optional): a list of entity_ids to subset on the crosstabs analysis entity_id_list : [] models_list_query : \"select unnest(ARRAY[226]) :: int as model_id\" as_of_dates_query : \"select generate_series('2017-12-01'::date, '2018-09-01'::date, interval '1month') as as_of_date\" #don't change this query unless strictly necessary. It is just validating pairs of (model_id,as_of_date) #it is just a join with distinct (model_id, as_of_date) in a predictions table models_dates_join_query : | select model_id, as_of_date from models_list_query as m cross join as_of_dates_query a join (select distinct model_id, as_of_date from test_results.predictions) as p using (model_id, as_of_date) #features_query must join models_dates_join_query with 1 or more features table using as_of_date features_query : | select m.model_id, m.as_of_date, f4.entity_id, f4.results_entity_id_1month_result_fail_avg, f4.results_entity_id_3month_result_fail_avg, f4.results_entity_id_6month_result_fail_avg, f2.inspection_types_zip_code_1month_type_canvass_sum, f3.risks_zip_code_1month_risk_high_sum, f4.results_entity_id_6month_result_pass_avg, f3.risks_entity_id_all_risk_high_sum, f2.inspection_types_zip_code_3month_type_canvass_sum, f4.results_entity_id_6month_result_pass_sum, f2.inspection_types_entity_id_all_type_canvass_sum from features.inspection_types_aggregation_imputed as f2 inner join features.risks_aggregation_imputed as f3 using (entity_id, as_of_date) inner join features.results_aggregation_imputed as f4 using (entity_id, as_of_date) inner join models_dates_join_query as m using (as_of_date) #the predictions query must return model_id, as_of_date, entity_id, score, label_value, rank_abs and rank_pct #it must join models_dates_join_query using both model_id and as_of_date predictions_query : | select model_id, as_of_date, entity_id, score, label_value, coalesce(rank_abs_no_ties, row_number() over (partition by (model_id, as_of_date) order by score desc)) as rank_abs, coalesce(rank_pct_no_ties*100, ntile(100) over (partition by (model_id, as_of_date) order by score desc)) as rank_pct from test_results.predictions join models_dates_join_query using(model_id, as_of_date) where model_id in (select model_id from models_list_query) and as_of_date in (select as_of_date from as_of_dates_query) triage --tb crosstabs /triage/eis_crosstabs_config.yaml When it finishes, you could explore the table with the following code: with significant_features as ( select feature_column , as_of_date , threshold_unit from test_results . eis_crosstabs where metric = 'ttest_p' and value < 0 . 05 and as_of_date = '2018-09-01' ) select distinct model_id , as_of_date :: date as as_of_date , format ( '%s %s' , threshold_value , t1 . threshold_unit ) as threshold , feature_column , value as \"ratio PP / PN\" from test_results . eis_crosstabs as t1 inner join significant_features as t2 using ( feature_column , as_of_date ) where metric = 'ratio_predicted_positive_over_predicted_negative' and t1 . threshold_unit = 'pct' order by value desc model id as of date threshold feature column ratio PP / PN 226 2018-09-01 5 pct results entity id 1month result fail avg 11.7306052855925 226 2018-09-01 5 pct results entity id 3month result fail avg 3.49082798996376 226 2018-09-01 5 pct results entity id 6month result fail avg 1.27344759545161 226 2018-09-01 5 pct risks zip code 1month risk high sum 1.17488357227451 226 2018-09-01 5 pct inspection types entity id all type canvass sum 0.946432281075976 226 2018-09-01 5 pct inspection types zip code 3month type canvass sum 0.888940127100436 226 2018-09-01 5 pct results entity id 6month result pass sum 0.041806916457784 226 2018-09-01 5 pct results entity id 6month result pass avg 0.0232523724927717 This table represents the ratio between the predicted positives at the top 5% and predicted negatives (the rest). For example, you can see that in PP are eleven times more inspected if they have a failed inspection in the last month, 3.5 times more if they have a failed inspection in the previous 3 months, etc. Where to go from here # Ready to get started with your own data? Check out the suggested project workflow for some tips about how to iterate and tune the pipeline for your project. Want to work through another example? Take a look at our resource prioritization case study","title":"Early Warning System"},{"location":"dirtyduck/eis/#an-early-intervention-system-chicago-food-inspections","text":"Before continue, Did you\u2026? This case study, part of the dirtyduck tutorial, assumes that you already setup the tutorial\u2019s infrastructure and load the dataset. If you didn\u2019t setup the infrastructure go here , If you didn't load the data, you can do it very quickly or you can follow all the steps and explanations about the data .","title":"An Early Intervention System: Chicago food inspections"},{"location":"dirtyduck/eis/#problem-description","text":"Triage is designed to build, among other things, early warning systems (also called early intervention, EIS). While there are several differences between modeling early warnings and inspection prioritization, perhaps the biggest differences is that the entity is active (i.e. it is doing stuff for which an outcome will happen) in EIS, but passive (e.g it is inspected) in resource prioritization . Among other things, this difference affects the way the outcome is built. Saying that, here's the question we want to answer: Will my restaurant be inspected in the next Y Y period of time? Where X X could be 3 days, 2 months, 1 year, etc. We will translate that problem to Will my restaurant be at the top- X X facilities most likely to be inspected in the next Y Y period of time? Knowing the answer to this question enables you (as the restaurant owner or manager) to prepare for the inspection.","title":"Problem description"},{"location":"dirtyduck/eis/#what-are-the-outcomes","text":"The trick to note is that on any given day there are two possible outcomes: the facility was inspected and the facility wasn't inspected . Our outcomes table will be larger than in the resource prioritization example because we need an outcome for every active facility on every date. The following image tries to exemplify this reasoning: Figure. The image shows three facilities (blue, red and orange), and next to each, a temporal line with 6 days (0-5). Each dot represents the event (whether an inspection happened). Yellow means the inspection happened ( TRUE outcome) and blue means it didn't ( FALSE outcome). Each facility in the image had two inspections, six in total. Fortunately, triage will help us to create this table.","title":"What are the outcomes?"},{"location":"dirtyduck/eis/#what-are-the-entities-of-interest-the-cohort","text":"We are interested in predict only in active facilities (remember, in this case study, you own a restaurant, What is the point on predict if your restaurant is already closed for good?). This is the same cohort as the cohort table in the resource prioritization case study Experiment description file You could check the meaning about experiment description files (or configuration files) in A deeper look into triage . First the usual stuff. Note that we are changing model_comment and label_definition (remember that this is used for generating the hash that differentiates models and model groups). config_version : 'v7' model_comment : 'eis: 01' random_seed : 23895478 user_metadata : label_definition : 'inspected' experiment_type : 'eis' description : | EIS 01 purpose : 'model creation' org : 'DSaPP' team : 'Tutorial' author : 'Your name here' etl_date : '2019-05-07' model_group_keys : - 'class_path' - 'parameters' - 'feature_names' - 'feature_groups' - 'cohort_name' - 'state' - 'label_name' - 'label_timespan' - 'training_as_of_date_frequency' - 'max_training_history' - 'label_definition' - 'experiment_type' - 'org' - 'team' - 'author' - 'etl_date' For the labels the query is pretty simple, if the facility showed in the data, it will get a positive outcome, if not they will get a negative outcome label_config : query : | select entity_id, True::integer as outcome from semantic.events where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id include_missing_labels_in_train_as : False name : 'inspected' Note the two introduced changes in this block, first, the outcome is True , because all our observations represent inspected facilities (see discussion above and in particular previous image), second, we added the line include_missing_labels_in_train_as: False . This line tells triage to incorporate all the missing facilities in the training matrices with False as the label . As stated we will use the same configuration block for cohorts that we used in inspections: cohort_config : query : | select e.entity_id from semantic.entities as e where daterange(start_time, end_time, '[]') @> '{as_of_date}'::date name : 'active_facilities'","title":"What are the entities of interest? The cohort"},{"location":"dirtyduck/eis/#modeling-using-machine-learning","text":"We need to specify the temporal configuration, this section should reflect the operationalization of the model. Let\u2019s assume that every facility owner needs 6 months to prepare for an inspection. So, the model needs to answer the question: Will my restaurant be inspected in the next 6 months?","title":"Modeling Using Machine Learning"},{"location":"dirtyduck/eis/#temporal-configuration","text":"temporal_config : feature_start_time : '2010-01-04' feature_end_time : '2018-06-01' label_start_time : '2014-06-01' label_end_time : '2018-06-01' model_update_frequency : '6month' training_label_timespans : [ '6month' ] training_as_of_date_frequencies : '6month' test_durations : '6month' test_label_timespans : [ '6month' ] test_as_of_date_frequencies : '6month' max_training_histories : '5y' As before, you can generate the image of the temporal blocks: # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/eis_01.yaml --show-timechop What? \u2026 Bastion? bastion is the docker container that contains all the setup required to run this tutorial, if this is the first time that you see this word, you should stop and revisit setup infrastructure . Figure. Temporal blocks for the Early Warning System. We want to predict the most likely facilities to be inspected in the following 6 months","title":"Temporal configuration"},{"location":"dirtyduck/eis/#features","text":"Regarding the features, we will use the same ones that were used in inspections prioritization : feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates_imputation : count : type : 'zero_noflag' aggregates : - quantity : total : \"*\" metrics : - 'count' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'risks' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero' avg : type : 'zero' categoricals : - column : 'risk' choices : [ 'low' , 'medium' , 'high' ] metrics : - 'sum' - 'avg' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - 'zip_code' - prefix : 'results' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : all : type : 'zero' categoricals : - column : 'result' choice_query : 'select distinct result from semantic.events' metrics : - 'sum' - 'avg' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'inspection_types' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero_noflag' categoricals : - column : 'type' choice_query : 'select distinct type from semantic.events where type is not null' metrics : - 'sum' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - 'zip_code' We specify that we want to use all possible feature-group combinations for training: feature_group_definition : prefix : - 'inspections' - 'results' - 'risks' - 'inspection_types' feature_group_strategies : [ 'all' ] i.e. all will train models with all the features groups, leave-one-in will use only one of the feature groups for traning, and lastly, leave-one-out will train the model with all the features except one.","title":"Features"},{"location":"dirtyduck/eis/#algorithm-and-hyperparameters","text":"We will begin defining some basic models as baselines. 'triage.component.catwalk.baselines.thresholders.SimpleThresholder' : rules : - [ 'inspections_entity_id_1month_total_count > 0' ] - [ 'results_entity_id_1month_result_fail_sum > 0' ] - [ 'risks_entity_id_1month_risk_high_sum > 0' ] 'triage.component.catwalk.baselines.rankers.PercentileRankOneFeature' : feature : [ 'risks_entity_id_all_risk_high_sum' , 'inspections_entity_id_all_total_count' , 'results_entity_id_all_result_fail_sum' ] descend : [ True ] 'sklearn.dummy.DummyClassifier' : strategy : [ 'prior' , 'stratified' ] 'sklearn.tree.DecisionTreeClassifier' : criterion : [ 'gini' ] max_features : [ 'sqrt' ] max_depth : [ 1 , 2 , 5 , ~ ] min_samples_split : [ 2 ] 'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression' : penalty : [ 'l1' , 'l2' ] C : [ 0.000001 , 0.0001 , 0.01 , 1.0 ] How did I know the name of the features? triage has a very useful utility called featuretest triage featuretest experiments/eis_01.yaml 2018 -01-01 You can use for testing the definition of your features and also to see if the way that the features are calculated is actually what do you expect. Here we are using it just to check the name of the generated features. triage will create 20 model groups : algorithms and hyperparameters (4 DecisionTreeClassifier , 8 ScaledLogisticRegression , 2 DummyClassifier , 3 SimpleThresholder and 3 PercentileRankOneFeature ) \u00d7 1 features sets (1 all ). The total number of models is three times that (we have 6 time blocks, so 120 models). scoring : testing_metric_groups : - metrics : [ precision@ , recall@ ] thresholds : percentiles : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 95 , 100 ] top_n : [ 1 , 5 , 10 , 25 , 50 , 100 , 250 , 500 , 1000 ] training_metric_groups : - metrics : [ accuracy ] - metrics : [ precision@ , recall@ ] thresholds : percentiles : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 95 , 100 ] top_n : [ 1 , 5 , 10 , 25 , 50 , 100 , 250 , 500 , 1000 ] As a last step, we validate that the configuration file is correct: # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/eis_01.yaml --validate-only And then just run it: # Remember to run this in bastion NOT in your laptop shell! time triage experiment experiments/eis_01.yaml Protip We are including the command time in order to get the total running time of the experiment. You can remove it, if you like. This will take a lot amount of time (on my computer took 3h 42m), so, grab your coffee, chat with your coworkers, check your email, or read the DSSG blog . It's taking that long for several reasons: There are a lot of models, parameters, etc. We are running in serial mode (i.e. not in parallel). The database is running on your laptop. You can solve 2 and 3. For the second point you could use the docker container that has the multicore option enabled. For 3, I recommed you to use a PostgreSQL database in the cloud, such as Amazon's PostgreSQL RDS (we will explore this later in running triage in AWS Batch). After the experiment finishes, we can create the following table: with features_groups as ( select model_group_id , split_part ( unnest ( feature_list ), '_' , 1 ) as feature_groups from model_metadata . model_groups ), features_arrays as ( select model_group_id , array_agg ( distinct feature_groups ) as feature_groups from features_groups group by model_group_id ) select model_group_id , regexp_replace ( model_type , '^.*\\.' , '' ) as model_type , hyperparameters , feature_groups , array_agg ( model_id order by train_end_time asc ) as models , array_agg ( train_end_time :: date order by train_end_time asc ) as times , array_agg ( to_char ( stochastic_value , '0.999' ) order by train_end_time asc ) as \"precision@10% (stochastic)\" from model_metadata . models join features_arrays using ( model_group_id ) join test_results . evaluations using ( model_id ) where model_comment ~ 'eis' and metric || parameter = 'precision@10_pct' group by model_group_id , model_type , hyperparameters , feature_groups order by model_group_id ; model_group_id model_type hyperparameters feature_groups models times precision@10% (stochastic) 1 SimpleThresholder {\"rules\": [\"inspections_entity_id_1month_total_count > 0\"]} {inspection,inspections,results,risks} {1,19,37,55,73,91} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.358\",\" 0.231\",\" 0.321\",\" 0.267\",\" 0.355\",\" 0.239\"} 2 SimpleThresholder {\"rules\": [\"results_entity_id_1month_result_fail_sum > 0\"]} {inspection,inspections,results,risks} {2,20,38,56,74,92} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.316\",\" 0.316\",\" 0.323\",\" 0.344\",\" 0.330\",\" 0.312\"} 3 SimpleThresholder {\"rules\": [\"risks_entity_id_1month_risk_high_sum > 0\"]} {inspection,inspections,results,risks} {3,21,39,57,75,93} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.364\",\" 0.248\",\" 0.355\",\" 0.286\",\" 0.371\",\" 0.257\"} 4 PercentileRankOneFeature {\"descend\": true, \"feature\": \"risks_entity_id_all_risk_high_sum\"} {inspection,inspections,results,risks} {4,22,40,58,76,94} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.121\",\" 0.193\",\" 0.124\",\" 0.230\",\" 0.112\",\" 0.161\"} 5 PercentileRankOneFeature {\"descend\": true, \"feature\": \"inspections_entity_id_all_total_count\"} {inspection,inspections,results,risks} {5,23,41,59,77,95} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.076\",\" 0.133\",\" 0.098\",\" 0.101\",\" 0.086\",\" 0.082\"} 6 PercentileRankOneFeature {\"descend\": true, \"feature\": \"results_entity_id_all_result_fail_sum\"} {inspection,inspections,results,risks} {6,24,42,60,78,96} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.237\",\" 0.274\",\" 0.250\",\" 0.275\",\" 0.225\",\" 0.221\"} 7 DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 1, \"max_features\": \"sqrt\", \"min_samples_split\": 2} {inspection,inspections,results,risks} {7,25,43,61,79,97} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.284\",\" 0.441\",\" 0.559\",\" 0.479\",\" 0.463\",\" 0.412\"} 8 DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"min_samples_split\": 2} {inspection,inspections,results,risks} {8,26,44,62,80,98} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.401\",\" 0.388\",\" 0.533\",\" 0.594\",\" 0.519\",\" 0.649\"} 9 DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 5, \"max_features\": \"sqrt\", \"min_samples_split\": 2} {inspection,inspections,results,risks} {9,27,45,63,81,99} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.594\",\" 0.876\",\" 0.764\",\" 0.843\",\" 0.669\",\" 0.890\"} 10 DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"sqrt\", \"min_samples_split\": 2} {inspection,inspections,results,risks} {10,28,46,64,82,100} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.484\",\" 0.542\",\" 0.566\",\" 0.589\",\" 0.565\",\" 0.546\"} 11 ScaledLogisticRegression {\"C\": 0.000001, \"penalty\": \"l1\"} {inspection,inspections,results,risks} {11,29,47,65,83,101} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.272\",\" 0.318\",\" 0.306\",\" 0.328\",\" 0.292\",\" 0.281\"} 12 ScaledLogisticRegression {\"C\": 0.000001, \"penalty\": \"l2\"} {inspection,inspections,results,risks} {12,30,48,66,84,102} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.382\",\" 0.187\",\" 0.375\",\" 0.261\",\" 0.419\",\" 0.233\"} 13 ScaledLogisticRegression {\"C\": 0.0001, \"penalty\": \"l1\"} {inspection,inspections,results,risks} {13,31,49,67,85,103} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.275\",\" 0.314\",\" 0.306\",\" 0.329\",\" 0.462\",\" 0.421\"} 14 ScaledLogisticRegression {\"C\": 0.0001, \"penalty\": \"l2\"} {inspection,inspections,results,risks} {14,32,50,68,86,104} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.562\",\" 0.454\",\" 0.765\",\" 0.821\",\" 0.758\",\" 0.828\"} 15 ScaledLogisticRegression {\"C\": 0.01, \"penalty\": \"l1\"} {inspection,inspections,results,risks} {15,33,51,69,87,105} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.745\",\" 0.863\",\" 0.807\",\" 0.867\",\" 0.826\",\" 0.873\"} 16 ScaledLogisticRegression {\"C\": 0.01, \"penalty\": \"l2\"} {inspection,inspections,results,risks} {16,34,52,70,88,106} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.739\",\" 0.863\",\" 0.793\",\" 0.870\",\" 0.822\",\" 0.874\"} 17 ScaledLogisticRegression {\"C\": 1.0, \"penalty\": \"l1\"} {inspection,inspections,results,risks} {17,35,53,71,89,107} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.706\",\" 0.769\",\" 0.796\",\" 0.846\",\" 0.822\",\" 0.868\"} 18 ScaledLogisticRegression {\"C\": 1.0, \"penalty\": \"l2\"} {inspection,inspections,results,risks} {18,36,54,72,90,108} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.694\",\" 0.779\",\" 0.793\",\" 0.845\",\" 0.823\",\" 0.867\"} 19 DummyClassifier {\"strategy\": \"prior\"} {inspection,inspections,results,risks} {109,111,113,115,117,119} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.273\",\" 0.316\",\" 0.306\",\" 0.332\",\" 0.295\",\" 0.282\"} 20 DummyClassifier {\"strategy\": \"stratified\"} {inspection,inspections,results,risks} {110,112,114,116,118,120} {2014-12-01,2015-06-01,2015-12-01,2016-06-01,2016-12-01,2017-06-01} {\" 0.272\",\" 0.314\",\" 0.301\",\" 0.343\",\" 0.292\",\" 0.287\"} Protip You could have a \"real time\" version of the previous query while you are running the experiment config file with triage. Just execute \\watch n in the psql console and it will be refreshed every n n seconds","title":"Algorithm and hyperparameters"},{"location":"dirtyduck/eis/#lets-explore-more-second-grid","text":"After the baseline we will explore a more robust set of algorithms. We will use a different experiment config file: eis_02.yaml . The only differences between this experiment config file and the previous are in the user_metadata section: config_version : 'v7' model_comment : 'eis: 02' random_seed : 23895478 user_metadata : label_definition : 'inspected' experiment_type : 'eis' description : | EIS 02 purpose : 'model creation' org : 'DSaPP' team : 'Tutorial' author : 'Your name here' etl_date : '2019-05-07' and in the grid_config : grid_config : ## Boosting 'sklearn.ensemble.AdaBoostClassifier' : n_estimators : [ 1000 , 2000 ] 'sklearn.ensemble.GradientBoostingClassifier' : n_estimators : [ 1000 , 2000 ] learning_rate : [ 0.01 , 1.0 ] subsample : [ 0.5 , 1.0 ] min_samples_split : [ 2 ] max_depth : [ 2 , 5 ] ## Forest 'sklearn.tree.DecisionTreeClassifier' : criterion : [ 'gini' ] max_depth : [ 2 , 5 , 10 ] min_samples_split : [ 2 , 10 , 50 ] 'sklearn.ensemble.RandomForestClassifier' : n_estimators : [ 10000 ] criterion : [ 'gini' ] max_depth : [ 2 , 5 , 10 ] max_features : [ 'sqrt' ] min_samples_split : [ 2 , 10 , 50 ] n_jobs : [ -1 ] 'sklearn.ensemble.ExtraTreesClassifier' : n_estimators : [ 10000 ] criterion : [ 'gini' ] max_depth : [ 2 , 5 , 10 ] max_features : [ 'sqrt' ] min_samples_split : [ 2 , 10 , 50 ] n_jobs : [ -1 ] This new experiment configuration file will add 37 models groups to our experiment. You can run this experiment with: # Remember to run this in bastion NOT in your laptop shell! time triage experiment experiments/eis_02.yaml","title":"Let\u2019s explore more: second grid"},{"location":"dirtyduck/eis/#audition-so-many-models-how-can-i-choose-the-best-one","text":"Let\u2019s select the best model groups, using Audition. We need to make small changes to the /triage/audition/eis_audition_config.yaml compared to the inspection\u2019s one: # CHOOSE MODEL GROUPS model_groups : query : | select distinct(model_group_id) from model_metadata.model_groups where model_config ->> 'experiment_type' ~ 'eis' # CHOOSE TIMESTAMPS/TRAIN END TIMES time_stamps : query : | select distinct train_end_time from model_metadata.models where model_group_id in ({}) and extract(day from train_end_time) in (1) and train_end_time >= '2014-01-01' # FILTER filter : metric : 'precision@' # metric of interest parameter : '10_pct' # parameter of interest max_from_best : 1.0 # The maximum value that the given metric can be worse than the best model for a given train end time. threshold_value : 0.0 # The worst absolute value that the given metric should be. distance_table : 'eis_distance_table' # name of the distance table models_table : 'models' # name of the models table # RULES rules : - shared_parameters : - metric : 'precision@' parameter : '10_pct' selection_rules : - name : 'best_current_value' # Pick the model group with the best current metric value n : 5 - name : 'best_average_value' # Pick the model with the highest average metric value n : 5 - name : 'lowest_metric_variance' # Pick the model with the lowest metric variance n : 5 - name : 'most_frequent_best_dist' # Pick the model that is most frequently within `dist_from_best_case` dist_from_best_case : [ 0.05 ] n : 5 And then we run the simulation of the rules againts the experiment as: triage audition -c ./audition/eis_audition_config.yaml --directory audition/eis Audition will create several plots that will help you to sort out which is the best model group to use (like in a production setting or just to generate your list).","title":"Audition: So many models, how can I choose the best one?"},{"location":"dirtyduck/eis/#filtering-model-groups","text":"Audition will generate two plots that are meant to be used together: model performance over time and distance from best . Figure. Model group performance over time. In this case the metric show is precision@10% . The black dashed line represents the (theoretical) system's performance if we select the best performant model in a every evaluation date. The colored lines represents different model groups. All the model groups that share an algorithm will be colored the same. Figure. Proportion of all the models in a model group that are separated from the best model. The distance is measured in percentual points, i.e. How much less precision at 10 percent of the population compared to the best model in that date.","title":"Filtering model groups"},{"location":"dirtyduck/eis/#selecting-the-best-rule-or-strategy-for-choosing-model-groups","text":"In this phase of the audition, you will see what will happen in the next time if you choose your model group with an specific strategy or rule. You then, can calculate the regret . Regret is defined as the difference between the performance of the best model evaluated on the \"next time\" and the performance of the model selected by a particular rule. Figure. Given a strategy for selecting model groups (in the plot 4 are shown), What will be the performace of the model group chosen by that strategy in the next evaluation date? Figure. Given a strategy for selecting model groups (in the plot 4 are shown). What will be the distance (*regret ) to the best theoretical model in the following evaluation date.* Figure. Expected regret for the strategies. The less the better. It seems that the worst strategy (the one with the bigger \u201cregret\u201d) for selecting a model_group is lowest_metric_variance_precision . The other three seem almost indistinguishable. We will dig in using Postmodeling. And afterwards instead of using the feature importance to characterize the facilities, we will explore how the model is splitting the facilities using crosstabs . As before, the best 3 model groups per strategy will be stored in the file /triage/audition/eis/results_model_group_ids.json { \"most_frequent_best_dist_precision@_10_pct_0.05\" : [ 15 , 16 , 17 , 9 , 18 ], \"lowest_metric_variance_precision@_10_pct\" : [ 2 , 5 , 19 , 11 , 6 ], \"best_average_value_precision@_10_pct\" : [ 15 , 16 , 17 , 18 , 9 ], \"best_current_value_precision@_10_pct\" : [ 9 , 16 , 15 , 17 , 18 ] }","title":"Selecting the best rule or strategy for choosing model groups"},{"location":"dirtyduck/eis/#postmodeling-inspecting-the-best-models-closely","text":"Given that almost all the strategies perform well, we will change the parameter model_group_id in the postmodeling's configuration file and we will use the complete set of model groups selected by audition: # Postmodeling Configuration File project_path : '/triage' # Project path defined in triage with matrices and models audition_output_path : '/triage/audition/eis/results_model_group_ids.json' thresholds : # Thresholds for2 defining positive predictions rank_abs : [ 50 , 100 , 250 ] rank_pct : [ 5 , 10 , 25 ] baseline_query : | # SQL query for defining a baseline for comparison in plots. It needs a metric and parameter select g.model_group_id, m.model_id, extract('year' from m.evaluation_end_time) as as_of_date_year, m.metric, m.parameter, m.stochastic_value, m.num_labeled_examples, m.num_labeled_above_threshold, m.num_positive_labels from test_results.evaluations m left join model_metadata.models g using(model_id) where g.model_group_id = 20 and metric = 'precision@' and parameter = '10_pct' max_depth_error_tree : 5 # For error trees, how depth the decision trees should go? n_features_plots : 10 # Number of features for importances figsize : [ 12 , 12 ] # Default size for plots fontsize : 20 # Default fontsize for plots Launch jupyter in bastion : jupyter-notebook \u2013-ip = 0 .0.0.0 --port = 56406 --allow-root And then in your browser location bar type: http://0.0.0.0:56406","title":"Postmodeling: Inspecting the best models closely"},{"location":"dirtyduck/eis/#setup","text":"%matplotlib inline import pandas as pd import numpy as np from collections import OrderedDict from triage.component.postmodeling.contrast.utils.aux_funcs import create_pgconn, get_models_ids from triage.component.catwalk.storage import ProjectStorage, ModelStorageEngine, MatrixStorageEngine from triage.component.postmodeling.contrast.parameters import PostmodelParameters from triage.component.postmodeling.contrast.model_evaluator import ModelEvaluator from triage.component.postmodeling.contrast.model_group_evaluator import ModelGroupEvaluator params = PostmodelParameters('../triage/eis_postmodeling_config.yaml') engine = create_pgconn('database.yaml') # Model group object (useful to compare across model_groups and models in time) audited_models_class = ModelGroupEvaluator(tuple(params.model_group_id), engine)","title":"Setup"},{"location":"dirtyduck/eis/#model-groups","text":"Let\u2019s start with the behavior in time of the selected model groups audited_models_class.plot_prec_across_time(param_type='rank_pct', param=10, baseline=True, baseline_query=params.baseline_query, metric='precision@', figsize=params.figsize) Every model selected by audition has a very similar performance across time, and they are ~2.5 times above the baseline in precision@10%. We could also check the recall of the model groups. audited_models_class.plot_prec_across_time(param_type='rank_pct', param=10, metric='recall@', figsize=params.figsize) That behavior is similar for the recall@10%, except for the model group 69 audited_models_class.plot_jaccard_preds(param_type='rank_pct', param=10, temporal_comparison=True) There are a high jaccard similarity between some model groups across time. This could be an indicator that they are so similar that you can choose any and it won\u2019t matter.","title":"Model groups"},{"location":"dirtyduck/eis/#going-deeper-with-a-model","text":"We will choose the model group 64 as the winner. select mg . model_group_id , mg . model_type , mg . hyperparameters , array_agg ( model_id order by train_end_time ) as models from model_metadata . model_groups as mg inner join model_metadata . models using ( model_group_id ) where model_group_id = 76 group by 1 , 2 , 3 model group id model type hyperparameters models 64 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max features \": \"sqrt\", \"n estimators \": 500, \"min samples leaf \": 1, \"min samples split \": 50} {190,208,226} But before going to production and start making predictions in unseen data, let\u2019s see what the particular models are doing. Postmodeling created a ModelEvaluator (similar to the ModelGroupEvaluator ) to do this exploration: models_76 = { f'{model}': ModelEvaluator(76, model, engine) for model in [198,216,234] } In this tutorial, we will just show some parts of the analysis in the most recent model, but feel free of exploring the behavior of all the models in this model group, and check if you can detect any pattern. Feature importances models_76['234'].plot_feature_importances(path=params.project_path, n_features_plots=params.n_features_plots, figsize=params.figsize) models_76['234'].plot_feature_group_average_importances()","title":"Going deeper with a model"},{"location":"dirtyduck/eis/#crosstabs-how-are-the-entities-classified","text":"Model interpretation is a huge topic nowadays, the most obvious path is using the features importance from the model. This could be useful, but we could do a lot better. Triage uses crosstabs as a different approach that complements the list of features importance . crosstabs will run statistical tests to compare the predicted positive and the predicted false facilities in each feature. output : schema : 'test_results' table : 'eis_crosstabs' thresholds : rank_abs : [ 50 ] rank_pct : [ 5 ] #(optional): a list of entity_ids to subset on the crosstabs analysis entity_id_list : [] models_list_query : \"select unnest(ARRAY[226]) :: int as model_id\" as_of_dates_query : \"select generate_series('2017-12-01'::date, '2018-09-01'::date, interval '1month') as as_of_date\" #don't change this query unless strictly necessary. It is just validating pairs of (model_id,as_of_date) #it is just a join with distinct (model_id, as_of_date) in a predictions table models_dates_join_query : | select model_id, as_of_date from models_list_query as m cross join as_of_dates_query a join (select distinct model_id, as_of_date from test_results.predictions) as p using (model_id, as_of_date) #features_query must join models_dates_join_query with 1 or more features table using as_of_date features_query : | select m.model_id, m.as_of_date, f4.entity_id, f4.results_entity_id_1month_result_fail_avg, f4.results_entity_id_3month_result_fail_avg, f4.results_entity_id_6month_result_fail_avg, f2.inspection_types_zip_code_1month_type_canvass_sum, f3.risks_zip_code_1month_risk_high_sum, f4.results_entity_id_6month_result_pass_avg, f3.risks_entity_id_all_risk_high_sum, f2.inspection_types_zip_code_3month_type_canvass_sum, f4.results_entity_id_6month_result_pass_sum, f2.inspection_types_entity_id_all_type_canvass_sum from features.inspection_types_aggregation_imputed as f2 inner join features.risks_aggregation_imputed as f3 using (entity_id, as_of_date) inner join features.results_aggregation_imputed as f4 using (entity_id, as_of_date) inner join models_dates_join_query as m using (as_of_date) #the predictions query must return model_id, as_of_date, entity_id, score, label_value, rank_abs and rank_pct #it must join models_dates_join_query using both model_id and as_of_date predictions_query : | select model_id, as_of_date, entity_id, score, label_value, coalesce(rank_abs_no_ties, row_number() over (partition by (model_id, as_of_date) order by score desc)) as rank_abs, coalesce(rank_pct_no_ties*100, ntile(100) over (partition by (model_id, as_of_date) order by score desc)) as rank_pct from test_results.predictions join models_dates_join_query using(model_id, as_of_date) where model_id in (select model_id from models_list_query) and as_of_date in (select as_of_date from as_of_dates_query) triage --tb crosstabs /triage/eis_crosstabs_config.yaml When it finishes, you could explore the table with the following code: with significant_features as ( select feature_column , as_of_date , threshold_unit from test_results . eis_crosstabs where metric = 'ttest_p' and value < 0 . 05 and as_of_date = '2018-09-01' ) select distinct model_id , as_of_date :: date as as_of_date , format ( '%s %s' , threshold_value , t1 . threshold_unit ) as threshold , feature_column , value as \"ratio PP / PN\" from test_results . eis_crosstabs as t1 inner join significant_features as t2 using ( feature_column , as_of_date ) where metric = 'ratio_predicted_positive_over_predicted_negative' and t1 . threshold_unit = 'pct' order by value desc model id as of date threshold feature column ratio PP / PN 226 2018-09-01 5 pct results entity id 1month result fail avg 11.7306052855925 226 2018-09-01 5 pct results entity id 3month result fail avg 3.49082798996376 226 2018-09-01 5 pct results entity id 6month result fail avg 1.27344759545161 226 2018-09-01 5 pct risks zip code 1month risk high sum 1.17488357227451 226 2018-09-01 5 pct inspection types entity id all type canvass sum 0.946432281075976 226 2018-09-01 5 pct inspection types zip code 3month type canvass sum 0.888940127100436 226 2018-09-01 5 pct results entity id 6month result pass sum 0.041806916457784 226 2018-09-01 5 pct results entity id 6month result pass avg 0.0232523724927717 This table represents the ratio between the predicted positives at the top 5% and predicted negatives (the rest). For example, you can see that in PP are eleven times more inspected if they have a failed inspection in the last month, 3.5 times more if they have a failed inspection in the previous 3 months, etc.","title":"Crosstabs: How are the entities classified?"},{"location":"dirtyduck/eis/#where-to-go-from-here","text":"Ready to get started with your own data? Check out the suggested project workflow for some tips about how to iterate and tune the pipeline for your project. Want to work through another example? Take a look at our resource prioritization case study","title":"Where to go from here"},{"location":"dirtyduck/for_the_impatient/","text":"For the impatient # If you want to skip all the cleansing and transformation and deep directly into triage you can execute the following inside bastion : psql ${ DATABASE_URL } -c \"\\copy raw.inspections from program 'curl \" https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType = DOWNLOAD \"' HEADER CSV\" psql ${ DATABASE_URL } < /sql/create_cleaned_inspections_table.sql psql ${ DATABASE_URL } < /sql/create_violations_table.sql psql ${ DATABASE_URL } < /sql/create_semantic_tables.sql If everything works, you should end with two new schemas: cleaned and semantic . You could check that (from psql ) With \\ dn List of schemas Name Owner cleaned food_user postgis food_user public postgres raw food_user semantic food_user","title":"Quick setup"},{"location":"dirtyduck/for_the_impatient/#for-the-impatient","text":"If you want to skip all the cleansing and transformation and deep directly into triage you can execute the following inside bastion : psql ${ DATABASE_URL } -c \"\\copy raw.inspections from program 'curl \" https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType = DOWNLOAD \"' HEADER CSV\" psql ${ DATABASE_URL } < /sql/create_cleaned_inspections_table.sql psql ${ DATABASE_URL } < /sql/create_violations_table.sql psql ${ DATABASE_URL } < /sql/create_semantic_tables.sql If everything works, you should end with two new schemas: cleaned and semantic . You could check that (from psql ) With \\ dn List of schemas Name Owner cleaned food_user postgis food_user public postgres raw food_user semantic food_user","title":"For the impatient"},{"location":"dirtyduck/infrastructure/","text":"Setting up the Infrastructure # In every data science project you will need several tools to help analyze the data in an efficient 1 manner. Examples include a place to store the data (e.g. database management system or DBMS ); a way to put your model to work, e.g. a way that allows the model to ingest new data and make predictions (an API ); and a way to examine the performance of trained models (e.g. monitor tools). This tutorial includes a script for managing the infrastructure 2 in a transparent way. The infrastructure of this tutorial has three pieces: a postgresql database called food_db , a container that executes triage experiments (we will use this when trying to scale up), a container for interacting with the data called bastion . bastion includes a postgresql client (so you can interact with the database) 3 and a full python environment (so you can code or modify the things for the tutorial). The only thing you need installed on your laptop is docker . From your command line (terminal) run the following command from the repo directory: ./tutorial.sh Usage: ./tutorial.sh {up|down|build|rebuild|run|logs|status|clean} OPTIONS: -h|help Show this message up Starts Food DB down Stops Food DB build Builds images (food_db and bastion) rebuild Builds images (food_db and bastion) ignoring if they already exists -l|logs Shows container's logs status Shows status of the containers -d|clean Removes containers, images, volumes, netrowrks INFRASTRUCTURE: Build the DB's infrastructure: $ ./tutorial.sh up Check the status of the containers: $ ./tutorial.sh status Stop the tutorial's DB's infrastructure: $ ./tutorial.sh down Destroy all the resources related to the tutorial: $ ./tutorial.sh clean View the infrastructure logs: $ ./tutorial.sh -l Following the instructions on the screen, we can start the infrastructure with: ./tutorial.sh up You can check that everything is running smoothly with status by using the following command: ./tutorial.sh status Name Command State Ports ------------------------------------------------------------------------ tutorial_db docker-entrypoint.sh postgres Up 0.0.0.0:5434->5432/tcp To access bastion , where the postgresql client is, submit the command: ./tutorial.sh bastion Your prompt should change to something like: root@485373fb3c64:/$ NOTE : The number you see will be different (i.e. not 485373fb3c64 ). Inside bastion , type the next command to connect to the database psql ${ DATABASE_URL } The prompt will change again to (or something very similar): psql (10.7 (Debian 10.7-1.pgdg90+1)) Type \"help\" for help. food=# The previous command is using psql , a powerful command line client for the Postgresql database. If you want to use this client fully, check psql's documentation . The database is now running and is named food . It should contain a single table named inspections in the schema raw . Let's check the structure of the inspections table. Type the following command: \\ d raw . inspections Column Type Collation Nullable Default inspection character varying not null dba_name character varying aka_name character varying license_num numeric facility_type character varying risk character varying address character varying city character varying state character varying zip character varying date date type character varying results character varying violations character varying latitude numeric longitude numeric location character varying historical_wards character varying zip_codes character varying community_areas character varying census_tracts character varying wards character varying Info Column historical_wards contains the wards code from 2003 - 2015 That's it! We will work with thi table of raw inspections data. You can disconnect from the database by typing \\q . But don't leave the database yet! We still need to do a lot of things 4 Reproducible, scalable, flexible, etc. \u21a9 And other things through this tutorial, like the execution of the model training, etc. \u21a9 If you have a postgresql client installed, you can use psql -h 0.0.0.0 -p 5434 -d food -U food_user rather than the bastion container. \u21a9 Welcome to the not-so-sexy part of the (supposedly) sexiest job of the XXI century. \u21a9","title":"Infrastructure"},{"location":"dirtyduck/infrastructure/#setting-up-the-infrastructure","text":"In every data science project you will need several tools to help analyze the data in an efficient 1 manner. Examples include a place to store the data (e.g. database management system or DBMS ); a way to put your model to work, e.g. a way that allows the model to ingest new data and make predictions (an API ); and a way to examine the performance of trained models (e.g. monitor tools). This tutorial includes a script for managing the infrastructure 2 in a transparent way. The infrastructure of this tutorial has three pieces: a postgresql database called food_db , a container that executes triage experiments (we will use this when trying to scale up), a container for interacting with the data called bastion . bastion includes a postgresql client (so you can interact with the database) 3 and a full python environment (so you can code or modify the things for the tutorial). The only thing you need installed on your laptop is docker . From your command line (terminal) run the following command from the repo directory: ./tutorial.sh Usage: ./tutorial.sh {up|down|build|rebuild|run|logs|status|clean} OPTIONS: -h|help Show this message up Starts Food DB down Stops Food DB build Builds images (food_db and bastion) rebuild Builds images (food_db and bastion) ignoring if they already exists -l|logs Shows container's logs status Shows status of the containers -d|clean Removes containers, images, volumes, netrowrks INFRASTRUCTURE: Build the DB's infrastructure: $ ./tutorial.sh up Check the status of the containers: $ ./tutorial.sh status Stop the tutorial's DB's infrastructure: $ ./tutorial.sh down Destroy all the resources related to the tutorial: $ ./tutorial.sh clean View the infrastructure logs: $ ./tutorial.sh -l Following the instructions on the screen, we can start the infrastructure with: ./tutorial.sh up You can check that everything is running smoothly with status by using the following command: ./tutorial.sh status Name Command State Ports ------------------------------------------------------------------------ tutorial_db docker-entrypoint.sh postgres Up 0.0.0.0:5434->5432/tcp To access bastion , where the postgresql client is, submit the command: ./tutorial.sh bastion Your prompt should change to something like: root@485373fb3c64:/$ NOTE : The number you see will be different (i.e. not 485373fb3c64 ). Inside bastion , type the next command to connect to the database psql ${ DATABASE_URL } The prompt will change again to (or something very similar): psql (10.7 (Debian 10.7-1.pgdg90+1)) Type \"help\" for help. food=# The previous command is using psql , a powerful command line client for the Postgresql database. If you want to use this client fully, check psql's documentation . The database is now running and is named food . It should contain a single table named inspections in the schema raw . Let's check the structure of the inspections table. Type the following command: \\ d raw . inspections Column Type Collation Nullable Default inspection character varying not null dba_name character varying aka_name character varying license_num numeric facility_type character varying risk character varying address character varying city character varying state character varying zip character varying date date type character varying results character varying violations character varying latitude numeric longitude numeric location character varying historical_wards character varying zip_codes character varying community_areas character varying census_tracts character varying wards character varying Info Column historical_wards contains the wards code from 2003 - 2015 That's it! We will work with thi table of raw inspections data. You can disconnect from the database by typing \\q . But don't leave the database yet! We still need to do a lot of things 4 Reproducible, scalable, flexible, etc. \u21a9 And other things through this tutorial, like the execution of the model training, etc. \u21a9 If you have a postgresql client installed, you can use psql -h 0.0.0.0 -p 5434 -d food -U food_user rather than the bastion container. \u21a9 Welcome to the not-so-sexy part of the (supposedly) sexiest job of the XXI century. \u21a9","title":"Setting up the Infrastructure"},{"location":"dirtyduck/inspections/","text":"Resource prioritization systems: Chicago food inspections # Before continue, Did you\u2026? This case study, part of the dirtyduck tutorial, assumes that you already setup the tutorial\u2019s infrastructure and load the dataset. If you didn\u2019t setup the infrastructure go here , If you didn't load the data, you can do it very quickly or you can follow all the steps and explanations about the data . Problem description # We want to generate a list of facilities that will have a critical or serious food violation if inspected. The scenario is the following: you work for the City of Chicago and you have limited food inspectors, so you try to prioritize them to focus on the highest-risk facilities. So you will use the data to answer the next question: Which X X facilities are most likely to fail a food inspection in the following Y Y period of time? A more technical way of writing the question is: What is the probability distribution of facilities that are at risk of fail a food inspection if they are inspected in the following period of time? 1 If you want to focus on major violations only, you can do that too: Which X X facilities are most likely to have a critical or serious violation in the following Y Y period of time? This situation is very common in governmental agencies that provide social services: they need to prioritize their resources and use them in the facilities that are most likely to have problems We will use machine learning to accomplish this. This means that we will use historical data to train our models, and we will use temporal cross validation to test the performance of them. For the resource prioritization problems there are commonly two problems with the data: (a) bias and (b) incompleteness. First, note that our data have bias: We only have data on facilities that were inspected. That means that our data set contains information about the probability of have a violation ( V V ) given that the facility was inspected ( I I ), P(V|I) P(V|I) . But the probability that we try to find is P(V) P(V) . A different problem that our data set could have is if our dataset contains all the facilities in Chicago, i.e. if our entities table represents the Universe of facilities. There are almost 40,000 entities in our database. We could make the case that every facility in Chicago is in the database, since every facility that opens will be subject to an inspection. We will assume that all the facilities are in our data. What do you want to predict? # We are interested in two different outcomes : Which facilities are likely to fail an inspection? The outcome takes a 1 if the inspection had at least one result = 'fail' and a 0 otherwise. Which facilities fail an inspection with a major violation? Critical violations are coded between 1-14 , serious violations between 15-29 , everything above 30 is assumed to be a minor violation. The label takes a 1 if the inspection had at least one result = 'fail' and a violation between 1 and 29, and a 0 otherwise. I want to learn more about the data Check the Data preparation section! Data Changes On 7/1/2018 the Chicago Department of Public Health\u2019s Food Protection unit changed the definition of violations. The changes don\u2019t affect structurally the dataset (e.g. how the violations are inputted to the database), but the redefinition will change the distribution and interpretation of the violation codes. See here . We can extract the severity of the violation using the following code: select event_id , entity_id , date , result , array_agg ( distinct obj ->> 'severity' ) as violations_severity , ( result = 'fail' ) as failed , coalesce ( ( result = 'fail' and ( 'serious' = ANY ( array_agg ( obj ->> 'severity' )) or 'critical' = ANY ( array_agg ( obj ->> 'severity' ))) ), false ) as failed_major_violation from ( select event_id , entity_id , date , result , jsonb_array_elements ( violations :: jsonb ) as obj from semantic . events limit 20 ) as t1 group by entity_id , event_id , date , result order by date desc ; event_id entity_id date result violations_severity failed failed_major_violation 1770568 30841 2016-05-11 pass {minor} f f 1763967 30841 2016-05-03 fail {critical,minor,serious} t t 1434534 21337 2014-04-03 pass {NULL} f f 1343315 22053 2013-06-06 fail {minor,serious} t t 1235707 21337 2013-03-27 pass {NULL} f f 537439 13458 2011-06-10 fail {NULL} t f 569377 5570 2011-06-01 pass {NULL} f f The outcome will be used by triage to generate the labels (once that we define a time span of interest). The following image tries to show the meaning of the outcomes for the inspection failed problem definition. Figure. The image shows three facilities (blue, red and orange) and, next to each, a temporal line with 6 days (0-5). Each dot represents an inspection. Color is the outcome of the inspection. Green means the facility passed the inspection, and red means it failed. Each facility in the image had two inspections, but only the facility in the middle passed both. Modeling Using Machine Learning # It is time to put these steps together. All the coding is complete ( triage dev team did that for us); we just need to modify the triage experiment\u2019s configuration file. Defining a baseline # As a first step, lets do an experiment that defines our baseline . The rationale of this is that the knowing the baseline will allow us to verify if our Machine Learning model is better than the baseline. The baseline in our example will be a random selection of facilities 2 . This is implemented as a DummyClassifier in scikit-learn . Another advantage of starting with the baseline, is that is very fast to train ( DummyClassifier is not computationally expensive) , so it will help us to verify that the experiment configuration is correct without waiting for a long time. Experiment description file You could check the meaning about experiment description files (or configuration files) in A deeper look into triage . We need to write the experiment config file for that. Let's break it down and explain their sections. The config file for this first experiment is located in triage/experiments/inspections_baseline.yaml . The first lines of the experiment config file specify the config-file version ( v7 at the moment of writing this tutorial), a comment ( model_comment , which will end up as a value in the model_metadata.models table), and a list of user-defined metadata ( user_metadata ) that can help to identify the resulting model groups. For this example, if you run experiments that share a temporal configuration but that use different label definitions (say, labeling inspections with any violation as positive versus only labeling inspections with major violations as positive), you can use the user metadata keys to indicate that the matrices from these experiments have different labeling criteria. The matrices from the two experiments will have different filenames (and should not be overwritten or incorrectly used), and if you add the label_definition key to the model_group_keys , models made on different label definitions will belong to different model groups. Note Obviously, change 'Your name here' for your name (if you like) Next comes the temporal configuration section. The first four parameters are related to the availability of data: How much data you have for feature creation? How much data you have for label generation? Data Changes On 7/1/2018 the Chicago Department of Public Health\u2019s Food Protection unit changed the definition of violations. The changes don\u2019t affect structurally the dataset (e.g. how the violations are inputted to the database), but the redefinition will change the distribution and interpretation of the violation codes. See here . The next parameters are related to the training intervals: How frequently to retrain models? ( model_update_frequency ) How many rows per entity in the train matrices? ( training_as_of_date_frequencies ) How much time is covered by labels in the training matrices? ( training_label_timespans ) The remaining elements are related to the testing matrices. For inspections , you can choose them as follows: test_as_of_date_frequencies is planning/scheduling frequency test_durations how far ahead do you schedule inspections? test_label_timespan is equal to test_durations Let's assume that we need to do rounds of inspections every 6 months ( test_as_of_date_frequencies = 6month ) and we need to complete that round in exactly in that time (i.e. 6 months) ( test_durations = test_label_timespan = 6month ). We will assume that the data is more or less stable 3 , at least for now, so model_update_frequency = 6month. temporal_config : feature_start_time : '2010-01-04' feature_end_time : '2018-06-01' label_start_time : '2015-01-01' label_end_time : '2018-06-01' model_update_frequency : '6month' training_label_timespans : [ '6month' ] training_as_of_date_frequencies : '6month' test_durations : '0d' test_label_timespans : [ '6month' ] test_as_of_date_frequencies : '6month' max_training_histories : '5y' We can visualize the time splitting using the function show-timechop (See A deeper look into triage for more information) # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/inspections_baseline.yaml --show-timechop Figure. Temporal blocks for the inspections baseline experiment We need to specify our labels. For this first experiment we will use the label failed , using the same query from the simple_skeleton_experiment.yaml label_config : query : | select entity_id, bool_or(result = 'fail')::integer as outcome from semantic.events where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id name : 'failed_inspections' It should be obvious, but let's state it anyway: We are only training in facilities that were inspected, but we will test our model in all the facilities in our cohort 4 . So, in the train matrices we will have only 0 and 1 as possible labels, but in the test matrices we will found 0 , 1 and NULL . I want to learn more about this\u2026 In the section regarding to Early Warning Systems we will learn how to incorporate all the facilities of the cohort in the train matrices. We just want to include active facilities in our matrices, so we tell triage to take that in account: cohort_config : query : | select e.entity_id from semantic.entities as e where daterange(start_time, end_time, '[]') @> '{as_of_date}'::date name : 'active_facilities' Triage will generate the features for us, but we need to tell it which features we want in the section feature_aggregations . Here, each entry describes a collate.SpacetimeAggregation object and the arguments needed to create it 5 . For this experiment, we will use only one feature (number of inspections). DummyClassifier don't use any feature to do the \"prediction\", so we won't expend compute cycles doing the feature/matrix creation: feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates_imputation : count : type : 'zero_noflag' aggregates : - quantity : total : \"*\" metrics : - 'count' intervals : [ 'all' ] groups : - 'entity_id' feature_group_definition : prefix : - 'inspections' feature_group_strategies : [ 'all' ] If we observe the image generated from the temporal_config section, each particular date is the beginning of the rectangles that describes the rows in the matrix. In that date ( as_of_date in timechop parlance) we will calculate the feature, and we will repeat that for every other rectangle in that image. Now, let's discuss how we will specify the models to try (remember that the model is specified by the algorithm, the hyperparameters, and the subset of features to use). In triage you need to specify in the grid_config section a list of machine learning algorithms that you want to train and a list of hyperparameters. You can use any algorithm that you want; the only requirement is that it respects the sklearn API. grid_config : 'sklearn.dummy.DummyClassifier' : strategy : [ uniform ] Finally, we should define wich metrics we care about for evaluating our model. Here we will concentrate only in precision and recall at an specific value k k 6 . In this setting k k represents the resource\u2019s constraint: It is the number of inspections that the city could do in a month given all the inspectors available. scoring : testing_metric_groups : - metrics : [ precision@ , recall@ , 'false negatives@' , 'false positives@' , 'true positives@' , 'true negatives@' ] thresholds : percentiles : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 95 , 100 ] top_n : [ 1 , 5 , 10 , 25 , 50 , 100 , 250 , 500 , 1000 ] - metrics : [ auc , accuracy ] training_metric_groups : - metrics : [ auc , accuracy ] - metrics : [ precision@ , recall@ ] thresholds : percentiles : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 95 , 100 ] top_n : [ 1 , 5 , 10 , 25 , 50 , 100 , 250 , 500 , 1000 ] You should be warned that precision and recall at k k in this setting is kind of ill-defined (because you will end with a lot of NULL labels, remember, only a few of facilities are inspected in each period) 7 . We will want a list of facilities to be inspected. The length of our list is constrained by our inspection resources, i.e. the answer to the question How many facilities can I inspect in a month? In this experiment we are assuming that the maximum capacity is 10% but we are evaluating for a larger space of possibilities (see top_n , percentiles above). The execution of the experiments can take a long time, so it is a good practice to validate the configuration file before running the model. You don't want to wait for hours (or days) and then discover that something went wrong. # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/inspections_baseline.yaml --validate-only If everything was ok, you should see an Experiment validation ran to completion with no errors . You can execute the experiment as 8 # Remember to run this in bastion NOT in your laptop shell! time triage experiment experiments/inspections_baseline.yaml Protip We are including the command time in order to get the total running time of the experiment. You can remove it, if you like. Don\u2019t be scared! This will print a lot of output! It is not an error! We can query the table experiments to see the quantity of work that triage needs to do select substring ( experiment_hash , 1 , 4 ) as experiment , config -> 'user_metadata' ->> 'description' as description , total_features , matrices_needed , models_needed from model_metadata . experiments ; experiment description total_features matrices_needed models_needed e912 \"Baseline calculation\\n\" 1 10 5 If everything is correct, triage will create 10 matrices (5 for training, 5 for testing) in triage/matrices and every matrix will be represented by two files, one with the metadata of the matrix (a yaml file) and one with the actual matrix (the gz file). # We will use some bash magic ls matrices | awk -F . '{print $NF}' | sort | uniq -c Triage also will store 5 trained models in triage/trained_models : ls trained_models | wc -l And it will populate the results schema in the database. As mentioned, we will get 1 model groups : select model_group_id , model_type , hyperparameters from model_metadata . model_groups where model_config ->> 'experiment_type' ~ 'inspection' model_group_id model_type hyperparameters 1 sklearn.dummy.DummyClassifier {\"strategy\": \"prior\"} And 5 models : select model_group_id , array_agg ( model_id ) as models , array_agg ( train_end_time :: date ) as train_end_times from model_metadata . models where model_comment ~ 'inspection' group by model_group_id order by model_group_id ; model_group_id models train_end_times 1 {1,2,3,4,5} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} From that last query, you should note that the order in which triage trains the models is from oldest to newest train_end_time and model_group , also in ascending order. It will not go to the next block until all the models groups are trained. You can check on which matrix each models were trained: select model_group_id , model_id , train_end_time :: date , substring ( model_hash , 1 , 5 ) as model_hash , substring ( train_matrix_uuid , 1 , 5 ) as train_matrix_uuid , ma . num_observations as observations , ma . lookback_duration as feature_lookback_duration , ma . feature_start_time from model_metadata . models as mo join model_metadata . matrices as ma on train_matrix_uuid = matrix_uuid where mo . model_comment ~ 'inspection' order by model_group_id , train_end_time asc ; model_group_id model_id train_end_time model_hash train_matrix_uuid observations feature_lookback_duration feature_start_time 1 1 2015-12-01 743a6 1c266 5790 5 years 2010-01-04 00:00:00 1 2 2016-06-01 5b656 755c6 11502 5 years 2010-01-04 00:00:00 1 3 2016-12-01 5c170 2aea8 17832 5 years 2010-01-04 00:00:00 1 4 2017-06-01 55c3e 3efdc 23503 5 years 2010-01-04 00:00:00 1 5 2017-12-01 ac993 a3762 29112 5 years 2010-01-04 00:00:00 Each model was trained with the matrix indicated in the column train_matrix_uuid . This uuid is the file name of the stored matrix. The model itself was stored under the file named with the model_hash . If you want to see in which matrix the model was tested you need to run the following query select distinct model_id , model_group_id , train_end_time :: date , substring ( model_hash , 1 , 5 ) as model_hash , substring ( ev . matrix_uuid , 1 , 5 ) as test_matrix_uuid , ma . num_observations as observations from model_metadata . models as mo join test_results . evaluations as ev using ( model_id ) join model_metadata . matrices as ma on ev . matrix_uuid = ma . matrix_uuid where mo . model_comment ~ 'inspection' order by model_group_id , train_end_time asc ; model_id model_group_id train_end_time model_hash test_matrix_uuid observations 1 1 2015-12-01 743a6 4a0ea 18719 2 1 2016-06-01 5b656 f908e 19117 3 1 2016-12-01 5c170 00a88 19354 4 1 2017-06-01 55c3e 8f3cf 19796 5 1 2017-12-01 ac993 417f0 20159 All the models were stored in /triage/trained_models/{model_hash} using the standard serialization of sklearn models. Every model was trained with the matrix train_matrix_uuid stored in the directory /triage/matrices . What's the performance of this model groups? \\ set k 0 . 10 -- This defines a variable, \"k = 0.10\" select distinct model_group_id , model_id , ma . feature_start_time :: date , train_end_time :: date , ev . evaluation_start_time :: date , ev . evaluation_end_time :: date , to_char ( ma . num_observations , '999,999' ) as observations , to_char ( ev . num_labeled_examples , '999,999' ) as \"total labeled examples\" , to_char ( ev . num_positive_labels , '999,999' ) as \"total positive labels\" , to_char ( ev . num_labeled_above_threshold , '999,999' ) as \"labeled examples@k%\" , to_char (: k * ma . num_observations , '999,999' ) as \"predicted positive (PP)\" , ARRAY [ to_char ( ev . best_value * ev . num_labeled_above_threshold , '999,999' ), to_char ( ev . worst_value * ev . num_labeled_above_threshold , '999,999' ), to_char ( ev . stochastic_value * ev . num_labeled_above_threshold , '999,999' )] as \"true positive (TP)@k% (best,worst,stochastic)\" , ARRAY [ to_char ( ev . best_value , '0.999' ), to_char ( ev . worst_value , '0.999' ), to_char ( ev . stochastic_value , '0.999' )] as \"precision@k% (best,worst,stochastic)\" , to_char ( ev . num_positive_labels * 1 . 0 / ev . num_labeled_examples , '0.999' ) as baserate , : k * 100 as \"k%\" from model_metadata . models as mo join test_results . evaluations as ev using ( model_id ) join model_metadata . matrices as ma on ev . matrix_uuid = ma . matrix_uuid where ev . metric || ev . parameter = 'precision@15_pct' and mo . model_comment ~ 'inspection' order by model_id , train_end_time asc ; order by model_id , train_end_time asc ; model_group_id model_id feature_start_time train_end_time evaluation_start_time evaluation_end_time observations total labeled examples total positive labels labeled examples@k% predicted positive (PP) true positive (TP)@k% (best,worst,stochastic) precision@k% (best,worst,stochastic) baserate k% 21 121 2010-01-04 2015-12-01 2015-12-01 2015-12-01 18,719 5,712 1,509 0 2,808 {\" 0\",\" 0\",\" 0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.264 15.00 21 122 2010-01-04 2016-06-01 2016-06-01 2016-06-01 19,117 6,330 1,742 0 2,868 {\" 0\",\" 0\",\" 0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.275 15.00 21 123 2010-01-04 2016-12-01 2016-12-01 2016-12-01 19,354 5,671 1,494 0 2,903 {\" 0\",\" 0\",\" 0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.263 15.00 21 124 2010-01-04 2017-06-01 2017-06-01 2017-06-01 19,796 5,609 1,474 0 2,969 {\" 0\",\" 0\",\" 0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.263 15.00 21 125 2010-01-04 2017-12-01 2017-12-01 2017-12-01 20,159 4,729 1,260 0 3,024 {\" 0\",\" 0\",\" 0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.266 15.00 The columns num_labeled_examples, num_labeled_above_threshold, num_positive_labels represent the number of selected entities on the prediction date that are labeled, the number of entities with a positive label above the threshold, and the number of entities with positive labels among all the labeled entities respectively. We added some extra columns: baserate , predicted positive (PP) and true positive (TP) . Baserate represents the proportion of the all the facilities that were inspected that failed the inspection, i.e. P(V|I) P(V|I) . The PP and TP are approximate since it were calculated using the value of k k or the precision value. But you could get the exact value of those from the test_results.predictions table. Also note that in the precision@k% column we are showing three numbers: best , worst , stochastic . They try to answer the question How do you break ties in the prediction score? This is important because it will affect the calculation of your metrics. The Triage proposed solution to this is calculate the metric in the best case scenario (score descending, all the true labels are at the top), and then do it in the worst case scenario (score descending, all the true labels are at the bottom) and then calculate the metric several times ( n=30 n=30 ) with the labels randomly shuffled (a.k.a. stochastic scenario ), so you get the mean metric, plus some confidence intervals. This problem is not specific of an inspection problem, is more related to simple models like a shallow Decision Tree or a Dummy Classifier when score ties likely will occur. Note how in this model, the stochastic value is close to the baserate , since we are selecting at random using the prior . Check this! Note that the baserate should be equal to the precision@100% , if is not there is something wrong \u2026 Creating a simple experiment # We will try two of the simplest machine learning algorithms: a Decision Tree Classifier ( DT ) and a Scaled Logistic Regression ( SLR ) 12 as a second experiment. The rationale of this is that the DT is very fast to train (so it will help us to verify that the experiment configuration is correct without waiting for a long time) and it helps you to understand the structure of your data. The config file for this first experiment is located in /triage/experiments/inspections_dt.yaml Note that we don't modify the temporal_config section neither the feature_aggregations , cohort_config or label_config . Triage is smart enough to use the previous tables and matrices instead of generating them from scratch. config_version : 'v7' model_comment : 'inspections: basic ML' user_metadata : label_definition : 'failed' experiment_type : 'inspections prioritization' file_name : 'inspections_dt.yaml' description : | DT and SLR purpose : 'data mining' org : 'DSaPP' team : 'Tutorial' author : 'Your name here' etl_date : '2019-02-21' Note that we don't modify the temporal_config section neither the cohort_config or label_config . Triage is smart enough to use the previous tables and matrices instead of generating them from scratch. For this experiment, we will add the following features: Number of different types of inspections the facility had in the last year (calculated for an as-of-date ). Number of different types of inspections that happened in the zip code in the last year from a particular day. Number of inspections Number/proportion of inspections by result type Number/proportion of times that a facility was classify with particular risk level In all of them we will do the aggregation in the last month, 3 months, 6 months, 1 year and historically. Remember that all this refers to events in the past, i.e. How many times the facility was marked with high risk in the previous 3 Months? , What is the proportion of failed inspections in the previous year? feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates_imputation : count : type : 'zero_noflag' aggregates : - quantity : total : \"*\" metrics : - 'count' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'risks' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero' avg : type : 'zero' categoricals : - column : 'risk' choices : [ 'low' , 'medium' , 'high' ] metrics : - 'sum' - 'avg' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - 'zip_code' - prefix : 'results' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : all : type : 'zero' categoricals : - column : 'result' choice_query : 'select distinct result from semantic.events' metrics : - 'sum' - 'avg' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'inspection_types' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero_noflag' categoricals : - column : 'type' choice_query : 'select distinct type from semantic.events where type is not null' metrics : - 'sum' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - 'zip_code' And as stated, we will train some Decision Trees, in particular we are interested in some shallow trees, and in a full grown tree. These trees will show you the structure of your data. We also will train some Scaled Logistic Regression, this will show us how \"linear\" is the data (or how the assumptions of the Logistic Regression holds in this data) grid_config : 'sklearn.tree.DecisionTreeClassifier' : criterion : [ 'gini' ] max_features : [ 'sqrt' ] max_depth : [ 1 , 2 , 5 , ~ ] min_samples_split : [ 2 , 10 , 50 ] 'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression' : penalty : [ 'l1' , 'l2' ] C : [ 0.000001 , 0.0001 , 0.01 , 1.0 ] About yaml and sklearn Some of the parameters in sklearn are None . If you want to try those you need to indicate it with yaml 's null or ~ keyword. Besides the algorithm and the hyperparameters, you should specify which subset of features use. First, in the section feature_group_definition you specify how to group the features (you can use the table name or the prefix from the section feature_aggregation ) and then a strategy for choosing the subsets: all (all the subsets at once), leave-one-out (try all the subsets except one, do that for all the combinations), or leave-one-in (just try subset at the time). feature_group_definition : prefix : - 'inspections' - 'results' - 'risks' - 'inspection_types' feature_group_strategies : [ 'all' ] Finally we will leave the scoring section as before. In this experiment we will end with 6 model groups (number of algorithms [1] \\times \\times number of hyperparameter combinations [2 \\times \\times 3 = 5] \\times \\times number of feature groups strategies [1]]). Also, we will create 18 models (3 per model group) given that we have 3 temporal blocks (one model per temporal group). Before running the experiment, remember to validate that the configuration is correct: # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/inspections_dt.yaml --validate-only and check the temporal cross validation: # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/inspections_dt.yaml --show-timechop Temporal blocks for inspections experiment. The label is a failed inspection in the next year. You can execute the experiment like this: # Remember to run this in bastion NOT in your laptop shell! time triage experiment experiments/inspections_dt.yaml Again, we can run the following sql to see which things triage needs to run: select substring ( experiment_hash , 1 , 4 ) as experiment , config -> 'user_metadata' ->> 'description' as description , total_features , matrices_needed , models_needed from model_metadata . experiments ; experiment description total_features matrices_needed models_needed e912 Baseline calculation 1 10 5 b535 DT and SLR 201 10 100 You can compare our two experiments and there are several differences, mainly in the order of magnitude. Like the number of features (1 vs 201) and models built (5 vs 100). The After the experiment finishes, you will get 19 new model_groups (1 per combination in grid_config ) select model_group_id , model_type , hyperparameters from model_metadata . model_groups where model_group_id not in ( 1 ); model_group_id model_type hyperparameters 2 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 1, \"max_features\": \"sqrt\", \"min_samples_split\": 2} 3 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 1, \"max_features\": \"sqrt\", \"min_samples_split\": 10} 4 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 1, \"max_features\": \"sqrt\", \"min_samples_split\": 50} 5 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"min_samples_split\": 2} 6 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"min_samples_split\": 10} 7 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"min_samples_split\": 50} 8 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 5, \"max_features\": \"sqrt\", \"min_samples_split\": 2} 9 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 5, \"max_features\": \"sqrt\", \"min_samples_split\": 10} 10 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 5, \"max_features\": \"sqrt\", \"min_samples_split\": 50} 11 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"sqrt\", \"min_samples_split\": 2} 12 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"sqrt\", \"min_samples_split\": 10} 13 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"sqrt\", \"min_samples_split\": 50} 14 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.000001, \"penalty\": \"l1\"} 15 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.000001, \"penalty\": \"l2\"} 16 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.0001, \"penalty\": \"l1\"} 17 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.0001, \"penalty\": \"l2\"} 18 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.01, \"penalty\": \"l1\"} 19 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.01, \"penalty\": \"l2\"} 20 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 1.0, \"penalty\": \"l1\"} 21 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 1.0, \"penalty\": \"l2\"} and 100 models (as stated before) select model_group_id , array_agg ( model_id ) as models , array_agg ( train_end_time ) as train_end_times from model_metadata . models where model_group_id not in ( 1 ) group by model_group_id order by model_group_id ; model_group_id models train_end_times 2 {6,26,46,66,86} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 3 {7,27,47,67,87} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 4 {8,28,48,68,88} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 5 {9,29,49,69,89} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 6 {10,30,50,70,90} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 7 {11,31,51,71,91} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 8 {12,32,52,72,92} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 9 {13,33,53,73,93} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 10 {14,34,54,74,94} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 11 {15,35,55,75,95} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 12 {16,36,56,76,96} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 13 {17,37,57,77,97} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 14 {18,38,58,78,98} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 15 {19,39,59,79,99} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 16 {20,40,60,80,100} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 17 {21,41,61,81,101} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 18 {22,42,62,82,102} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 19 {23,43,63,83,103} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 20 {24,44,64,84,104} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 21 {25,45,65,85,105} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} Let's see the performance over time of the models so far: select model_group_id , array_agg ( model_id order by ev . evaluation_start_time asc ) as models , array_agg ( ev . evaluation_start_time :: date order by ev . evaluation_start_time asc ) as evaluation_start_time , array_agg ( ev . evaluation_end_time :: date order by ev . evaluation_start_time asc ) as evaluation_end_time , array_agg ( to_char ( ev . num_labeled_examples , '999,999' ) order by ev . evaluation_start_time asc ) as labeled_examples , array_agg ( to_char ( ev . num_labeled_above_threshold , '999,999' ) order by ev . evaluation_start_time asc ) as labeled_above_threshold , array_agg ( to_char ( ev . num_positive_labels , '999,999' ) order by ev . evaluation_start_time asc ) as total_positive_labels , array_agg ( to_char ( ev . stochastic_value , '0.999' ) order by ev . evaluation_start_time asc ) as \"precision@15%\" from model_metadata . models as mo inner join model_metadata . model_groups as mg using ( model_group_id ) inner join test_results . evaluations as ev using ( model_id ) where mg . model_config ->> 'experiment_type' ~ 'inspection' and ev . metric || ev . parameter = 'precision@15_pct' and model_group_id between 2 and 21 group by model_group_id model_group_id models evaluation_start_time evaluation_end_time labeled_examples labeled_above_threshold total_positive_labels precision@15% 1 {1,2,3,4,5} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 0\",\" 0\",\" 0\",\" 0\",\" 0\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\"} 2 {6,26,46,66,86} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 0\",\" 578\",\" 730\",\" 574\",\" 0\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.000\",\" 0.352\",\" 0.316\",\" 0.315\",\" 0.000\"} 3 {7,27,47,67,87} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 161\",\" 622\",\" 0\",\" 433\",\" 0\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.283\",\" 0.203\",\" 0.000\",\" 0.282\",\" 0.000\"} 4 {8,28,48,68,88} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 161\",\" 1,171\",\" 0\",\" 0\",\" 995\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.285\",\" 0.348\",\" 0.000\",\" 0.000\",\" 0.306\"} 5 {9,29,49,69,89} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 726\",\" 1,318\",\" 362\",\" 489\",\" 291\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.306\",\" 0.360\",\" 0.213\",\" 0.294\",\" 0.241\"} 6 {10,30,50,70,90} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 992\",\" 730\",\" 0\",\" 176\",\" 290\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.294\",\" 0.349\",\" 0.000\",\" 0.324\",\" 0.334\"} 7 {11,31,51,71,91} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 1,023\",\" 325\",\" 329\",\" 176\",\" 1,033\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.323\",\" 0.400\",\" 0.347\",\" 0.323\",\" 0.315\"} 8 {12,32,52,72,92} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 1,250\",\" 1,013\",\" 737\",\" 1,104\",\" 939\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.331\",\" 0.280\",\" 0.327\",\" 0.335\",\" 0.365\"} 9 {13,33,53,73,93} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 595\",\" 649\",\" 547\",\" 1,000\",\" 841\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.281\",\" 0.250\",\" 0.309\",\" 0.350\",\" 0.356\"} 10 {14,34,54,74,94} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 1,012\",\" 887\",\" 856\",\" 387\",\" 851\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.345\",\" 0.339\",\" 0.342\",\" 0.248\",\" 0.327\"} 11 {15,35,55,75,95} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 0\",\" 0\",\" 0\",\" 0\",\" 0\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\"} 12 {16,36,56,76,96} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 1,094\",\" 1,033\",\" 878\",\" 880\",\" 842\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.278\",\" 0.332\",\" 0.285\",\" 0.311\",\" 0.299\"} 13 {17,37,57,77,97} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 995\",\" 1,117\",\" 987\",\" 623\",\" 651\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.313\",\" 0.324\",\" 0.320\",\" 0.318\",\" 0.299\"} 14 {18,38,58,78,98} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 0\",\" 0\",\" 0\",\" 0\",\" 0\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\"} 15 {19,39,59,79,99} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 771\",\" 582\",\" 845\",\" 570\",\" 625\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.246\",\" 0.227\",\" 0.243\",\" 0.244\",\" 0.232\"} 16 {20,40,60,80,100} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 0\",\" 0\",\" 0\",\" 0\",\" 0\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\"} 17 {21,41,61,81,101} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 783\",\" 587\",\" 813\",\" 586\",\" 570\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.250\",\" 0.235\",\" 0.253\",\" 0.259\",\" 0.253\"} 18 {22,42,62,82,102} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 551\",\" 649\",\" 588\",\" 552\",\" 444\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.310\",\" 0.336\",\" 0.355\",\" 0.330\",\" 0.372\"} 19 {23,43,63,83,103} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 1,007\",\" 776\",\" 818\",\" 784\",\" 725\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.343\",\" 0.409\",\" 0.373\",\" 0.366\",\" 0.421\"} 20 {24,44,64,84,104} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 797\",\" 971\",\" 887\",\" 770\",\" 745\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.311\",\" 0.355\",\" 0.347\",\" 0.395\",\" 0.431\"} 21 {25,45,65,85,105} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 943\",\" 953\",\" 837\",\" 730\",\" 699\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.320\",\" 0.363\",\" 0.349\",\" 0.397\",\" 0.429\"} Which model in production ( model selection ) is something that we will review later, with Audition , but for now, let's choose the model group 3 and see the predictions table: select model_id , entity_id , as_of_date :: date , round ( score , 2 ), label_value as label from test_results . predictions where model_id = 11 order by as_of_date asc , score desc limit 20 model_id entity_id as_of_date round label 11 26873 2015-06-01 0.49 11 26186 2015-06-01 0.49 11 25885 2015-06-01 0.49 11 24831 2015-06-01 0.49 11 24688 2015-06-01 0.49 11 21485 2015-06-01 0.49 11 20644 2015-06-01 0.49 11 20528 2015-06-01 0.49 11 19531 2015-06-01 0.49 11 18279 2015-06-01 0.49 11 17853 2015-06-01 0.49 11 17642 2015-06-01 0.49 11 16360 2015-06-01 0.49 11 15899 2015-06-01 0.49 11 15764 2015-06-01 0.49 11 15381 2015-06-01 0.49 11 15303 2015-06-01 0.49 11 14296 2015-06-01 0.49 11 14016 2015-06-01 0.49 11 27627 2015-06-01 0.49 NOTE: Given that this is a shallow tree, there will be a lot of entities with the same score ,you probably will get a different set of entities, since postgresql will sort them at random. It is important to know\u2026 Triage sorted the predictions at random using the random_seed from the experiment\u2019s config file. If you want the predictions being sorted in a different way add prediction: randk_tiebreaker: \"worst\" # or \"best\" or \"random\" Note that at the top of the list (sorted by as_of_date , and then by score ), the labels are NULL . This means that the facilities that you are classifying as high risk, actually weren't inspected in that as of date . So, you actually don't know if this is a correct prediction or not. This is a characteristic of all the resource optimization problems: You do not have all the information about the elements in your system 9 . So, how the precision/recall is calculated? The number that is show in the evaluations table is calculated using only the rows that have a non-null label. You could argue that this is fine, if you assume that the distribution of the label in the non-observed facilities is the same that the ones that were inspected that month 10 . We will come back to this problem in the Early Warning Systems. A more advanced experiment # Ok, let's add a more complete experiment. First the usual generalities. config_version : 'v7' model_comment : 'inspections: advanced' user_metadata : label_definition : 'failed' experiment_type : 'inspections prioritization' description : | Using Ensamble methods purpose : 'trying ensamble algorithms' org : 'DSaPP' team : 'Tutorial' author : 'Your name here' etl_date : '2019-02-21' We won't change anything related to features, cohort and label definition neither to temporal configuration. As before, we can check the temporal structure of our crossvalidation: # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/inspections_label_failed_01.yaml --show-timechop Figure. Temporal blocks for inspections experiment. The label is a failed inspection in the next month. We want to use all the features groups ( feature_group_definition ). The training will be made on matrices with all the feature groups, then leaving one feature group out at a time, leave-one-out (i.e. one model with inspections and results , another with inspections and risks , and another with results and risks), and finally leaving one feature group in at a time (i.e. a model with inspections only, another with results only, and a third with risks` only). feature_group_definition : prefix : - 'inspections' - 'results' - 'risks' - 'inspection_types' feature_group_strategies : [ 'all' , 'leave-one-in' , 'leave-one-out' ] Finally, we will try some RandomForestClassifier : grid_config : 'sklearn.ensemble.RandomForestClassifier' : n_estimators : [ 10000 ] criterion : [ 'gini' ] max_depth : [ 2 , 5 , 10 ] max_features : [ 'sqrt' ] min_samples_split : [ 2 , 10 , 50 ] n_jobs : [ -1 ] 'sklearn.ensemble.ExtraTreesClassifier' : n_estimators : [ 10000 ] criterion : [ 'gini' ] max_depth : [ 2 , 5 , 10 ] max_features : [ 'sqrt' ] min_samples_split : [ 2 , 10 , 50 ] n_jobs : [ -1 ] scoring : testing_metric_groups : - metrics : [ precision@ , recall@ ] thresholds : percentiles : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 95 , 100 ] top_n : [ 1 , 5 , 10 , 25 , 50 , 100 , 250 , 500 , 1000 ] training_metric_groups : - metrics : [ accuracy ] - metrics : [ precision@ , recall@ ] thresholds : percentiles : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 95 , 100 ] top_n : [ 1 , 5 , 10 , 25 , 50 , 100 , 250 , 500 , 1000 ] Before running, let's verify the configuration file # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/inspections_label_failed_01.yaml --validate-only You can execute the experiment with # Remember to run this in bastion NOT in your laptop shell! time triage experiment experiments/inspections_label_failed_01.yaml This will take a looooong time to run. The reason for that is easy to understand: We are computing a lot of models: 6 time splits, 18 model groups and 9 features sets (one for all , four for leave_one_in and four for leave_one_out ), so 6 \\times 18 \\times 9 = 486 6 \\times 18 \\times 9 = 486 extra models. Well, now we have a lot of models. How can you pick the best one? You could try the following query: with features_groups as ( select model_group_id , split_part ( unnest ( feature_list ), '_' , 1 ) as feature_groups from model_metadata . model_groups ), features_arrays as ( select model_group_id , array_agg ( distinct feature_groups ) as feature_groups from features_groups group by model_group_id ) select model_group_id , model_type , hyperparameters , feature_groups , array_agg ( to_char ( stochastic_value , '0.999' ) order by train_end_time asc ) filter ( where metric = 'precision@' ) as \"precision@15%\" , array_agg ( to_char ( stochastic_value , '0.999' ) order by train_end_time asc ) filter ( where metric = 'recall@' ) as \"recall@15%\" from model_metadata . models join features_arrays using ( model_group_id ) join test_results . evaluations using ( model_id ) where model_comment ~ 'inspection' and parameter = '15_pct' group by model_group_id , model_type , hyperparameters , feature_groups order by model_group_id ; This is a long table \u2026 model_group_id model_type hyperparameters feature_groups precision@15% recall@15% 1 sklearn.dummy.DummyClassifier {\"strategy\": \"prior\"} {inspections} {\" 0.339\",\" 0.366\",\" 0.378\"} {\" 0.153\",\" 0.151\",\" 0.149\"} 2 sklearn.tree.DecisionTreeClassifier {\"max_depth\": 2, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.347\",\" 0.394\",\" 0.466\"} {\" 0.155\",\" 0.153\",\" 0.180\"} 3 sklearn.tree.DecisionTreeClassifier {\"max_depth\": 2, \"min_samples_split\": 5} {inspection,inspections,results,risks} {\" 0.349\",\" 0.397\",\" 0.468\"} {\" 0.156\",\" 0.154\",\" 0.181\"} 4 sklearn.tree.DecisionTreeClassifier {\"max_depth\": 10, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.409\",\" 0.407\",\" 0.470\"} {\" 0.179\",\" 0.163\",\" 0.178\"} 5 sklearn.tree.DecisionTreeClassifier {\"max_depth\": 10, \"min_samples_split\": 5} {inspection,inspections,results,risks} {\" 0.416\",\" 0.409\",\" 0.454\"} {\" 0.183\",\" 0.160\",\" 0.169\"} 6 sklearn.tree.DecisionTreeClassifier {\"max_depth\": null, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.368\",\" 0.394\",\" 0.413\"} {\" 0.165\",\" 0.161\",\" 0.160\"} 7 sklearn.tree.DecisionTreeClassifier {\"max_depth\": null, \"min_samples_split\": 5} {inspection,inspections,results,risks} {\" 0.386\",\" 0.397\",\" 0.417\"} {\" 0.171\",\" 0.161\",\" 0.162\"} 8 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.441\",\" 0.471\",\" 0.513\"} {\" 0.190\",\" 0.187\",\" 0.193\"} 9 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.470\",\" 0.478\",\" 0.532\"} {\" 0.200\",\" 0.189\",\" 0.200\"} 10 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection,inspections,results,risks} {\" 0.481\",\" 0.479\",\" 0.513\"} {\" 0.204\",\" 0.189\",\" 0.193\"} 11 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection,inspections,results,risks} {\" 0.474\",\" 0.472\",\" 0.535\"} {\" 0.202\",\" 0.183\",\" 0.199\"} 12 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspections} {\" 0.428\",\" 0.417\",\" 0.389\"} {\" 0.179\",\" 0.149\",\" 0.148\"} 13 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspections} {\" 0.428\",\" 0.417\",\" 0.390\"} {\" 0.180\",\" 0.149\",\" 0.148\"} 14 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspections} {\" 0.427\",\" 0.417\",\" 0.376\"} {\" 0.179\",\" 0.149\",\" 0.140\"} 15 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspections} {\" 0.428\",\" 0.417\",\" 0.380\"} {\" 0.179\",\" 0.149\",\" 0.143\"} 16 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {results} {\" 0.415\",\" 0.398\",\" 0.407\"} {\" 0.180\",\" 0.157\",\" 0.157\"} 17 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {results} {\" 0.393\",\" 0.401\",\" 0.404\"} {\" 0.171\",\" 0.158\",\" 0.155\"} 18 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {results} {\" 0.436\",\" 0.425\",\" 0.447\"} {\" 0.191\",\" 0.169\",\" 0.171\"} 19 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {results} {\" 0.432\",\" 0.423\",\" 0.438\"} {\" 0.188\",\" 0.168\",\" 0.167\"} 20 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {risks} {\" 0.413\",\" 0.409\",\" 0.431\"} {\" 0.184\",\" 0.170\",\" 0.166\"} 21 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {risks} {\" 0.407\",\" 0.391\",\" 0.459\"} {\" 0.180\",\" 0.159\",\" 0.179\"} 22 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {risks} {\" 0.418\",\" 0.432\",\" 0.469\"} {\" 0.184\",\" 0.176\",\" 0.181\"} 23 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {risks} {\" 0.427\",\" 0.431\",\" 0.476\"} {\" 0.187\",\" 0.176\",\" 0.183\"} 24 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection} {\" 0.435\",\" 0.483\",\" 0.483\"} {\" 0.193\",\" 0.194\",\" 0.186\"} 25 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection} {\" 0.448\",\" 0.465\",\" 0.518\"} {\" 0.196\",\" 0.188\",\" 0.202\"} 26 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection} {\" 0.446\",\" 0.446\",\" 0.508\"} {\" 0.189\",\" 0.179\",\" 0.193\"} 27 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection} {\" 0.459\",\" 0.444\",\" 0.513\"} {\" 0.198\",\" 0.176\",\" 0.198\"} 28 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection,results,risks} {\" 0.472\",\" 0.479\",\" 0.506\"} {\" 0.202\",\" 0.191\",\" 0.190\"} 29 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection,results,risks} {\" 0.476\",\" 0.486\",\" 0.532\"} {\" 0.202\",\" 0.191\",\" 0.199\"} 30 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection,results,risks} {\" 0.485\",\" 0.454\",\" 0.535\"} {\" 0.203\",\" 0.180\",\" 0.204\"} 31 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection,results,risks} {\" 0.479\",\" 0.497\",\" 0.521\"} {\" 0.205\",\" 0.193\",\" 0.196\"} 32 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection,inspections,risks} {\" 0.437\",\" 0.432\",\" 0.474\"} {\" 0.191\",\" 0.178\",\" 0.181\"} 33 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection,inspections,risks} {\" 0.459\",\" 0.468\",\" 0.501\"} {\" 0.202\",\" 0.191\",\" 0.197\"} 34 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection,inspections,risks} {\" 0.461\",\" 0.448\",\" 0.482\"} {\" 0.201\",\" 0.181\",\" 0.187\"} 35 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection,inspections,risks} {\" 0.463\",\" 0.445\",\" 0.497\"} {\" 0.200\",\" 0.180\",\" 0.189\"} 36 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection,inspections,results} {\" 0.462\",\" 0.448\",\" 0.513\"} {\" 0.199\",\" 0.177\",\" 0.191\"} 37 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection,inspections,results} {\" 0.465\",\" 0.491\",\" 0.537\"} {\" 0.197\",\" 0.190\",\" 0.203\"} 38 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection,inspections,results} {\" 0.459\",\" 0.481\",\" 0.522\"} {\" 0.193\",\" 0.187\",\" 0.198\"} 39 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection,inspections,results} {\" 0.474\",\" 0.479\",\" 0.536\"} {\" 0.203\",\" 0.188\",\" 0.201\"} 40 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspections,results,risks} {\" 0.436\",\" 0.429\",\" 0.490\"} {\" 0.189\",\" 0.174\",\" 0.185\"} 41 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspections,results,risks} {\" 0.441\",\" 0.448\",\" 0.515\"} {\" 0.190\",\" 0.180\",\" 0.194\"} 42 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspections,results,risks} {\" 0.460\",\" 0.475\",\" 0.481\"} {\" 0.198\",\" 0.189\",\" 0.178\"} 43 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspections,results,risks} {\" 0.465\",\" 0.446\",\" 0.496\"} {\" 0.199\",\" 0.179\",\" 0.187\"} This table summarizes all our experiments, but it is very difficult to use if you want to choose the best combination of hyperparameters and algorithm (i.e. the model group). In the next section will solve this dilemma with the support of audition . Selecting the best model # 43 model groups ! How to pick the best one and use it for making predictions with new data? What do you mean by \u201cthe best\u201d? This is not as easy as it sounds, due to several factors: You can try to pick the best using a metric specified in the config file ( precision@ and recall@ ), but at what point of time? Maybe different model groups are best at different prediction times. You can just use the one that performs best on the last test set. You can value a model group that provides consistent results over time. It might not be the best on any test set, but you can feel more confident that it will continue to perform similarly. If there are several model groups that perform similarly and their lists are more or less similar, maybe it doesn't really matter which you pick. Remember\u2026 Before move on, remember the two main caveats for the value of the metric in this kind of ML problems: Could be many entities with the same predicted risk score ( ties ) Could be a lot of entities without a label (Weren't inspected, so we don\u2019t know) We included a simple configuration file in /triage/audition/inspection_audition_config.yaml with some rules: # CHOOSE MODEL GROUPS model_groups : query : | select distinct(model_group_id) from model_metadata.model_groups where model_config ->> 'experiment_type' ~ 'inspection' # CHOOSE TIMESTAMPS/TRAIN END TIMES time_stamps : query : | select distinct train_end_time from model_metadata.models where model_group_id in ({}) and extract(day from train_end_time) in (1) and train_end_time >= '2014-01-01' # FILTER filter : metric : 'precision@' # metric of interest parameter : '10_pct' # parameter of interest max_from_best : 1.0 # The maximum value that the given metric can be worse than the best model for a given train end time. threshold_value : 0.0 # The worst absolute value that the given metric should be. distance_table : 'inspections_distance_table' # name of the distance table models_table : 'models' # name of the models table # RULES rules : - shared_parameters : - metric : 'precision@' parameter : '10_pct' selection_rules : - name : 'best_current_value' # Pick the model group with the best current metric value n : 3 - name : 'best_average_value' # Pick the model with the highest average metric value n : 3 - name : 'lowest_metric_variance' # Pick the model with the lowest metric variance n : 3 - name : 'most_frequent_best_dist' # Pick the model that is most frequently within `dist_from_best_case` dist_from_best_case : [ 0.05 ] n : 3 Audition will have each rule give you the best n n model groups based on the metric and parameter following that rule for the most recent time period (in all the rules shown n n = 3). We can run the simulation of the rules against the experiment as: # Run this in bastion\u2026 triage --tb audition -c inspection_audition_config.yaml --directory audition/inspections Audition will create several plots that will help you to sort out which is the best model group to use (like in a production setting or just to generate your predictions list). Filtering model groups # Most of the time, audition should be used in a iterative fashion, the result of each iteration will be a reduced set of models groups and a best rule for selecting model groups. If you look again at the audition configuration file above you can filter the number of models to consider using the parameters max_from_best and threshold_value . The former will filter out models groups with models which performance in the metric is farther than the max_from_best (In this case we are allowing all the models, since max_from_best = 1.0 , if you want less models you could choose 0.1 for example, and you will remove the DummyClassifier and some DecisionTreeClassifiers ). threshold_value filter out all the models groups with models performing below that the specified value. This could be important if you don\u2019t find acceptable models with metrics that are that low. Audition will generate two plots that are meant to be used together: model performance over time and distance from best . Figure. Model group performance over time. In this case the metric show is precision@10% . We didn\u2019t filter out any model group, so the 45 model groups are shown. See discussion above to learn how to plot less model groups. The black dashed line represents the (theoretical) system's performance if we select the best performant model in a every evaluation date. The colored lines represents different model groups. All the model groups that share an algorithm will be colored the same. Next figure shows the proportion of models that are behind the best model. The distance is measured in percentual points. You could use this plot to filter out more model groups, changing the value of max_from_best in the configuration file. This plot is hard to read, but is very helpful since it shows you the consistency of the model group: How consistently are the model group in a specific range, let's say 20 points, from the best? Figure. Proportion of *models in a model group that are separated from the best model. The distance is measured in percentual points, i.e. How much less precision at 10 percent of the population compared to the best model in that date.* In the figure, you can see that the ~60% of the DummyClassifier models are ~18 percentual points below of the best. Selecting the best rule or strategy for choosing model groups # In this phase of the audition, you will see what will happen in the next time if you choose your model group with an specific strategy or rule. We call this the regret of the strategies. We define regret as Regret Is the difference in performance between the model group you picked and the best one in the next time period. The next plot show the best model group selected by the strategies specified in the configuration file: Figure. Given a strategy for selecting model groups (in the figure, 4 are shown), What will be the performace of the model group chosen by that strategy in the next evaluation date? It seems that the strategies best average and best current value select the same model group . Figure. Given a strategy for selecting model groups (in the plot 4 are shown). What will be the distance (*regret ) to the best theoretical model in the following evaluation date?* Obviously, you don\u2019t know the future, but with the available data, if you stick to an a particular strategy, How much you will regret about that decision? Figure. Expected regret for the strategies. The less the better. The best 3 model groups per strategy will be stored in the file [[file:audition/inspections/results_model_group_ids.json][results_model_group_ids.json]] : { \"best_current_value_precision@_10_pct\" : [ 39 , 30 , 9 ], \"best_average_value_precision@_10_pct\" : [ 39 , 9 , 29 ], \"lowest_metric_variance_precision@_10_pct\" : [ 1 , 5 , 19 ], \"most_frequent_best_dist_precision@_10_pct_0.05\" : [ 8 , 9 , 10 ] } The analysis suggests that the best strategies are select the model groups ( 39,9,29 ) which have the best average precision@10% value or, select the best model group ( 39,30,9 ) using precision@10% today and use it for the next time period. You will note that both strategies share two models groups and differ in one. In the next two sections, we will investigate further those four model groups selected by audition , using the Postmodeling tool set. Postmodeling: Inspecting the best models closely # Postmodeling will help you to understand the behaviour orf your selected models (from audition) As in Audition , we will split the postmodeling process in two parts. The first one is about exploring the model groups filtered by audition, with the objective of select one. The second part is about learning about models in the model group that was selected. We will setup some parameters in the postmodeling configuration file located at /triage/postmodeling/inspection_postmodeling_config.yaml , mainly where is the audition\u2019s output file located. # Postmodeling Configuration File project_path : '/triage' # Project path defined in triage with matrices and models model_group_id : - 39 - 9 - 29 - 30 thresholds : # Thresholds for defining positive predictions rank_abs : [ 50 , 100 , 250 ] rank_pct : [ 5 , 10 , 25 ] baseline_query : | # SQL query for defining a baseline for comparison in plots. It needs a metric and parameter select g.model_group_id, m.model_id, extract('year' from m.evaluation_end_time) as as_of_date_year, m.metric, m.parameter, m.value, m.num_labeled_examples, m.num_labeled_above_threshold, m.num_positive_labels from test_results.evaluations m left join model_metadata.models g using(model_id) where g.model_group_id = 1 and metric = 'precision@' and parameter = '10_pct' max_depth_error_tree : 5 # For error trees, how depth the decision trees should go? n_features_plots : 10 # Number of features for importances figsize : [ 12 , 12 ] # Default size for plots fontsize : 20 # Default fontsize for plots Compared to the previous sections, postmodeling is not an automated process (yet). Hence, to do the following part of the tutorial, you need to run jupyter inside bastion as follows: jupyter-notebook \u2013-ip = 0 .0.0.0 --port = 56406 --allow-root And then in your browser type 11 : http://0.0.0.0:56406 Now that you are in a jupyter notebook, type the following: %matplotlib inline import matplotlib #matplotlib.use('Agg') import triage import pandas as pd import numpy as np from collections import OrderedDict from triage.component.postmodeling.contrast.utils.aux_funcs import create_pgconn, get_models_ids from triage.component.catwalk.storage import ProjectStorage, ModelStorageEngine, MatrixStorageEngine from triage.component.postmodeling.contrast.parameters import PostmodelParameters from triage.component.postmodeling.contrast.model_evaluator import ModelEvaluator from triage.component.postmodeling.contrast. model_group_evaluator import ModelGroupEvaluator After importing, we need to create an sqlalchemy engine for connecting to the database, and read the configuration file. params = PostmodelParameters('inspection_postmodeling_config.yaml') engine = create_pgconn('database.yaml') Postmodeling provides the object ModelGroupEvaluator to compare different model groups . audited_models_class = ModelGroupEvaluator(tuple(params.model_group_id), engine) Comparing the audited model groups # First we will compare the performance of the audited model groups and the baseline over time. First, we will plot precision@10_pct audited_models_class.plot_prec_across_time(param_type='rank_pct', param=10, baseline=True, baseline_query=params.baseline_query, metric='precision@', figsize=params.figsize) Figure. Precision@10% over time from the best performing model groups selected by Audition and now the recall@10_pct audited_models_class.plot_prec_across_time(param_type='rank_pct', param=10, metric='recall@', figsize=params.figsize) Figure. Recall@10% over time from the best performing model groups selected by Audition All the selected model groups have a very similar performance. Let\u2019s see if they are predicting similar lists of facilities that are at risk of fail an inspection. audited_models_class.plot_jaccard_preds(param_type='rank_pct', param=10, temporal_comparison=True) Figure. How similar are the model groups\u2019 generated list? We use Jaccard similarity on the predicted lists (length of list 10%) to asses the overlap between lists. The plot will shows the overlap of the predicted list containing the 10% of the facilities between model groups at each as of date . The lists are at least 50% similar. Tip Why the models are not learning the same? You should investigate why is that so. This could lead you to defining new features or some another conclusion about your data, but for this tutorial we will move on. Going deeper with a model # Imagine that after a deeper analysis, you decide to choose model group 39 select mg . model_group_id , mg . model_type , mg . hyperparameters , array_agg ( model_id order by train_end_time ) as models from model_metadata . model_groups as mg inner join model_metadata . models using ( model_group_id ) where model_group_id = 39 group by 1 , 2 , 3 model_group_id model_type hyperparameters models 39 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {53,89,125} We will investigate what the particular models are doing. Postmodeling created a ModelEvaluator (similar to the ModelGroupEvaluator ) to do this exploration: models_39 = { f'{model}': ModelEvaluator(39, model, engine) for model in [53,89,125] } In this tutorial, we will just show some parts of the analysis in the most recent model, but feel free of exploring the behavior of all the models in this model group, and check if you can detect any pattern. Feature importances models_39['125'].plot_feature_importances(path=params.project_path, n_features_plots=params.n_features_plots, figsize=params.figsize) Figure. Top 10 feature importances for de model group 11 at 2016-06-01 (i.e. model 125). models_39['125'].plot_feature_group_average_importances() Figure. Feature group \u201cimportance\u201d (we are basically taking the average of all the feature importances in a feature group) for the model group 39, model 125. Our Policy menu The following plot depicts the behavior of the metrics if you change the length of the facilities predicted at risk (i.e. the k k ). This plot is important from the decision making point of view, since it could be used as a policy menu . models_39['125'].plot_precision_recall_n() Figure. Plot of Precision and Recall over the proportion of the facilities. This plot is used as a \"policy menu\" since allows you to see how much you will gain if you invest more resources or how much you will sacrifice if you reduce your budget for resources. This is also known as \u201cRayid plot\u201d at DSaPP. We selected this model group because it was the best at precision at 10% (i.e. the model group consistently chose facilities will fail inspections at the top 10% of the risk). With the plot above you could decide to double your resources (maybe hiring more inspectors so you could inspect 20% of the facilities) and with this model you will double the detection of facilities that will fail inspections (from ~18% to ~30% in recall) with only a few percent points less of precision ~45% to ~40% (this means that 6 in 10 facilities that the inspectors visit will pass the inspection). You could also go the other way around: if you reduce the length of the list from 10% to 5%, well you will gain a little of precision, but your recall will be ~5%. Where to go from here # Ready to get started with your own data? Check out the suggested project workflow for some tips about how to iterate and tune the pipeline for your project. Want to work through another example? Take a look at our early warning system case study If you assume a uniform distribution, it will make sense to select facilities at random. \u21a9 The underlying assumption here is that the City of Chicago is currently doing random selection for the inspections. This is not true (and probably unfair). In a real project, you will setup a real baseline and you will compare your models against it. This baseline could be a rule or a model. \u21a9 You need to check this! Fortunately, triage allows you to try several options here, so, if you think that this is too high or too low you can change that and fit your needs. \u21a9 Think about it: we can\u2019t learn the relationship between the features and the label if we don't know the label. \u21a9 Confused? Check A deeper look at triage for more details. \u21a9 The formulas are, for precision@k , is the proportion of facilities correctly identified in the top- k k facilities ranked by risk: $$ precision@k = \\frac{TP \\in k}{k} $$ This is a measure about how efficiently are your system using your resources. recall@k , in the other hand is the proportion of all the facilities that are risk found in the top- k k $$ recall@k = \\frac{TP \\in k}{TP} $$ recall is a measure about the coverage of your system, i.e. how good is identifying in the top- k k the facilities at risk . One possible variation of this is to only include in the denominator the labeled rows in k k . This is the approach used by triage . \u21a9 We will explore how to one way to tackle this in the advance part of this tutorial. \u21a9 The flags -no-save-predictions and profile are not necessary but useful. The first one configure triage to not store the predictions (at this stage you don't need them, and you can always could recreate them from the model and the matrix). This will save you execution time. The flag profile stores the execution profile times in a file, so you can check which models or matrices are taking a lot of time on been built. \u21a9 From a more mathematical point of view: Your data actually reflects the empirical probability: P(violation|inspected) P(violation|inspected) , i.e. the probability of find a violation given that the facility is inspected. But the probability that you want is P(violation) P(violation) (yes, I know that there are no such things as unconditional probabilities, please bare with me),i.e. the probability that the facility is in violation. \u21a9 You should see that this assumption is very dangerous in other settings, for example, crime prediction. \u21a9 This assumes that you are in a GNU/Linux machine, if not (you should reconsider what are you doing with your life) you should change the ip address ( 0.0.0.0 ) and use the one from the docker virtual machine. \u21a9 For some reason, sklearn doesn\u2019t scale the inputs to the Logistic Regression, so we (DSaPP) developed a version that does that. \u21a9","title":"Resource prioritization"},{"location":"dirtyduck/inspections/#resource-prioritization-systems-chicago-food-inspections","text":"Before continue, Did you\u2026? This case study, part of the dirtyduck tutorial, assumes that you already setup the tutorial\u2019s infrastructure and load the dataset. If you didn\u2019t setup the infrastructure go here , If you didn't load the data, you can do it very quickly or you can follow all the steps and explanations about the data .","title":"Resource prioritization systems: Chicago food inspections"},{"location":"dirtyduck/inspections/#problem-description","text":"We want to generate a list of facilities that will have a critical or serious food violation if inspected. The scenario is the following: you work for the City of Chicago and you have limited food inspectors, so you try to prioritize them to focus on the highest-risk facilities. So you will use the data to answer the next question: Which X X facilities are most likely to fail a food inspection in the following Y Y period of time? A more technical way of writing the question is: What is the probability distribution of facilities that are at risk of fail a food inspection if they are inspected in the following period of time? 1 If you want to focus on major violations only, you can do that too: Which X X facilities are most likely to have a critical or serious violation in the following Y Y period of time? This situation is very common in governmental agencies that provide social services: they need to prioritize their resources and use them in the facilities that are most likely to have problems We will use machine learning to accomplish this. This means that we will use historical data to train our models, and we will use temporal cross validation to test the performance of them. For the resource prioritization problems there are commonly two problems with the data: (a) bias and (b) incompleteness. First, note that our data have bias: We only have data on facilities that were inspected. That means that our data set contains information about the probability of have a violation ( V V ) given that the facility was inspected ( I I ), P(V|I) P(V|I) . But the probability that we try to find is P(V) P(V) . A different problem that our data set could have is if our dataset contains all the facilities in Chicago, i.e. if our entities table represents the Universe of facilities. There are almost 40,000 entities in our database. We could make the case that every facility in Chicago is in the database, since every facility that opens will be subject to an inspection. We will assume that all the facilities are in our data.","title":"Problem description"},{"location":"dirtyduck/inspections/#what-do-you-want-to-predict","text":"We are interested in two different outcomes : Which facilities are likely to fail an inspection? The outcome takes a 1 if the inspection had at least one result = 'fail' and a 0 otherwise. Which facilities fail an inspection with a major violation? Critical violations are coded between 1-14 , serious violations between 15-29 , everything above 30 is assumed to be a minor violation. The label takes a 1 if the inspection had at least one result = 'fail' and a violation between 1 and 29, and a 0 otherwise. I want to learn more about the data Check the Data preparation section! Data Changes On 7/1/2018 the Chicago Department of Public Health\u2019s Food Protection unit changed the definition of violations. The changes don\u2019t affect structurally the dataset (e.g. how the violations are inputted to the database), but the redefinition will change the distribution and interpretation of the violation codes. See here . We can extract the severity of the violation using the following code: select event_id , entity_id , date , result , array_agg ( distinct obj ->> 'severity' ) as violations_severity , ( result = 'fail' ) as failed , coalesce ( ( result = 'fail' and ( 'serious' = ANY ( array_agg ( obj ->> 'severity' )) or 'critical' = ANY ( array_agg ( obj ->> 'severity' ))) ), false ) as failed_major_violation from ( select event_id , entity_id , date , result , jsonb_array_elements ( violations :: jsonb ) as obj from semantic . events limit 20 ) as t1 group by entity_id , event_id , date , result order by date desc ; event_id entity_id date result violations_severity failed failed_major_violation 1770568 30841 2016-05-11 pass {minor} f f 1763967 30841 2016-05-03 fail {critical,minor,serious} t t 1434534 21337 2014-04-03 pass {NULL} f f 1343315 22053 2013-06-06 fail {minor,serious} t t 1235707 21337 2013-03-27 pass {NULL} f f 537439 13458 2011-06-10 fail {NULL} t f 569377 5570 2011-06-01 pass {NULL} f f The outcome will be used by triage to generate the labels (once that we define a time span of interest). The following image tries to show the meaning of the outcomes for the inspection failed problem definition. Figure. The image shows three facilities (blue, red and orange) and, next to each, a temporal line with 6 days (0-5). Each dot represents an inspection. Color is the outcome of the inspection. Green means the facility passed the inspection, and red means it failed. Each facility in the image had two inspections, but only the facility in the middle passed both.","title":"What do you want to predict?"},{"location":"dirtyduck/inspections/#modeling-using-machine-learning","text":"It is time to put these steps together. All the coding is complete ( triage dev team did that for us); we just need to modify the triage experiment\u2019s configuration file.","title":"Modeling Using Machine Learning"},{"location":"dirtyduck/inspections/#defining-a-baseline","text":"As a first step, lets do an experiment that defines our baseline . The rationale of this is that the knowing the baseline will allow us to verify if our Machine Learning model is better than the baseline. The baseline in our example will be a random selection of facilities 2 . This is implemented as a DummyClassifier in scikit-learn . Another advantage of starting with the baseline, is that is very fast to train ( DummyClassifier is not computationally expensive) , so it will help us to verify that the experiment configuration is correct without waiting for a long time. Experiment description file You could check the meaning about experiment description files (or configuration files) in A deeper look into triage . We need to write the experiment config file for that. Let's break it down and explain their sections. The config file for this first experiment is located in triage/experiments/inspections_baseline.yaml . The first lines of the experiment config file specify the config-file version ( v7 at the moment of writing this tutorial), a comment ( model_comment , which will end up as a value in the model_metadata.models table), and a list of user-defined metadata ( user_metadata ) that can help to identify the resulting model groups. For this example, if you run experiments that share a temporal configuration but that use different label definitions (say, labeling inspections with any violation as positive versus only labeling inspections with major violations as positive), you can use the user metadata keys to indicate that the matrices from these experiments have different labeling criteria. The matrices from the two experiments will have different filenames (and should not be overwritten or incorrectly used), and if you add the label_definition key to the model_group_keys , models made on different label definitions will belong to different model groups. Note Obviously, change 'Your name here' for your name (if you like) Next comes the temporal configuration section. The first four parameters are related to the availability of data: How much data you have for feature creation? How much data you have for label generation? Data Changes On 7/1/2018 the Chicago Department of Public Health\u2019s Food Protection unit changed the definition of violations. The changes don\u2019t affect structurally the dataset (e.g. how the violations are inputted to the database), but the redefinition will change the distribution and interpretation of the violation codes. See here . The next parameters are related to the training intervals: How frequently to retrain models? ( model_update_frequency ) How many rows per entity in the train matrices? ( training_as_of_date_frequencies ) How much time is covered by labels in the training matrices? ( training_label_timespans ) The remaining elements are related to the testing matrices. For inspections , you can choose them as follows: test_as_of_date_frequencies is planning/scheduling frequency test_durations how far ahead do you schedule inspections? test_label_timespan is equal to test_durations Let's assume that we need to do rounds of inspections every 6 months ( test_as_of_date_frequencies = 6month ) and we need to complete that round in exactly in that time (i.e. 6 months) ( test_durations = test_label_timespan = 6month ). We will assume that the data is more or less stable 3 , at least for now, so model_update_frequency = 6month. temporal_config : feature_start_time : '2010-01-04' feature_end_time : '2018-06-01' label_start_time : '2015-01-01' label_end_time : '2018-06-01' model_update_frequency : '6month' training_label_timespans : [ '6month' ] training_as_of_date_frequencies : '6month' test_durations : '0d' test_label_timespans : [ '6month' ] test_as_of_date_frequencies : '6month' max_training_histories : '5y' We can visualize the time splitting using the function show-timechop (See A deeper look into triage for more information) # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/inspections_baseline.yaml --show-timechop Figure. Temporal blocks for the inspections baseline experiment We need to specify our labels. For this first experiment we will use the label failed , using the same query from the simple_skeleton_experiment.yaml label_config : query : | select entity_id, bool_or(result = 'fail')::integer as outcome from semantic.events where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id name : 'failed_inspections' It should be obvious, but let's state it anyway: We are only training in facilities that were inspected, but we will test our model in all the facilities in our cohort 4 . So, in the train matrices we will have only 0 and 1 as possible labels, but in the test matrices we will found 0 , 1 and NULL . I want to learn more about this\u2026 In the section regarding to Early Warning Systems we will learn how to incorporate all the facilities of the cohort in the train matrices. We just want to include active facilities in our matrices, so we tell triage to take that in account: cohort_config : query : | select e.entity_id from semantic.entities as e where daterange(start_time, end_time, '[]') @> '{as_of_date}'::date name : 'active_facilities' Triage will generate the features for us, but we need to tell it which features we want in the section feature_aggregations . Here, each entry describes a collate.SpacetimeAggregation object and the arguments needed to create it 5 . For this experiment, we will use only one feature (number of inspections). DummyClassifier don't use any feature to do the \"prediction\", so we won't expend compute cycles doing the feature/matrix creation: feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates_imputation : count : type : 'zero_noflag' aggregates : - quantity : total : \"*\" metrics : - 'count' intervals : [ 'all' ] groups : - 'entity_id' feature_group_definition : prefix : - 'inspections' feature_group_strategies : [ 'all' ] If we observe the image generated from the temporal_config section, each particular date is the beginning of the rectangles that describes the rows in the matrix. In that date ( as_of_date in timechop parlance) we will calculate the feature, and we will repeat that for every other rectangle in that image. Now, let's discuss how we will specify the models to try (remember that the model is specified by the algorithm, the hyperparameters, and the subset of features to use). In triage you need to specify in the grid_config section a list of machine learning algorithms that you want to train and a list of hyperparameters. You can use any algorithm that you want; the only requirement is that it respects the sklearn API. grid_config : 'sklearn.dummy.DummyClassifier' : strategy : [ uniform ] Finally, we should define wich metrics we care about for evaluating our model. Here we will concentrate only in precision and recall at an specific value k k 6 . In this setting k k represents the resource\u2019s constraint: It is the number of inspections that the city could do in a month given all the inspectors available. scoring : testing_metric_groups : - metrics : [ precision@ , recall@ , 'false negatives@' , 'false positives@' , 'true positives@' , 'true negatives@' ] thresholds : percentiles : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 95 , 100 ] top_n : [ 1 , 5 , 10 , 25 , 50 , 100 , 250 , 500 , 1000 ] - metrics : [ auc , accuracy ] training_metric_groups : - metrics : [ auc , accuracy ] - metrics : [ precision@ , recall@ ] thresholds : percentiles : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 95 , 100 ] top_n : [ 1 , 5 , 10 , 25 , 50 , 100 , 250 , 500 , 1000 ] You should be warned that precision and recall at k k in this setting is kind of ill-defined (because you will end with a lot of NULL labels, remember, only a few of facilities are inspected in each period) 7 . We will want a list of facilities to be inspected. The length of our list is constrained by our inspection resources, i.e. the answer to the question How many facilities can I inspect in a month? In this experiment we are assuming that the maximum capacity is 10% but we are evaluating for a larger space of possibilities (see top_n , percentiles above). The execution of the experiments can take a long time, so it is a good practice to validate the configuration file before running the model. You don't want to wait for hours (or days) and then discover that something went wrong. # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/inspections_baseline.yaml --validate-only If everything was ok, you should see an Experiment validation ran to completion with no errors . You can execute the experiment as 8 # Remember to run this in bastion NOT in your laptop shell! time triage experiment experiments/inspections_baseline.yaml Protip We are including the command time in order to get the total running time of the experiment. You can remove it, if you like. Don\u2019t be scared! This will print a lot of output! It is not an error! We can query the table experiments to see the quantity of work that triage needs to do select substring ( experiment_hash , 1 , 4 ) as experiment , config -> 'user_metadata' ->> 'description' as description , total_features , matrices_needed , models_needed from model_metadata . experiments ; experiment description total_features matrices_needed models_needed e912 \"Baseline calculation\\n\" 1 10 5 If everything is correct, triage will create 10 matrices (5 for training, 5 for testing) in triage/matrices and every matrix will be represented by two files, one with the metadata of the matrix (a yaml file) and one with the actual matrix (the gz file). # We will use some bash magic ls matrices | awk -F . '{print $NF}' | sort | uniq -c Triage also will store 5 trained models in triage/trained_models : ls trained_models | wc -l And it will populate the results schema in the database. As mentioned, we will get 1 model groups : select model_group_id , model_type , hyperparameters from model_metadata . model_groups where model_config ->> 'experiment_type' ~ 'inspection' model_group_id model_type hyperparameters 1 sklearn.dummy.DummyClassifier {\"strategy\": \"prior\"} And 5 models : select model_group_id , array_agg ( model_id ) as models , array_agg ( train_end_time :: date ) as train_end_times from model_metadata . models where model_comment ~ 'inspection' group by model_group_id order by model_group_id ; model_group_id models train_end_times 1 {1,2,3,4,5} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} From that last query, you should note that the order in which triage trains the models is from oldest to newest train_end_time and model_group , also in ascending order. It will not go to the next block until all the models groups are trained. You can check on which matrix each models were trained: select model_group_id , model_id , train_end_time :: date , substring ( model_hash , 1 , 5 ) as model_hash , substring ( train_matrix_uuid , 1 , 5 ) as train_matrix_uuid , ma . num_observations as observations , ma . lookback_duration as feature_lookback_duration , ma . feature_start_time from model_metadata . models as mo join model_metadata . matrices as ma on train_matrix_uuid = matrix_uuid where mo . model_comment ~ 'inspection' order by model_group_id , train_end_time asc ; model_group_id model_id train_end_time model_hash train_matrix_uuid observations feature_lookback_duration feature_start_time 1 1 2015-12-01 743a6 1c266 5790 5 years 2010-01-04 00:00:00 1 2 2016-06-01 5b656 755c6 11502 5 years 2010-01-04 00:00:00 1 3 2016-12-01 5c170 2aea8 17832 5 years 2010-01-04 00:00:00 1 4 2017-06-01 55c3e 3efdc 23503 5 years 2010-01-04 00:00:00 1 5 2017-12-01 ac993 a3762 29112 5 years 2010-01-04 00:00:00 Each model was trained with the matrix indicated in the column train_matrix_uuid . This uuid is the file name of the stored matrix. The model itself was stored under the file named with the model_hash . If you want to see in which matrix the model was tested you need to run the following query select distinct model_id , model_group_id , train_end_time :: date , substring ( model_hash , 1 , 5 ) as model_hash , substring ( ev . matrix_uuid , 1 , 5 ) as test_matrix_uuid , ma . num_observations as observations from model_metadata . models as mo join test_results . evaluations as ev using ( model_id ) join model_metadata . matrices as ma on ev . matrix_uuid = ma . matrix_uuid where mo . model_comment ~ 'inspection' order by model_group_id , train_end_time asc ; model_id model_group_id train_end_time model_hash test_matrix_uuid observations 1 1 2015-12-01 743a6 4a0ea 18719 2 1 2016-06-01 5b656 f908e 19117 3 1 2016-12-01 5c170 00a88 19354 4 1 2017-06-01 55c3e 8f3cf 19796 5 1 2017-12-01 ac993 417f0 20159 All the models were stored in /triage/trained_models/{model_hash} using the standard serialization of sklearn models. Every model was trained with the matrix train_matrix_uuid stored in the directory /triage/matrices . What's the performance of this model groups? \\ set k 0 . 10 -- This defines a variable, \"k = 0.10\" select distinct model_group_id , model_id , ma . feature_start_time :: date , train_end_time :: date , ev . evaluation_start_time :: date , ev . evaluation_end_time :: date , to_char ( ma . num_observations , '999,999' ) as observations , to_char ( ev . num_labeled_examples , '999,999' ) as \"total labeled examples\" , to_char ( ev . num_positive_labels , '999,999' ) as \"total positive labels\" , to_char ( ev . num_labeled_above_threshold , '999,999' ) as \"labeled examples@k%\" , to_char (: k * ma . num_observations , '999,999' ) as \"predicted positive (PP)\" , ARRAY [ to_char ( ev . best_value * ev . num_labeled_above_threshold , '999,999' ), to_char ( ev . worst_value * ev . num_labeled_above_threshold , '999,999' ), to_char ( ev . stochastic_value * ev . num_labeled_above_threshold , '999,999' )] as \"true positive (TP)@k% (best,worst,stochastic)\" , ARRAY [ to_char ( ev . best_value , '0.999' ), to_char ( ev . worst_value , '0.999' ), to_char ( ev . stochastic_value , '0.999' )] as \"precision@k% (best,worst,stochastic)\" , to_char ( ev . num_positive_labels * 1 . 0 / ev . num_labeled_examples , '0.999' ) as baserate , : k * 100 as \"k%\" from model_metadata . models as mo join test_results . evaluations as ev using ( model_id ) join model_metadata . matrices as ma on ev . matrix_uuid = ma . matrix_uuid where ev . metric || ev . parameter = 'precision@15_pct' and mo . model_comment ~ 'inspection' order by model_id , train_end_time asc ; order by model_id , train_end_time asc ; model_group_id model_id feature_start_time train_end_time evaluation_start_time evaluation_end_time observations total labeled examples total positive labels labeled examples@k% predicted positive (PP) true positive (TP)@k% (best,worst,stochastic) precision@k% (best,worst,stochastic) baserate k% 21 121 2010-01-04 2015-12-01 2015-12-01 2015-12-01 18,719 5,712 1,509 0 2,808 {\" 0\",\" 0\",\" 0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.264 15.00 21 122 2010-01-04 2016-06-01 2016-06-01 2016-06-01 19,117 6,330 1,742 0 2,868 {\" 0\",\" 0\",\" 0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.275 15.00 21 123 2010-01-04 2016-12-01 2016-12-01 2016-12-01 19,354 5,671 1,494 0 2,903 {\" 0\",\" 0\",\" 0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.263 15.00 21 124 2010-01-04 2017-06-01 2017-06-01 2017-06-01 19,796 5,609 1,474 0 2,969 {\" 0\",\" 0\",\" 0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.263 15.00 21 125 2010-01-04 2017-12-01 2017-12-01 2017-12-01 20,159 4,729 1,260 0 3,024 {\" 0\",\" 0\",\" 0\"} {\" 0.000\",\" 0.000\",\" 0.000\"} 0.266 15.00 The columns num_labeled_examples, num_labeled_above_threshold, num_positive_labels represent the number of selected entities on the prediction date that are labeled, the number of entities with a positive label above the threshold, and the number of entities with positive labels among all the labeled entities respectively. We added some extra columns: baserate , predicted positive (PP) and true positive (TP) . Baserate represents the proportion of the all the facilities that were inspected that failed the inspection, i.e. P(V|I) P(V|I) . The PP and TP are approximate since it were calculated using the value of k k or the precision value. But you could get the exact value of those from the test_results.predictions table. Also note that in the precision@k% column we are showing three numbers: best , worst , stochastic . They try to answer the question How do you break ties in the prediction score? This is important because it will affect the calculation of your metrics. The Triage proposed solution to this is calculate the metric in the best case scenario (score descending, all the true labels are at the top), and then do it in the worst case scenario (score descending, all the true labels are at the bottom) and then calculate the metric several times ( n=30 n=30 ) with the labels randomly shuffled (a.k.a. stochastic scenario ), so you get the mean metric, plus some confidence intervals. This problem is not specific of an inspection problem, is more related to simple models like a shallow Decision Tree or a Dummy Classifier when score ties likely will occur. Note how in this model, the stochastic value is close to the baserate , since we are selecting at random using the prior . Check this! Note that the baserate should be equal to the precision@100% , if is not there is something wrong \u2026","title":"Defining a baseline"},{"location":"dirtyduck/inspections/#creating-a-simple-experiment","text":"We will try two of the simplest machine learning algorithms: a Decision Tree Classifier ( DT ) and a Scaled Logistic Regression ( SLR ) 12 as a second experiment. The rationale of this is that the DT is very fast to train (so it will help us to verify that the experiment configuration is correct without waiting for a long time) and it helps you to understand the structure of your data. The config file for this first experiment is located in /triage/experiments/inspections_dt.yaml Note that we don't modify the temporal_config section neither the feature_aggregations , cohort_config or label_config . Triage is smart enough to use the previous tables and matrices instead of generating them from scratch. config_version : 'v7' model_comment : 'inspections: basic ML' user_metadata : label_definition : 'failed' experiment_type : 'inspections prioritization' file_name : 'inspections_dt.yaml' description : | DT and SLR purpose : 'data mining' org : 'DSaPP' team : 'Tutorial' author : 'Your name here' etl_date : '2019-02-21' Note that we don't modify the temporal_config section neither the cohort_config or label_config . Triage is smart enough to use the previous tables and matrices instead of generating them from scratch. For this experiment, we will add the following features: Number of different types of inspections the facility had in the last year (calculated for an as-of-date ). Number of different types of inspections that happened in the zip code in the last year from a particular day. Number of inspections Number/proportion of inspections by result type Number/proportion of times that a facility was classify with particular risk level In all of them we will do the aggregation in the last month, 3 months, 6 months, 1 year and historically. Remember that all this refers to events in the past, i.e. How many times the facility was marked with high risk in the previous 3 Months? , What is the proportion of failed inspections in the previous year? feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates_imputation : count : type : 'zero_noflag' aggregates : - quantity : total : \"*\" metrics : - 'count' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'risks' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero' avg : type : 'zero' categoricals : - column : 'risk' choices : [ 'low' , 'medium' , 'high' ] metrics : - 'sum' - 'avg' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - 'zip_code' - prefix : 'results' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : all : type : 'zero' categoricals : - column : 'result' choice_query : 'select distinct result from semantic.events' metrics : - 'sum' - 'avg' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'inspection_types' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero_noflag' categoricals : - column : 'type' choice_query : 'select distinct type from semantic.events where type is not null' metrics : - 'sum' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - 'zip_code' And as stated, we will train some Decision Trees, in particular we are interested in some shallow trees, and in a full grown tree. These trees will show you the structure of your data. We also will train some Scaled Logistic Regression, this will show us how \"linear\" is the data (or how the assumptions of the Logistic Regression holds in this data) grid_config : 'sklearn.tree.DecisionTreeClassifier' : criterion : [ 'gini' ] max_features : [ 'sqrt' ] max_depth : [ 1 , 2 , 5 , ~ ] min_samples_split : [ 2 , 10 , 50 ] 'triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression' : penalty : [ 'l1' , 'l2' ] C : [ 0.000001 , 0.0001 , 0.01 , 1.0 ] About yaml and sklearn Some of the parameters in sklearn are None . If you want to try those you need to indicate it with yaml 's null or ~ keyword. Besides the algorithm and the hyperparameters, you should specify which subset of features use. First, in the section feature_group_definition you specify how to group the features (you can use the table name or the prefix from the section feature_aggregation ) and then a strategy for choosing the subsets: all (all the subsets at once), leave-one-out (try all the subsets except one, do that for all the combinations), or leave-one-in (just try subset at the time). feature_group_definition : prefix : - 'inspections' - 'results' - 'risks' - 'inspection_types' feature_group_strategies : [ 'all' ] Finally we will leave the scoring section as before. In this experiment we will end with 6 model groups (number of algorithms [1] \\times \\times number of hyperparameter combinations [2 \\times \\times 3 = 5] \\times \\times number of feature groups strategies [1]]). Also, we will create 18 models (3 per model group) given that we have 3 temporal blocks (one model per temporal group). Before running the experiment, remember to validate that the configuration is correct: # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/inspections_dt.yaml --validate-only and check the temporal cross validation: # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/inspections_dt.yaml --show-timechop Temporal blocks for inspections experiment. The label is a failed inspection in the next year. You can execute the experiment like this: # Remember to run this in bastion NOT in your laptop shell! time triage experiment experiments/inspections_dt.yaml Again, we can run the following sql to see which things triage needs to run: select substring ( experiment_hash , 1 , 4 ) as experiment , config -> 'user_metadata' ->> 'description' as description , total_features , matrices_needed , models_needed from model_metadata . experiments ; experiment description total_features matrices_needed models_needed e912 Baseline calculation 1 10 5 b535 DT and SLR 201 10 100 You can compare our two experiments and there are several differences, mainly in the order of magnitude. Like the number of features (1 vs 201) and models built (5 vs 100). The After the experiment finishes, you will get 19 new model_groups (1 per combination in grid_config ) select model_group_id , model_type , hyperparameters from model_metadata . model_groups where model_group_id not in ( 1 ); model_group_id model_type hyperparameters 2 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 1, \"max_features\": \"sqrt\", \"min_samples_split\": 2} 3 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 1, \"max_features\": \"sqrt\", \"min_samples_split\": 10} 4 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 1, \"max_features\": \"sqrt\", \"min_samples_split\": 50} 5 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"min_samples_split\": 2} 6 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"min_samples_split\": 10} 7 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"min_samples_split\": 50} 8 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 5, \"max_features\": \"sqrt\", \"min_samples_split\": 2} 9 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 5, \"max_features\": \"sqrt\", \"min_samples_split\": 10} 10 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": 5, \"max_features\": \"sqrt\", \"min_samples_split\": 50} 11 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"sqrt\", \"min_samples_split\": 2} 12 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"sqrt\", \"min_samples_split\": 10} 13 sklearn.tree.DecisionTreeClassifier {\"criterion\": \"gini\", \"max_depth\": null, \"max_features\": \"sqrt\", \"min_samples_split\": 50} 14 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.000001, \"penalty\": \"l1\"} 15 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.000001, \"penalty\": \"l2\"} 16 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.0001, \"penalty\": \"l1\"} 17 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.0001, \"penalty\": \"l2\"} 18 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.01, \"penalty\": \"l1\"} 19 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 0.01, \"penalty\": \"l2\"} 20 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 1.0, \"penalty\": \"l1\"} 21 triage.component.catwalk.estimators.classifiers.ScaledLogisticRegression {\"C\": 1.0, \"penalty\": \"l2\"} and 100 models (as stated before) select model_group_id , array_agg ( model_id ) as models , array_agg ( train_end_time ) as train_end_times from model_metadata . models where model_group_id not in ( 1 ) group by model_group_id order by model_group_id ; model_group_id models train_end_times 2 {6,26,46,66,86} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 3 {7,27,47,67,87} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 4 {8,28,48,68,88} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 5 {9,29,49,69,89} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 6 {10,30,50,70,90} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 7 {11,31,51,71,91} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 8 {12,32,52,72,92} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 9 {13,33,53,73,93} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 10 {14,34,54,74,94} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 11 {15,35,55,75,95} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 12 {16,36,56,76,96} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 13 {17,37,57,77,97} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 14 {18,38,58,78,98} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 15 {19,39,59,79,99} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 16 {20,40,60,80,100} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 17 {21,41,61,81,101} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 18 {22,42,62,82,102} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 19 {23,43,63,83,103} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 20 {24,44,64,84,104} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} 21 {25,45,65,85,105} {\"2015-12-01 00:00:00\",\"2016-06-01 00:00:00\",\"2016-12-01 00:00:00\",\"2017-06-01 00:00:00\",\"2017-12-01 00:00:00\"} Let's see the performance over time of the models so far: select model_group_id , array_agg ( model_id order by ev . evaluation_start_time asc ) as models , array_agg ( ev . evaluation_start_time :: date order by ev . evaluation_start_time asc ) as evaluation_start_time , array_agg ( ev . evaluation_end_time :: date order by ev . evaluation_start_time asc ) as evaluation_end_time , array_agg ( to_char ( ev . num_labeled_examples , '999,999' ) order by ev . evaluation_start_time asc ) as labeled_examples , array_agg ( to_char ( ev . num_labeled_above_threshold , '999,999' ) order by ev . evaluation_start_time asc ) as labeled_above_threshold , array_agg ( to_char ( ev . num_positive_labels , '999,999' ) order by ev . evaluation_start_time asc ) as total_positive_labels , array_agg ( to_char ( ev . stochastic_value , '0.999' ) order by ev . evaluation_start_time asc ) as \"precision@15%\" from model_metadata . models as mo inner join model_metadata . model_groups as mg using ( model_group_id ) inner join test_results . evaluations as ev using ( model_id ) where mg . model_config ->> 'experiment_type' ~ 'inspection' and ev . metric || ev . parameter = 'precision@15_pct' and model_group_id between 2 and 21 group by model_group_id model_group_id models evaluation_start_time evaluation_end_time labeled_examples labeled_above_threshold total_positive_labels precision@15% 1 {1,2,3,4,5} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 0\",\" 0\",\" 0\",\" 0\",\" 0\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\"} 2 {6,26,46,66,86} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 0\",\" 578\",\" 730\",\" 574\",\" 0\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.000\",\" 0.352\",\" 0.316\",\" 0.315\",\" 0.000\"} 3 {7,27,47,67,87} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 161\",\" 622\",\" 0\",\" 433\",\" 0\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.283\",\" 0.203\",\" 0.000\",\" 0.282\",\" 0.000\"} 4 {8,28,48,68,88} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 161\",\" 1,171\",\" 0\",\" 0\",\" 995\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.285\",\" 0.348\",\" 0.000\",\" 0.000\",\" 0.306\"} 5 {9,29,49,69,89} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 726\",\" 1,318\",\" 362\",\" 489\",\" 291\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.306\",\" 0.360\",\" 0.213\",\" 0.294\",\" 0.241\"} 6 {10,30,50,70,90} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 992\",\" 730\",\" 0\",\" 176\",\" 290\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.294\",\" 0.349\",\" 0.000\",\" 0.324\",\" 0.334\"} 7 {11,31,51,71,91} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 1,023\",\" 325\",\" 329\",\" 176\",\" 1,033\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.323\",\" 0.400\",\" 0.347\",\" 0.323\",\" 0.315\"} 8 {12,32,52,72,92} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 1,250\",\" 1,013\",\" 737\",\" 1,104\",\" 939\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.331\",\" 0.280\",\" 0.327\",\" 0.335\",\" 0.365\"} 9 {13,33,53,73,93} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 595\",\" 649\",\" 547\",\" 1,000\",\" 841\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.281\",\" 0.250\",\" 0.309\",\" 0.350\",\" 0.356\"} 10 {14,34,54,74,94} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 1,012\",\" 887\",\" 856\",\" 387\",\" 851\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.345\",\" 0.339\",\" 0.342\",\" 0.248\",\" 0.327\"} 11 {15,35,55,75,95} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 0\",\" 0\",\" 0\",\" 0\",\" 0\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\"} 12 {16,36,56,76,96} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 1,094\",\" 1,033\",\" 878\",\" 880\",\" 842\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.278\",\" 0.332\",\" 0.285\",\" 0.311\",\" 0.299\"} 13 {17,37,57,77,97} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 995\",\" 1,117\",\" 987\",\" 623\",\" 651\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.313\",\" 0.324\",\" 0.320\",\" 0.318\",\" 0.299\"} 14 {18,38,58,78,98} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 0\",\" 0\",\" 0\",\" 0\",\" 0\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\"} 15 {19,39,59,79,99} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 771\",\" 582\",\" 845\",\" 570\",\" 625\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.246\",\" 0.227\",\" 0.243\",\" 0.244\",\" 0.232\"} 16 {20,40,60,80,100} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 0\",\" 0\",\" 0\",\" 0\",\" 0\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\",\" 0.000\"} 17 {21,41,61,81,101} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 783\",\" 587\",\" 813\",\" 586\",\" 570\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.250\",\" 0.235\",\" 0.253\",\" 0.259\",\" 0.253\"} 18 {22,42,62,82,102} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 551\",\" 649\",\" 588\",\" 552\",\" 444\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.310\",\" 0.336\",\" 0.355\",\" 0.330\",\" 0.372\"} 19 {23,43,63,83,103} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 1,007\",\" 776\",\" 818\",\" 784\",\" 725\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.343\",\" 0.409\",\" 0.373\",\" 0.366\",\" 0.421\"} 20 {24,44,64,84,104} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 797\",\" 971\",\" 887\",\" 770\",\" 745\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.311\",\" 0.355\",\" 0.347\",\" 0.395\",\" 0.431\"} 21 {25,45,65,85,105} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {2015-12-01,2016-06-01,2016-12-01,2017-06-01,2017-12-01} {\" 5,712\",\" 6,330\",\" 5,671\",\" 5,609\",\" 4,729\"} {\" 943\",\" 953\",\" 837\",\" 730\",\" 699\"} {\" 1,509\",\" 1,742\",\" 1,494\",\" 1,474\",\" 1,260\"} {\" 0.320\",\" 0.363\",\" 0.349\",\" 0.397\",\" 0.429\"} Which model in production ( model selection ) is something that we will review later, with Audition , but for now, let's choose the model group 3 and see the predictions table: select model_id , entity_id , as_of_date :: date , round ( score , 2 ), label_value as label from test_results . predictions where model_id = 11 order by as_of_date asc , score desc limit 20 model_id entity_id as_of_date round label 11 26873 2015-06-01 0.49 11 26186 2015-06-01 0.49 11 25885 2015-06-01 0.49 11 24831 2015-06-01 0.49 11 24688 2015-06-01 0.49 11 21485 2015-06-01 0.49 11 20644 2015-06-01 0.49 11 20528 2015-06-01 0.49 11 19531 2015-06-01 0.49 11 18279 2015-06-01 0.49 11 17853 2015-06-01 0.49 11 17642 2015-06-01 0.49 11 16360 2015-06-01 0.49 11 15899 2015-06-01 0.49 11 15764 2015-06-01 0.49 11 15381 2015-06-01 0.49 11 15303 2015-06-01 0.49 11 14296 2015-06-01 0.49 11 14016 2015-06-01 0.49 11 27627 2015-06-01 0.49 NOTE: Given that this is a shallow tree, there will be a lot of entities with the same score ,you probably will get a different set of entities, since postgresql will sort them at random. It is important to know\u2026 Triage sorted the predictions at random using the random_seed from the experiment\u2019s config file. If you want the predictions being sorted in a different way add prediction: randk_tiebreaker: \"worst\" # or \"best\" or \"random\" Note that at the top of the list (sorted by as_of_date , and then by score ), the labels are NULL . This means that the facilities that you are classifying as high risk, actually weren't inspected in that as of date . So, you actually don't know if this is a correct prediction or not. This is a characteristic of all the resource optimization problems: You do not have all the information about the elements in your system 9 . So, how the precision/recall is calculated? The number that is show in the evaluations table is calculated using only the rows that have a non-null label. You could argue that this is fine, if you assume that the distribution of the label in the non-observed facilities is the same that the ones that were inspected that month 10 . We will come back to this problem in the Early Warning Systems.","title":"Creating a simple experiment"},{"location":"dirtyduck/inspections/#a-more-advanced-experiment","text":"Ok, let's add a more complete experiment. First the usual generalities. config_version : 'v7' model_comment : 'inspections: advanced' user_metadata : label_definition : 'failed' experiment_type : 'inspections prioritization' description : | Using Ensamble methods purpose : 'trying ensamble algorithms' org : 'DSaPP' team : 'Tutorial' author : 'Your name here' etl_date : '2019-02-21' We won't change anything related to features, cohort and label definition neither to temporal configuration. As before, we can check the temporal structure of our crossvalidation: # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/inspections_label_failed_01.yaml --show-timechop Figure. Temporal blocks for inspections experiment. The label is a failed inspection in the next month. We want to use all the features groups ( feature_group_definition ). The training will be made on matrices with all the feature groups, then leaving one feature group out at a time, leave-one-out (i.e. one model with inspections and results , another with inspections and risks , and another with results and risks), and finally leaving one feature group in at a time (i.e. a model with inspections only, another with results only, and a third with risks` only). feature_group_definition : prefix : - 'inspections' - 'results' - 'risks' - 'inspection_types' feature_group_strategies : [ 'all' , 'leave-one-in' , 'leave-one-out' ] Finally, we will try some RandomForestClassifier : grid_config : 'sklearn.ensemble.RandomForestClassifier' : n_estimators : [ 10000 ] criterion : [ 'gini' ] max_depth : [ 2 , 5 , 10 ] max_features : [ 'sqrt' ] min_samples_split : [ 2 , 10 , 50 ] n_jobs : [ -1 ] 'sklearn.ensemble.ExtraTreesClassifier' : n_estimators : [ 10000 ] criterion : [ 'gini' ] max_depth : [ 2 , 5 , 10 ] max_features : [ 'sqrt' ] min_samples_split : [ 2 , 10 , 50 ] n_jobs : [ -1 ] scoring : testing_metric_groups : - metrics : [ precision@ , recall@ ] thresholds : percentiles : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 95 , 100 ] top_n : [ 1 , 5 , 10 , 25 , 50 , 100 , 250 , 500 , 1000 ] training_metric_groups : - metrics : [ accuracy ] - metrics : [ precision@ , recall@ ] thresholds : percentiles : [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 95 , 100 ] top_n : [ 1 , 5 , 10 , 25 , 50 , 100 , 250 , 500 , 1000 ] Before running, let's verify the configuration file # Remember to run this in bastion NOT in your laptop shell! triage experiment experiments/inspections_label_failed_01.yaml --validate-only You can execute the experiment with # Remember to run this in bastion NOT in your laptop shell! time triage experiment experiments/inspections_label_failed_01.yaml This will take a looooong time to run. The reason for that is easy to understand: We are computing a lot of models: 6 time splits, 18 model groups and 9 features sets (one for all , four for leave_one_in and four for leave_one_out ), so 6 \\times 18 \\times 9 = 486 6 \\times 18 \\times 9 = 486 extra models. Well, now we have a lot of models. How can you pick the best one? You could try the following query: with features_groups as ( select model_group_id , split_part ( unnest ( feature_list ), '_' , 1 ) as feature_groups from model_metadata . model_groups ), features_arrays as ( select model_group_id , array_agg ( distinct feature_groups ) as feature_groups from features_groups group by model_group_id ) select model_group_id , model_type , hyperparameters , feature_groups , array_agg ( to_char ( stochastic_value , '0.999' ) order by train_end_time asc ) filter ( where metric = 'precision@' ) as \"precision@15%\" , array_agg ( to_char ( stochastic_value , '0.999' ) order by train_end_time asc ) filter ( where metric = 'recall@' ) as \"recall@15%\" from model_metadata . models join features_arrays using ( model_group_id ) join test_results . evaluations using ( model_id ) where model_comment ~ 'inspection' and parameter = '15_pct' group by model_group_id , model_type , hyperparameters , feature_groups order by model_group_id ; This is a long table \u2026 model_group_id model_type hyperparameters feature_groups precision@15% recall@15% 1 sklearn.dummy.DummyClassifier {\"strategy\": \"prior\"} {inspections} {\" 0.339\",\" 0.366\",\" 0.378\"} {\" 0.153\",\" 0.151\",\" 0.149\"} 2 sklearn.tree.DecisionTreeClassifier {\"max_depth\": 2, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.347\",\" 0.394\",\" 0.466\"} {\" 0.155\",\" 0.153\",\" 0.180\"} 3 sklearn.tree.DecisionTreeClassifier {\"max_depth\": 2, \"min_samples_split\": 5} {inspection,inspections,results,risks} {\" 0.349\",\" 0.397\",\" 0.468\"} {\" 0.156\",\" 0.154\",\" 0.181\"} 4 sklearn.tree.DecisionTreeClassifier {\"max_depth\": 10, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.409\",\" 0.407\",\" 0.470\"} {\" 0.179\",\" 0.163\",\" 0.178\"} 5 sklearn.tree.DecisionTreeClassifier {\"max_depth\": 10, \"min_samples_split\": 5} {inspection,inspections,results,risks} {\" 0.416\",\" 0.409\",\" 0.454\"} {\" 0.183\",\" 0.160\",\" 0.169\"} 6 sklearn.tree.DecisionTreeClassifier {\"max_depth\": null, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.368\",\" 0.394\",\" 0.413\"} {\" 0.165\",\" 0.161\",\" 0.160\"} 7 sklearn.tree.DecisionTreeClassifier {\"max_depth\": null, \"min_samples_split\": 5} {inspection,inspections,results,risks} {\" 0.386\",\" 0.397\",\" 0.417\"} {\" 0.171\",\" 0.161\",\" 0.162\"} 8 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.441\",\" 0.471\",\" 0.513\"} {\" 0.190\",\" 0.187\",\" 0.193\"} 9 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection,inspections,results,risks} {\" 0.470\",\" 0.478\",\" 0.532\"} {\" 0.200\",\" 0.189\",\" 0.200\"} 10 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection,inspections,results,risks} {\" 0.481\",\" 0.479\",\" 0.513\"} {\" 0.204\",\" 0.189\",\" 0.193\"} 11 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection,inspections,results,risks} {\" 0.474\",\" 0.472\",\" 0.535\"} {\" 0.202\",\" 0.183\",\" 0.199\"} 12 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspections} {\" 0.428\",\" 0.417\",\" 0.389\"} {\" 0.179\",\" 0.149\",\" 0.148\"} 13 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspections} {\" 0.428\",\" 0.417\",\" 0.390\"} {\" 0.180\",\" 0.149\",\" 0.148\"} 14 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspections} {\" 0.427\",\" 0.417\",\" 0.376\"} {\" 0.179\",\" 0.149\",\" 0.140\"} 15 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspections} {\" 0.428\",\" 0.417\",\" 0.380\"} {\" 0.179\",\" 0.149\",\" 0.143\"} 16 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {results} {\" 0.415\",\" 0.398\",\" 0.407\"} {\" 0.180\",\" 0.157\",\" 0.157\"} 17 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {results} {\" 0.393\",\" 0.401\",\" 0.404\"} {\" 0.171\",\" 0.158\",\" 0.155\"} 18 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {results} {\" 0.436\",\" 0.425\",\" 0.447\"} {\" 0.191\",\" 0.169\",\" 0.171\"} 19 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {results} {\" 0.432\",\" 0.423\",\" 0.438\"} {\" 0.188\",\" 0.168\",\" 0.167\"} 20 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {risks} {\" 0.413\",\" 0.409\",\" 0.431\"} {\" 0.184\",\" 0.170\",\" 0.166\"} 21 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {risks} {\" 0.407\",\" 0.391\",\" 0.459\"} {\" 0.180\",\" 0.159\",\" 0.179\"} 22 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {risks} {\" 0.418\",\" 0.432\",\" 0.469\"} {\" 0.184\",\" 0.176\",\" 0.181\"} 23 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {risks} {\" 0.427\",\" 0.431\",\" 0.476\"} {\" 0.187\",\" 0.176\",\" 0.183\"} 24 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection} {\" 0.435\",\" 0.483\",\" 0.483\"} {\" 0.193\",\" 0.194\",\" 0.186\"} 25 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection} {\" 0.448\",\" 0.465\",\" 0.518\"} {\" 0.196\",\" 0.188\",\" 0.202\"} 26 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection} {\" 0.446\",\" 0.446\",\" 0.508\"} {\" 0.189\",\" 0.179\",\" 0.193\"} 27 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection} {\" 0.459\",\" 0.444\",\" 0.513\"} {\" 0.198\",\" 0.176\",\" 0.198\"} 28 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection,results,risks} {\" 0.472\",\" 0.479\",\" 0.506\"} {\" 0.202\",\" 0.191\",\" 0.190\"} 29 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection,results,risks} {\" 0.476\",\" 0.486\",\" 0.532\"} {\" 0.202\",\" 0.191\",\" 0.199\"} 30 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection,results,risks} {\" 0.485\",\" 0.454\",\" 0.535\"} {\" 0.203\",\" 0.180\",\" 0.204\"} 31 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection,results,risks} {\" 0.479\",\" 0.497\",\" 0.521\"} {\" 0.205\",\" 0.193\",\" 0.196\"} 32 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection,inspections,risks} {\" 0.437\",\" 0.432\",\" 0.474\"} {\" 0.191\",\" 0.178\",\" 0.181\"} 33 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection,inspections,risks} {\" 0.459\",\" 0.468\",\" 0.501\"} {\" 0.202\",\" 0.191\",\" 0.197\"} 34 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection,inspections,risks} {\" 0.461\",\" 0.448\",\" 0.482\"} {\" 0.201\",\" 0.181\",\" 0.187\"} 35 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection,inspections,risks} {\" 0.463\",\" 0.445\",\" 0.497\"} {\" 0.200\",\" 0.180\",\" 0.189\"} 36 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspection,inspections,results} {\" 0.462\",\" 0.448\",\" 0.513\"} {\" 0.199\",\" 0.177\",\" 0.191\"} 37 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspection,inspections,results} {\" 0.465\",\" 0.491\",\" 0.537\"} {\" 0.197\",\" 0.190\",\" 0.203\"} 38 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspection,inspections,results} {\" 0.459\",\" 0.481\",\" 0.522\"} {\" 0.193\",\" 0.187\",\" 0.198\"} 39 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspection,inspections,results} {\" 0.474\",\" 0.479\",\" 0.536\"} {\" 0.203\",\" 0.188\",\" 0.201\"} 40 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 2} {inspections,results,risks} {\" 0.436\",\" 0.429\",\" 0.490\"} {\" 0.189\",\" 0.174\",\" 0.185\"} 41 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 2} {inspections,results,risks} {\" 0.441\",\" 0.448\",\" 0.515\"} {\" 0.190\",\" 0.180\",\" 0.194\"} 42 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 100, \"min_samples_split\": 10} {inspections,results,risks} {\" 0.460\",\" 0.475\",\" 0.481\"} {\" 0.198\",\" 0.189\",\" 0.178\"} 43 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {inspections,results,risks} {\" 0.465\",\" 0.446\",\" 0.496\"} {\" 0.199\",\" 0.179\",\" 0.187\"} This table summarizes all our experiments, but it is very difficult to use if you want to choose the best combination of hyperparameters and algorithm (i.e. the model group). In the next section will solve this dilemma with the support of audition .","title":"A more advanced experiment"},{"location":"dirtyduck/inspections/#selecting-the-best-model","text":"43 model groups ! How to pick the best one and use it for making predictions with new data? What do you mean by \u201cthe best\u201d? This is not as easy as it sounds, due to several factors: You can try to pick the best using a metric specified in the config file ( precision@ and recall@ ), but at what point of time? Maybe different model groups are best at different prediction times. You can just use the one that performs best on the last test set. You can value a model group that provides consistent results over time. It might not be the best on any test set, but you can feel more confident that it will continue to perform similarly. If there are several model groups that perform similarly and their lists are more or less similar, maybe it doesn't really matter which you pick. Remember\u2026 Before move on, remember the two main caveats for the value of the metric in this kind of ML problems: Could be many entities with the same predicted risk score ( ties ) Could be a lot of entities without a label (Weren't inspected, so we don\u2019t know) We included a simple configuration file in /triage/audition/inspection_audition_config.yaml with some rules: # CHOOSE MODEL GROUPS model_groups : query : | select distinct(model_group_id) from model_metadata.model_groups where model_config ->> 'experiment_type' ~ 'inspection' # CHOOSE TIMESTAMPS/TRAIN END TIMES time_stamps : query : | select distinct train_end_time from model_metadata.models where model_group_id in ({}) and extract(day from train_end_time) in (1) and train_end_time >= '2014-01-01' # FILTER filter : metric : 'precision@' # metric of interest parameter : '10_pct' # parameter of interest max_from_best : 1.0 # The maximum value that the given metric can be worse than the best model for a given train end time. threshold_value : 0.0 # The worst absolute value that the given metric should be. distance_table : 'inspections_distance_table' # name of the distance table models_table : 'models' # name of the models table # RULES rules : - shared_parameters : - metric : 'precision@' parameter : '10_pct' selection_rules : - name : 'best_current_value' # Pick the model group with the best current metric value n : 3 - name : 'best_average_value' # Pick the model with the highest average metric value n : 3 - name : 'lowest_metric_variance' # Pick the model with the lowest metric variance n : 3 - name : 'most_frequent_best_dist' # Pick the model that is most frequently within `dist_from_best_case` dist_from_best_case : [ 0.05 ] n : 3 Audition will have each rule give you the best n n model groups based on the metric and parameter following that rule for the most recent time period (in all the rules shown n n = 3). We can run the simulation of the rules against the experiment as: # Run this in bastion\u2026 triage --tb audition -c inspection_audition_config.yaml --directory audition/inspections Audition will create several plots that will help you to sort out which is the best model group to use (like in a production setting or just to generate your predictions list).","title":"Selecting the best model"},{"location":"dirtyduck/inspections/#filtering-model-groups","text":"Most of the time, audition should be used in a iterative fashion, the result of each iteration will be a reduced set of models groups and a best rule for selecting model groups. If you look again at the audition configuration file above you can filter the number of models to consider using the parameters max_from_best and threshold_value . The former will filter out models groups with models which performance in the metric is farther than the max_from_best (In this case we are allowing all the models, since max_from_best = 1.0 , if you want less models you could choose 0.1 for example, and you will remove the DummyClassifier and some DecisionTreeClassifiers ). threshold_value filter out all the models groups with models performing below that the specified value. This could be important if you don\u2019t find acceptable models with metrics that are that low. Audition will generate two plots that are meant to be used together: model performance over time and distance from best . Figure. Model group performance over time. In this case the metric show is precision@10% . We didn\u2019t filter out any model group, so the 45 model groups are shown. See discussion above to learn how to plot less model groups. The black dashed line represents the (theoretical) system's performance if we select the best performant model in a every evaluation date. The colored lines represents different model groups. All the model groups that share an algorithm will be colored the same. Next figure shows the proportion of models that are behind the best model. The distance is measured in percentual points. You could use this plot to filter out more model groups, changing the value of max_from_best in the configuration file. This plot is hard to read, but is very helpful since it shows you the consistency of the model group: How consistently are the model group in a specific range, let's say 20 points, from the best? Figure. Proportion of *models in a model group that are separated from the best model. The distance is measured in percentual points, i.e. How much less precision at 10 percent of the population compared to the best model in that date.* In the figure, you can see that the ~60% of the DummyClassifier models are ~18 percentual points below of the best.","title":"Filtering model groups"},{"location":"dirtyduck/inspections/#selecting-the-best-rule-or-strategy-for-choosing-model-groups","text":"In this phase of the audition, you will see what will happen in the next time if you choose your model group with an specific strategy or rule. We call this the regret of the strategies. We define regret as Regret Is the difference in performance between the model group you picked and the best one in the next time period. The next plot show the best model group selected by the strategies specified in the configuration file: Figure. Given a strategy for selecting model groups (in the figure, 4 are shown), What will be the performace of the model group chosen by that strategy in the next evaluation date? It seems that the strategies best average and best current value select the same model group . Figure. Given a strategy for selecting model groups (in the plot 4 are shown). What will be the distance (*regret ) to the best theoretical model in the following evaluation date?* Obviously, you don\u2019t know the future, but with the available data, if you stick to an a particular strategy, How much you will regret about that decision? Figure. Expected regret for the strategies. The less the better. The best 3 model groups per strategy will be stored in the file [[file:audition/inspections/results_model_group_ids.json][results_model_group_ids.json]] : { \"best_current_value_precision@_10_pct\" : [ 39 , 30 , 9 ], \"best_average_value_precision@_10_pct\" : [ 39 , 9 , 29 ], \"lowest_metric_variance_precision@_10_pct\" : [ 1 , 5 , 19 ], \"most_frequent_best_dist_precision@_10_pct_0.05\" : [ 8 , 9 , 10 ] } The analysis suggests that the best strategies are select the model groups ( 39,9,29 ) which have the best average precision@10% value or, select the best model group ( 39,30,9 ) using precision@10% today and use it for the next time period. You will note that both strategies share two models groups and differ in one. In the next two sections, we will investigate further those four model groups selected by audition , using the Postmodeling tool set.","title":"Selecting the best rule or strategy for choosing model groups"},{"location":"dirtyduck/inspections/#postmodeling-inspecting-the-best-models-closely","text":"Postmodeling will help you to understand the behaviour orf your selected models (from audition) As in Audition , we will split the postmodeling process in two parts. The first one is about exploring the model groups filtered by audition, with the objective of select one. The second part is about learning about models in the model group that was selected. We will setup some parameters in the postmodeling configuration file located at /triage/postmodeling/inspection_postmodeling_config.yaml , mainly where is the audition\u2019s output file located. # Postmodeling Configuration File project_path : '/triage' # Project path defined in triage with matrices and models model_group_id : - 39 - 9 - 29 - 30 thresholds : # Thresholds for defining positive predictions rank_abs : [ 50 , 100 , 250 ] rank_pct : [ 5 , 10 , 25 ] baseline_query : | # SQL query for defining a baseline for comparison in plots. It needs a metric and parameter select g.model_group_id, m.model_id, extract('year' from m.evaluation_end_time) as as_of_date_year, m.metric, m.parameter, m.value, m.num_labeled_examples, m.num_labeled_above_threshold, m.num_positive_labels from test_results.evaluations m left join model_metadata.models g using(model_id) where g.model_group_id = 1 and metric = 'precision@' and parameter = '10_pct' max_depth_error_tree : 5 # For error trees, how depth the decision trees should go? n_features_plots : 10 # Number of features for importances figsize : [ 12 , 12 ] # Default size for plots fontsize : 20 # Default fontsize for plots Compared to the previous sections, postmodeling is not an automated process (yet). Hence, to do the following part of the tutorial, you need to run jupyter inside bastion as follows: jupyter-notebook \u2013-ip = 0 .0.0.0 --port = 56406 --allow-root And then in your browser type 11 : http://0.0.0.0:56406 Now that you are in a jupyter notebook, type the following: %matplotlib inline import matplotlib #matplotlib.use('Agg') import triage import pandas as pd import numpy as np from collections import OrderedDict from triage.component.postmodeling.contrast.utils.aux_funcs import create_pgconn, get_models_ids from triage.component.catwalk.storage import ProjectStorage, ModelStorageEngine, MatrixStorageEngine from triage.component.postmodeling.contrast.parameters import PostmodelParameters from triage.component.postmodeling.contrast.model_evaluator import ModelEvaluator from triage.component.postmodeling.contrast. model_group_evaluator import ModelGroupEvaluator After importing, we need to create an sqlalchemy engine for connecting to the database, and read the configuration file. params = PostmodelParameters('inspection_postmodeling_config.yaml') engine = create_pgconn('database.yaml') Postmodeling provides the object ModelGroupEvaluator to compare different model groups . audited_models_class = ModelGroupEvaluator(tuple(params.model_group_id), engine)","title":"Postmodeling: Inspecting the best models closely"},{"location":"dirtyduck/inspections/#comparing-the-audited-model-groups","text":"First we will compare the performance of the audited model groups and the baseline over time. First, we will plot precision@10_pct audited_models_class.plot_prec_across_time(param_type='rank_pct', param=10, baseline=True, baseline_query=params.baseline_query, metric='precision@', figsize=params.figsize) Figure. Precision@10% over time from the best performing model groups selected by Audition and now the recall@10_pct audited_models_class.plot_prec_across_time(param_type='rank_pct', param=10, metric='recall@', figsize=params.figsize) Figure. Recall@10% over time from the best performing model groups selected by Audition All the selected model groups have a very similar performance. Let\u2019s see if they are predicting similar lists of facilities that are at risk of fail an inspection. audited_models_class.plot_jaccard_preds(param_type='rank_pct', param=10, temporal_comparison=True) Figure. How similar are the model groups\u2019 generated list? We use Jaccard similarity on the predicted lists (length of list 10%) to asses the overlap between lists. The plot will shows the overlap of the predicted list containing the 10% of the facilities between model groups at each as of date . The lists are at least 50% similar. Tip Why the models are not learning the same? You should investigate why is that so. This could lead you to defining new features or some another conclusion about your data, but for this tutorial we will move on.","title":"Comparing the audited model groups"},{"location":"dirtyduck/inspections/#going-deeper-with-a-model","text":"Imagine that after a deeper analysis, you decide to choose model group 39 select mg . model_group_id , mg . model_type , mg . hyperparameters , array_agg ( model_id order by train_end_time ) as models from model_metadata . model_groups as mg inner join model_metadata . models using ( model_group_id ) where model_group_id = 39 group by 1 , 2 , 3 model_group_id model_type hyperparameters models 39 sklearn.ensemble.RandomForestClassifier {\"criterion\": \"gini\", \"max_features\": \"sqrt\", \"n_estimators\": 250, \"min_samples_split\": 10} {53,89,125} We will investigate what the particular models are doing. Postmodeling created a ModelEvaluator (similar to the ModelGroupEvaluator ) to do this exploration: models_39 = { f'{model}': ModelEvaluator(39, model, engine) for model in [53,89,125] } In this tutorial, we will just show some parts of the analysis in the most recent model, but feel free of exploring the behavior of all the models in this model group, and check if you can detect any pattern. Feature importances models_39['125'].plot_feature_importances(path=params.project_path, n_features_plots=params.n_features_plots, figsize=params.figsize) Figure. Top 10 feature importances for de model group 11 at 2016-06-01 (i.e. model 125). models_39['125'].plot_feature_group_average_importances() Figure. Feature group \u201cimportance\u201d (we are basically taking the average of all the feature importances in a feature group) for the model group 39, model 125. Our Policy menu The following plot depicts the behavior of the metrics if you change the length of the facilities predicted at risk (i.e. the k k ). This plot is important from the decision making point of view, since it could be used as a policy menu . models_39['125'].plot_precision_recall_n() Figure. Plot of Precision and Recall over the proportion of the facilities. This plot is used as a \"policy menu\" since allows you to see how much you will gain if you invest more resources or how much you will sacrifice if you reduce your budget for resources. This is also known as \u201cRayid plot\u201d at DSaPP. We selected this model group because it was the best at precision at 10% (i.e. the model group consistently chose facilities will fail inspections at the top 10% of the risk). With the plot above you could decide to double your resources (maybe hiring more inspectors so you could inspect 20% of the facilities) and with this model you will double the detection of facilities that will fail inspections (from ~18% to ~30% in recall) with only a few percent points less of precision ~45% to ~40% (this means that 6 in 10 facilities that the inspectors visit will pass the inspection). You could also go the other way around: if you reduce the length of the list from 10% to 5%, well you will gain a little of precision, but your recall will be ~5%.","title":"Going deeper with a model"},{"location":"dirtyduck/inspections/#where-to-go-from-here","text":"Ready to get started with your own data? Check out the suggested project workflow for some tips about how to iterate and tune the pipeline for your project. Want to work through another example? Take a look at our early warning system case study If you assume a uniform distribution, it will make sense to select facilities at random. \u21a9 The underlying assumption here is that the City of Chicago is currently doing random selection for the inspections. This is not true (and probably unfair). In a real project, you will setup a real baseline and you will compare your models against it. This baseline could be a rule or a model. \u21a9 You need to check this! Fortunately, triage allows you to try several options here, so, if you think that this is too high or too low you can change that and fit your needs. \u21a9 Think about it: we can\u2019t learn the relationship between the features and the label if we don't know the label. \u21a9 Confused? Check A deeper look at triage for more details. \u21a9 The formulas are, for precision@k , is the proportion of facilities correctly identified in the top- k k facilities ranked by risk: $$ precision@k = \\frac{TP \\in k}{k} $$ This is a measure about how efficiently are your system using your resources. recall@k , in the other hand is the proportion of all the facilities that are risk found in the top- k k $$ recall@k = \\frac{TP \\in k}{TP} $$ recall is a measure about the coverage of your system, i.e. how good is identifying in the top- k k the facilities at risk . One possible variation of this is to only include in the denominator the labeled rows in k k . This is the approach used by triage . \u21a9 We will explore how to one way to tackle this in the advance part of this tutorial. \u21a9 The flags -no-save-predictions and profile are not necessary but useful. The first one configure triage to not store the predictions (at this stage you don't need them, and you can always could recreate them from the model and the matrix). This will save you execution time. The flag profile stores the execution profile times in a file, so you can check which models or matrices are taking a lot of time on been built. \u21a9 From a more mathematical point of view: Your data actually reflects the empirical probability: P(violation|inspected) P(violation|inspected) , i.e. the probability of find a violation given that the facility is inspected. But the probability that you want is P(violation) P(violation) (yes, I know that there are no such things as unconditional probabilities, please bare with me),i.e. the probability that the facility is in violation. \u21a9 You should see that this assumption is very dangerous in other settings, for example, crime prediction. \u21a9 This assumes that you are in a GNU/Linux machine, if not (you should reconsider what are you doing with your life) you should change the ip address ( 0.0.0.0 ) and use the one from the docker virtual machine. \u21a9 For some reason, sklearn doesn\u2019t scale the inputs to the Logistic Regression, so we (DSaPP) developed a version that does that. \u21a9","title":"Where to go from here"},{"location":"dirtyduck/ml_governance/","text":"Machine learning governance # When triage executes the experiment, it creates a series of new schemas for storing the copious output of the experiment. The schemas are test_results, train_results , and model_metadata . These schemas store the metadata of the trained models, features, parameters, and hyperparameters used in their training. It also stores the predictions and evaluations of the models on the test sets. The schema model_metadata is composed by the tables: \\ dt model_metadata . * List of relations Schema Name Type Owner model metadata experiment matrices table food user model metadata experiment models table food user model metadata experiments table food user model metadata list predictions table food user model metadata matrices table food user model metadata model groups table food user model metadata models table food user The tables contained in test_results are: \\ dt test_results . * List of relations Schema Name Type Owner test results aequitas table food user test results evaluations table food user test results individual importances table food user test results predictions table food user Lastly, if you have interest in how the model performed in the training data sets you could consult train_results \\ dt train_results . * List of relations Schema Name Type Owner train results aequitas table food user train results evaluations table food user train results feature importances table food user train results predictions table food user What are all the results tables about? # model_groups stores the algorithm ( model_type ), the hyperparameters ( hyperparameters ), and the features shared by a particular set of models. models contains data specific to a model: the model_group (you can use model_group_id for linking the model to a model group), temporal information (like train_end_time ), and the train matrix UUID ( train_matrix_uuid ). This UUID is important because it's the name of the file in which the matrix is stored. Lastly, {train, test}_results.predictions contains all the scores generated by every model for every entity. {train_test}_results.evaluation stores the value of all the metrics for every model, which were specified in the scoring section in the config file. model_metadata.experiments # This table has the two columns: experiment_hash and config \\ d model_metadata . experiments Table \"model metadata.experiments \" Column Type Collation Nullable Default experiment hash character varying not null config jsonb Indexes: \"experiments pkey \" PRIMARY KEY, btree (experiment hash ) Referenced by: TABLE \"model metadata.experiment matrices \" CONSTRAINT \"experiment matrices experiment hash fkey \" FOREIGN KEY (experiment hash ) REFERENCES model metadata.experiments (experiment hash ) TABLE \"model metadata.experiment models \" CONSTRAINT \"experiment models experiment hash fkey \" FOREIGN KEY (experiment hash ) REFERENCES model metadata.experiments (experiment hash ) TABLE \"model metadata.matrices \" CONSTRAINT \"matrices built by experiment fkey \" FOREIGN KEY (built by experiment ) REFERENCES model metadata.experiments (experiment hash ) TABLE \"model metadata.models \" CONSTRAINT \"models experiment hash fkey \" FOREIGN KEY (built by experiment ) REFERENCES model metadata.experiments (experiment hash ) experiment_hash contains the hash of the configuration file that we used for our triage run. 1 config that contains the configuration experiment file that we used for our triage run, stored as jsonb . select experiment_hash , config -> 'user_metadata' as user_metadata from model_metadata . experiments ; experiment hash user metadata 67a1d564d31811b9c20ca63672c25abd {\"org\": \"DSaPP\", \"team\": \"Tutorial\", \"author\": \"Adolfo De Unanue\", \"etl date \": \"2019-02-21\", \"experiment type \": \"test\", \"label definition \": \"failed inspection \"} We could use the following advice: If we are interested in all models that resulted from a certain config, we could lookup that config in model_metadata.experiments and then use its experiment_hash on other tables to find all the models that resulted from that configuration. metadata_model.model_groups # Do you remember how we defined in grid_config the different classifiers that we want triage to train? For example, we could use in a configuration file the following: 'sklearn.tree.DecisionTreeClassifier' : criterion : [ 'entropy' ] max_depth : [ 1 , 2 , 5 , 10 ] random_state : [ 2193 ] By doing so, we are saying that we want to train 4 decision trees ( max_depth is one of 1, 2, 5, 10 ). However, remember that we are using temporal crossvalidation to build our models, so we are going to have different temporal slices that we are training models on, e.g., 2010-2011, 2011-2012, etc. Therefore, we are going to train our four decision trees on each temporal slice. Therefore, the trained model (or the instance of that model) will change across temporal splits, but the configuration will remain the same. This table lets us keep track of the different configurations ( model_groups ) and gives us an id for each configuration ( model_group_id ). We can leverage the model_group_id to find all the models that were trained using the same config but on different slices of time. In our simple test configuration file we have: 'sklearn.dummy.DummyClassifier' : strategy : [ most_frequent ] Therefore, if we run the following select model_group_id , model_type , hyperparameters , model_config -> 'feature_groups' as feature_groups , model_config -> 'cohort_name' as cohort , model_config -> 'label_name' as label , model_config -> 'label_definition' as label_definition , model_config -> 'experiment_type' as experiment_type , model_config -> 'etl_date' as etl_date from model_metadata . model_groups ; model group id model type hyperparameters feature groups cohort label label definition experiment type etl date 1 sklearn.dummy.DummyClassifier {\"strategy\": \"most frequent \"} [\"prefix: results\", \"prefix: risks\", \"prefix: inspections\"] \"test facilities \" \"failed inspections \" \"failed inspection \" \"test\" \"2019-02-21\" You can see that a model group is defined by the classifier ( model_type ), its hyperparameters ( hyperparameters ), the features ( feature_list ) (not shown), and the model_config . The field model_config is created using information from the block model_group_keys . In our test configuration file the block is: model_group_keys : - 'class_path' - 'parameters' - 'feature_names' - 'feature_groups' - 'cohort_name' - 'state' - 'label_name' - 'label_timespan' - 'training_as_of_date_frequency' - 'max_training_history' - 'label_definition' - 'experiment_type' - 'org' - 'team' - 'author' - 'etl_date' What can we learn from that? For example, if we add a new feature and rerun triage , triage will create a new model_group even if the classifier and the hyperparameters are the same as before. model_metadata.models # This table stores the information about our actual models , i.e., instances of our classifiers trained on specific temporal slices. \\ d model_metadata . models Table \"model metadata.models \" Column Type Collation Nullable Default model id integer not null nextval('model metadata.models model id seq '::regclass) model group id integer model hash character varying run time timestamp without time zone batch run time timestamp without time zone model type character varying hyperparameters jsonb model comment text batch comment text config json built by experiment character varying train end time timestamp without time zone test boolean train matrix uuid text training label timespan interval model size real Indexes: \"models pkey \" PRIMARY KEY, btree (model id ) \"ix results models model hash \" UNIQUE, btree (model hash ) Foreign-key constraints: \"matrix uuid for models \" FOREIGN KEY (train matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) \"models experiment hash fkey \" FOREIGN KEY (built by experiment ) REFERENCES model metadata.experiments (experiment hash ) \"models model group id fkey \" FOREIGN KEY (model group id ) REFERENCES model metadata.model groups (model group id ) Referenced by: TABLE \"test results.evaluations \" CONSTRAINT \"evaluations model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) TABLE \"train results.feature importances \" CONSTRAINT \"feature importances model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) TABLE \"test results.individual importances \" CONSTRAINT \"individual importances model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) TABLE \"model metadata.list predictions \" CONSTRAINT \"list predictions model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) TABLE \"test results.predictions \" CONSTRAINT \"predictions model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) TABLE \"train results.evaluations \" CONSTRAINT \"train evaluations model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) TABLE \"train results.predictions \" CONSTRAINT \"train predictions model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) Noteworthy columns are: model_id : The id of the model (i.e., instance\u2026). We will use this ID to trace a model evaluation to a model_group and vice versa. model_group_id : The id of the models model group we encountered above. model_hash : The hash of our model. We can use the hash to load the actual model. It gets stored under TRIAGE_OUTPUT_PATH/trained_models/{model_hash} . We are going to this later to look at a trained decision tree. run_time : Time when the model was trained. model_type : The algorithm used for training. model_comment : Literally the text in the model_comment block in the configuration file hyperparameters : Hyperparameters used for the model configuration. built_by_experiment : The hash of our experiment. We encountered this value in the results.experiments table before. train_end_time : When building the training matrix, we included training samples up to this date. train_matrix_uuid : The hash of the matrix that we used to train this model. The matrix gets stored as csv under TRIAGE_OUTPUT_PATH/matrices/{train_matrix_uuid}.csv . This is helpful when trying to inspect the matrix and features that were used for training. train_label_timespan : How big was our window to get the labels for our training matrix? For example, a train_label_window of 1 year would mean that we look one year from a given date in the training matrix into the future to find the label for that training sample. model_metadata.matrices # This schema contains information about the matrices used in the model's training. You could use this information to debug your models. Important columns are matrix_uuid (The matrix gets stored as TRIAGE_OUTPUT_PATH/matrices/{train_matrix_uuid}.csv ), matrix_type (indicated if the matrix was used for training models or testing them), lookback_duration and feature_starttime (give information about the temporal setting of the features) and num_observations (size of the matrices). \\ d model_metadata . matrices Table \"model metadata.matrices \" Column Type Collation Nullable Default matrix id character varying matrix uuid character varying not null matrix type character varying labeling window interval num observations integer creation time timestamp with time zone now() lookback duration interval feature start time timestamp without time zone matrix metadata jsonb built by experiment character varying Indexes: \"matrices pkey \" PRIMARY KEY, btree (matrix uuid ) \"ix model metadata matrices matrix uuid \" UNIQUE, btree (matrix uuid ) Foreign-key constraints: \"matrices built by experiment fkey \" FOREIGN KEY (built by experiment ) REFERENCES model metadata.experiments (experiment hash ) Referenced by: TABLE \"test results.evaluations \" CONSTRAINT \"evaluations matrix uuid fkey \" FOREIGN KEY (matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) TABLE \"train results.evaluations \" CONSTRAINT \"evaluations matrix uuid fkey \" FOREIGN KEY (matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) TABLE \"model metadata.models \" CONSTRAINT \"matrix uuid for models \" FOREIGN KEY (train matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) TABLE \"test results.predictions \" CONSTRAINT \"matrix uuid for testpred \" FOREIGN KEY (matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) TABLE \"train results.predictions \" CONSTRAINT \"matrix uuid for trainpred \" FOREIGN KEY (matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) TABLE \"train results.predictions \" CONSTRAINT \"train predictions matrix uuid fkey \" FOREIGN KEY (matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) {test, train}_results.evaluations # These tables lets us analyze how well our models are doing. Based on the config that we used for our triage run, triage is calculating metrics and storing them in this table, e.g., our model's precision in top 10%. \\ d test_results . evaluations Table \"test results.evaluations \" Column Type Collation Nullable Default model id integer not null evaluation start time timestamp without time zone not null evaluation end time timestamp without time zone not null as of date frequency interval not null metric character varying not null parameter character varying not null value numeric num labeled examples integer num labeled above threshold integer num positive labels integer sort seed integer matrix uuid text Indexes: \"evaluations pkey \" PRIMARY KEY, btree (model id , evaluation start time , evaluation end time , as of date frequency , metric, parameter) Foreign-key constraints: \"evaluations matrix uuid fkey \" FOREIGN KEY (matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) \"evaluations model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) Its columns are: model_id : Our beloved model_id that we have encountered before. evaluation_start_time : After training the model, we evaluate it on a test matrix. This column tells us the earliest time that an example in our test matrix could have. evaluation_end_time : After training the model, we evaluate it on a test matrix. This column tells us the latest time that an example in our test matrix could have. metric : Indicates which metric we are evaluating, e.g., precision@ . parameter ::Indicates at which parameter we are evaluating our metric, e.g., a metric of precision@ and a parameter of 100.0_pct shows us the precision@100pct . value : The value observed for our metric@parameter. num_labeled_examples : The number of labeled examples in our test matrix. Why does it matter? It could be the case that we have entities that have no label for the test timeframe (for example, not all facilities will have an inspection). We still want to make predictions for these entities but can't include them when calculating performance metrics. num_labeled_above_threshold : How many examples above our threshold were labeled? num_positive_labels : The number of rows that had true positive labels. A look at the table shows that we have multiple rows for each model, each showing a different performance metric. select evaluation_end_time , model_id , metric || parameter as metric , value , num_labeled_examples , num_labeled_above_threshold , num_positive_labels from test_results . evaluations where parameter = '100.0_pct' ; evaluation end time model id metric value num labeled examples num labeled above threshold num positive labels 2016-01-01 00:00:00 1 precision@100.0 pct 0.6666666666666666 3 3 2 2016-01-01 00:00:00 1 recall@100.0 pct 1.0 3 3 2 2017-01-01 00:00:00 2 precision@100.0 pct 0.3333333333333333 3 3 1 2017-01-01 00:00:00 2 recall@100.0 pct 1.0 3 3 1 Remember that at 100%, the recall should be 1, and the precision is equal to the baserate . If these two things don't match, there are problems in your data, pipeline, etl. You must get this correct! What does this query tell us? We can now see how the different instances (trained on different temporal slices, but with the same model params) of a model group performs over time. Note how we only included the models that belong to our model group 1 . {test_train}_results.aequitas # Standard evaluation metrics don't tell us the entire story: what are the biases in our models? what is the fairest model? Given the bias_audit_config in the experiment config in which we defined what protected attributes we care about (e.g. ethnicity) and the specific thresholds our model is going to be used, Triage uses Aequitas to generate a bias report on each model and matrix, similar to standard evaluation metrics. The aequitas tables will have a row for each combination of: - model_id - subset_hash - tie_breaker (e.g. best, worst) - evaluation_start_time - evaluation_end_time - parameter (e.g. 25_abs , similar to evaluation metric thresholds) - attribute_name (e.g. 'facility_type') - attribute_value (e.g. 'kids_facility', 'restaurant') For each row Aequitas calculates the following group metrics: Metric Formula Description Predicted Positive The number of entities within a group where the decision is positive, i.e., Total Predictive Positive The total number of entities predicted positive across groups defined by Predicted Negative The number of entities within a group which decision is negative, i.e., Predicted Prevalence The fraction of entities within a group which were predicted as positive. Predicted Positive Rate The fraction of the entities predicted as positive that belong to a certain group. False Positive The number of entities of the group with and False Negative The number of entities of the group with and True Positive The number of entities of the group with and True Negative The number of entities of the group with and False Discovery Rate The fraction of false positives of a group within the predicted positive of the group. False Omission Rate The fraction of false negatives of a group within the predicted negative of the group. False Positive Rate The fraction of false positives of a group within the labeled negative of the group. False Negative Rate The fraction of false negatives of a group within the labeled positives of the group. In the context of public policy and social good we want to avoid providing less benefits to specific groups of entities, if the intervention is assistive, as well as, avoid hurting more specific groups, if the intervention is punitive. Therefore we define bias as a disparity measure of group metric values of a given group when compared with a reference group. This reference can be selected using different criteria. For instance, one could use the majority group (with larger size) across the groups defined by A, or the group with minimum group metric value, or the traditional approach of fixing a historically favored group (e.g ethnicity:caucasian). Each disparity metric for a given group is calculated as follows: To read about the bias metrics saved in this table, look at the Aequitas documentation . Table \"test_results.aequitas\" Column Type Collation Nullable Default model_id integer not null subset_hash character varying not null tie_breaker character varying not null evaluation_start_time timestamp without time zone not null evaluation_end_time timestamp without time zone not null matrix_uuid text parameter character varying not null attribute_name character varying not null attribute_value character varying not null total_entities integer group_label_pos integer group_label_neg integer group_size integer group_size_pct numeric prev numeric pp integer pn integer fp integer fn integer tn integer tp integer ppr numeric pprev numeric tpr numeric tnr numeric for numeric fdr numeric fpr numeric fnr numeric npv numeric precision numeric ppr_disparity numeric ppr_ref_group_value character varying pprev_disparity numeric pprev_ref_group_value character varying precision_disparity numeric precision_ref_group_value character varying fdr_disparity numeric fdr_ref_group_value character varying for_disparity numeric for_ref_group_value character varying fpr_disparity numeric fpr_ref_group_value character varying fnr_disparity numeric fnr_ref_group_value character varying tpr_disparity numeric tpr_ref_group_value character varying tnr_disparity numeric tnr_ref_group_value character varying npv_disparity numeric npv_ref_group_value character varying Statistical_Parity boolean Impact_Parity boolean FDR_Parity boolean FPR_Parity boolean FOR_Parity boolean FNR_Parity boolean TypeI_Parity boolean TypeII_Parity boolean Equalized_Odds boolean Unsupervised_Fairness boolean Supervised_Fairness boolean {test, train}_results.predictions # You can think of the previous table {test, train}_results.{test, train}_predictions as a summary of individuals predictions that our model is making. But where can you find the individual predictions that our model is making? (So you can generate a list from here). And where can we find the test matrix that the predictions are based on? Let us introduce you to the results.predictions table. Here is what its first row looks like: select model_id , entity_id , as_of_date , score , label_value , matrix_uuid from test_results . predictions where model_id = 1 order by score desc ; model id entity id as of date score label value matrix uuid 1 229 2016-01-01 00:00:00 1.0 1 cd0ae68d6ace43033b49ee0390c3583e 1 355 2016-01-01 00:00:00 1.0 1 cd0ae68d6ace43033b49ee0390c3583e 1 840 2016-01-01 00:00:00 1.0 0 cd0ae68d6ace43033b49ee0390c3583e As you can see, the table contains our models' predictions for a given entity and date. And do you notice the field matrix_uuid ? Doesn't it look similar to the fields from above that gave us the names of our training matrices? In fact, it is the same. You can find the test matrix that was used to make this prediction under TRIAGE_OUTPUT_PATH/matrices/{matrix_uuid}.csv . {test, train}_results.feature_importances # This tables store the feature importance of all the models. Footnotes # 1 Literally from the configuration file. If you modify something it will generate a new hash. Handle with care!","title":"Model governance"},{"location":"dirtyduck/ml_governance/#machine-learning-governance","text":"When triage executes the experiment, it creates a series of new schemas for storing the copious output of the experiment. The schemas are test_results, train_results , and model_metadata . These schemas store the metadata of the trained models, features, parameters, and hyperparameters used in their training. It also stores the predictions and evaluations of the models on the test sets. The schema model_metadata is composed by the tables: \\ dt model_metadata . * List of relations Schema Name Type Owner model metadata experiment matrices table food user model metadata experiment models table food user model metadata experiments table food user model metadata list predictions table food user model metadata matrices table food user model metadata model groups table food user model metadata models table food user The tables contained in test_results are: \\ dt test_results . * List of relations Schema Name Type Owner test results aequitas table food user test results evaluations table food user test results individual importances table food user test results predictions table food user Lastly, if you have interest in how the model performed in the training data sets you could consult train_results \\ dt train_results . * List of relations Schema Name Type Owner train results aequitas table food user train results evaluations table food user train results feature importances table food user train results predictions table food user","title":"Machine learning governance"},{"location":"dirtyduck/ml_governance/#what-are-all-the-results-tables-about","text":"model_groups stores the algorithm ( model_type ), the hyperparameters ( hyperparameters ), and the features shared by a particular set of models. models contains data specific to a model: the model_group (you can use model_group_id for linking the model to a model group), temporal information (like train_end_time ), and the train matrix UUID ( train_matrix_uuid ). This UUID is important because it's the name of the file in which the matrix is stored. Lastly, {train, test}_results.predictions contains all the scores generated by every model for every entity. {train_test}_results.evaluation stores the value of all the metrics for every model, which were specified in the scoring section in the config file.","title":"What are all the results tables about?"},{"location":"dirtyduck/ml_governance/#model_metadataexperiments","text":"This table has the two columns: experiment_hash and config \\ d model_metadata . experiments Table \"model metadata.experiments \" Column Type Collation Nullable Default experiment hash character varying not null config jsonb Indexes: \"experiments pkey \" PRIMARY KEY, btree (experiment hash ) Referenced by: TABLE \"model metadata.experiment matrices \" CONSTRAINT \"experiment matrices experiment hash fkey \" FOREIGN KEY (experiment hash ) REFERENCES model metadata.experiments (experiment hash ) TABLE \"model metadata.experiment models \" CONSTRAINT \"experiment models experiment hash fkey \" FOREIGN KEY (experiment hash ) REFERENCES model metadata.experiments (experiment hash ) TABLE \"model metadata.matrices \" CONSTRAINT \"matrices built by experiment fkey \" FOREIGN KEY (built by experiment ) REFERENCES model metadata.experiments (experiment hash ) TABLE \"model metadata.models \" CONSTRAINT \"models experiment hash fkey \" FOREIGN KEY (built by experiment ) REFERENCES model metadata.experiments (experiment hash ) experiment_hash contains the hash of the configuration file that we used for our triage run. 1 config that contains the configuration experiment file that we used for our triage run, stored as jsonb . select experiment_hash , config -> 'user_metadata' as user_metadata from model_metadata . experiments ; experiment hash user metadata 67a1d564d31811b9c20ca63672c25abd {\"org\": \"DSaPP\", \"team\": \"Tutorial\", \"author\": \"Adolfo De Unanue\", \"etl date \": \"2019-02-21\", \"experiment type \": \"test\", \"label definition \": \"failed inspection \"} We could use the following advice: If we are interested in all models that resulted from a certain config, we could lookup that config in model_metadata.experiments and then use its experiment_hash on other tables to find all the models that resulted from that configuration.","title":"model_metadata.experiments"},{"location":"dirtyduck/ml_governance/#metadata_modelmodel_groups","text":"Do you remember how we defined in grid_config the different classifiers that we want triage to train? For example, we could use in a configuration file the following: 'sklearn.tree.DecisionTreeClassifier' : criterion : [ 'entropy' ] max_depth : [ 1 , 2 , 5 , 10 ] random_state : [ 2193 ] By doing so, we are saying that we want to train 4 decision trees ( max_depth is one of 1, 2, 5, 10 ). However, remember that we are using temporal crossvalidation to build our models, so we are going to have different temporal slices that we are training models on, e.g., 2010-2011, 2011-2012, etc. Therefore, we are going to train our four decision trees on each temporal slice. Therefore, the trained model (or the instance of that model) will change across temporal splits, but the configuration will remain the same. This table lets us keep track of the different configurations ( model_groups ) and gives us an id for each configuration ( model_group_id ). We can leverage the model_group_id to find all the models that were trained using the same config but on different slices of time. In our simple test configuration file we have: 'sklearn.dummy.DummyClassifier' : strategy : [ most_frequent ] Therefore, if we run the following select model_group_id , model_type , hyperparameters , model_config -> 'feature_groups' as feature_groups , model_config -> 'cohort_name' as cohort , model_config -> 'label_name' as label , model_config -> 'label_definition' as label_definition , model_config -> 'experiment_type' as experiment_type , model_config -> 'etl_date' as etl_date from model_metadata . model_groups ; model group id model type hyperparameters feature groups cohort label label definition experiment type etl date 1 sklearn.dummy.DummyClassifier {\"strategy\": \"most frequent \"} [\"prefix: results\", \"prefix: risks\", \"prefix: inspections\"] \"test facilities \" \"failed inspections \" \"failed inspection \" \"test\" \"2019-02-21\" You can see that a model group is defined by the classifier ( model_type ), its hyperparameters ( hyperparameters ), the features ( feature_list ) (not shown), and the model_config . The field model_config is created using information from the block model_group_keys . In our test configuration file the block is: model_group_keys : - 'class_path' - 'parameters' - 'feature_names' - 'feature_groups' - 'cohort_name' - 'state' - 'label_name' - 'label_timespan' - 'training_as_of_date_frequency' - 'max_training_history' - 'label_definition' - 'experiment_type' - 'org' - 'team' - 'author' - 'etl_date' What can we learn from that? For example, if we add a new feature and rerun triage , triage will create a new model_group even if the classifier and the hyperparameters are the same as before.","title":"metadata_model.model_groups"},{"location":"dirtyduck/ml_governance/#model_metadatamodels","text":"This table stores the information about our actual models , i.e., instances of our classifiers trained on specific temporal slices. \\ d model_metadata . models Table \"model metadata.models \" Column Type Collation Nullable Default model id integer not null nextval('model metadata.models model id seq '::regclass) model group id integer model hash character varying run time timestamp without time zone batch run time timestamp without time zone model type character varying hyperparameters jsonb model comment text batch comment text config json built by experiment character varying train end time timestamp without time zone test boolean train matrix uuid text training label timespan interval model size real Indexes: \"models pkey \" PRIMARY KEY, btree (model id ) \"ix results models model hash \" UNIQUE, btree (model hash ) Foreign-key constraints: \"matrix uuid for models \" FOREIGN KEY (train matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) \"models experiment hash fkey \" FOREIGN KEY (built by experiment ) REFERENCES model metadata.experiments (experiment hash ) \"models model group id fkey \" FOREIGN KEY (model group id ) REFERENCES model metadata.model groups (model group id ) Referenced by: TABLE \"test results.evaluations \" CONSTRAINT \"evaluations model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) TABLE \"train results.feature importances \" CONSTRAINT \"feature importances model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) TABLE \"test results.individual importances \" CONSTRAINT \"individual importances model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) TABLE \"model metadata.list predictions \" CONSTRAINT \"list predictions model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) TABLE \"test results.predictions \" CONSTRAINT \"predictions model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) TABLE \"train results.evaluations \" CONSTRAINT \"train evaluations model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) TABLE \"train results.predictions \" CONSTRAINT \"train predictions model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) Noteworthy columns are: model_id : The id of the model (i.e., instance\u2026). We will use this ID to trace a model evaluation to a model_group and vice versa. model_group_id : The id of the models model group we encountered above. model_hash : The hash of our model. We can use the hash to load the actual model. It gets stored under TRIAGE_OUTPUT_PATH/trained_models/{model_hash} . We are going to this later to look at a trained decision tree. run_time : Time when the model was trained. model_type : The algorithm used for training. model_comment : Literally the text in the model_comment block in the configuration file hyperparameters : Hyperparameters used for the model configuration. built_by_experiment : The hash of our experiment. We encountered this value in the results.experiments table before. train_end_time : When building the training matrix, we included training samples up to this date. train_matrix_uuid : The hash of the matrix that we used to train this model. The matrix gets stored as csv under TRIAGE_OUTPUT_PATH/matrices/{train_matrix_uuid}.csv . This is helpful when trying to inspect the matrix and features that were used for training. train_label_timespan : How big was our window to get the labels for our training matrix? For example, a train_label_window of 1 year would mean that we look one year from a given date in the training matrix into the future to find the label for that training sample.","title":"model_metadata.models"},{"location":"dirtyduck/ml_governance/#model_metadatamatrices","text":"This schema contains information about the matrices used in the model's training. You could use this information to debug your models. Important columns are matrix_uuid (The matrix gets stored as TRIAGE_OUTPUT_PATH/matrices/{train_matrix_uuid}.csv ), matrix_type (indicated if the matrix was used for training models or testing them), lookback_duration and feature_starttime (give information about the temporal setting of the features) and num_observations (size of the matrices). \\ d model_metadata . matrices Table \"model metadata.matrices \" Column Type Collation Nullable Default matrix id character varying matrix uuid character varying not null matrix type character varying labeling window interval num observations integer creation time timestamp with time zone now() lookback duration interval feature start time timestamp without time zone matrix metadata jsonb built by experiment character varying Indexes: \"matrices pkey \" PRIMARY KEY, btree (matrix uuid ) \"ix model metadata matrices matrix uuid \" UNIQUE, btree (matrix uuid ) Foreign-key constraints: \"matrices built by experiment fkey \" FOREIGN KEY (built by experiment ) REFERENCES model metadata.experiments (experiment hash ) Referenced by: TABLE \"test results.evaluations \" CONSTRAINT \"evaluations matrix uuid fkey \" FOREIGN KEY (matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) TABLE \"train results.evaluations \" CONSTRAINT \"evaluations matrix uuid fkey \" FOREIGN KEY (matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) TABLE \"model metadata.models \" CONSTRAINT \"matrix uuid for models \" FOREIGN KEY (train matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) TABLE \"test results.predictions \" CONSTRAINT \"matrix uuid for testpred \" FOREIGN KEY (matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) TABLE \"train results.predictions \" CONSTRAINT \"matrix uuid for trainpred \" FOREIGN KEY (matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) TABLE \"train results.predictions \" CONSTRAINT \"train predictions matrix uuid fkey \" FOREIGN KEY (matrix uuid ) REFERENCES model metadata.matrices (matrix uuid )","title":"model_metadata.matrices"},{"location":"dirtyduck/ml_governance/#test-train_resultsevaluations","text":"These tables lets us analyze how well our models are doing. Based on the config that we used for our triage run, triage is calculating metrics and storing them in this table, e.g., our model's precision in top 10%. \\ d test_results . evaluations Table \"test results.evaluations \" Column Type Collation Nullable Default model id integer not null evaluation start time timestamp without time zone not null evaluation end time timestamp without time zone not null as of date frequency interval not null metric character varying not null parameter character varying not null value numeric num labeled examples integer num labeled above threshold integer num positive labels integer sort seed integer matrix uuid text Indexes: \"evaluations pkey \" PRIMARY KEY, btree (model id , evaluation start time , evaluation end time , as of date frequency , metric, parameter) Foreign-key constraints: \"evaluations matrix uuid fkey \" FOREIGN KEY (matrix uuid ) REFERENCES model metadata.matrices (matrix uuid ) \"evaluations model id fkey \" FOREIGN KEY (model id ) REFERENCES model metadata.models (model id ) Its columns are: model_id : Our beloved model_id that we have encountered before. evaluation_start_time : After training the model, we evaluate it on a test matrix. This column tells us the earliest time that an example in our test matrix could have. evaluation_end_time : After training the model, we evaluate it on a test matrix. This column tells us the latest time that an example in our test matrix could have. metric : Indicates which metric we are evaluating, e.g., precision@ . parameter ::Indicates at which parameter we are evaluating our metric, e.g., a metric of precision@ and a parameter of 100.0_pct shows us the precision@100pct . value : The value observed for our metric@parameter. num_labeled_examples : The number of labeled examples in our test matrix. Why does it matter? It could be the case that we have entities that have no label for the test timeframe (for example, not all facilities will have an inspection). We still want to make predictions for these entities but can't include them when calculating performance metrics. num_labeled_above_threshold : How many examples above our threshold were labeled? num_positive_labels : The number of rows that had true positive labels. A look at the table shows that we have multiple rows for each model, each showing a different performance metric. select evaluation_end_time , model_id , metric || parameter as metric , value , num_labeled_examples , num_labeled_above_threshold , num_positive_labels from test_results . evaluations where parameter = '100.0_pct' ; evaluation end time model id metric value num labeled examples num labeled above threshold num positive labels 2016-01-01 00:00:00 1 precision@100.0 pct 0.6666666666666666 3 3 2 2016-01-01 00:00:00 1 recall@100.0 pct 1.0 3 3 2 2017-01-01 00:00:00 2 precision@100.0 pct 0.3333333333333333 3 3 1 2017-01-01 00:00:00 2 recall@100.0 pct 1.0 3 3 1 Remember that at 100%, the recall should be 1, and the precision is equal to the baserate . If these two things don't match, there are problems in your data, pipeline, etl. You must get this correct! What does this query tell us? We can now see how the different instances (trained on different temporal slices, but with the same model params) of a model group performs over time. Note how we only included the models that belong to our model group 1 .","title":"{test, train}_results.evaluations"},{"location":"dirtyduck/ml_governance/#test_train_resultsaequitas","text":"Standard evaluation metrics don't tell us the entire story: what are the biases in our models? what is the fairest model? Given the bias_audit_config in the experiment config in which we defined what protected attributes we care about (e.g. ethnicity) and the specific thresholds our model is going to be used, Triage uses Aequitas to generate a bias report on each model and matrix, similar to standard evaluation metrics. The aequitas tables will have a row for each combination of: - model_id - subset_hash - tie_breaker (e.g. best, worst) - evaluation_start_time - evaluation_end_time - parameter (e.g. 25_abs , similar to evaluation metric thresholds) - attribute_name (e.g. 'facility_type') - attribute_value (e.g. 'kids_facility', 'restaurant') For each row Aequitas calculates the following group metrics: Metric Formula Description Predicted Positive The number of entities within a group where the decision is positive, i.e., Total Predictive Positive The total number of entities predicted positive across groups defined by Predicted Negative The number of entities within a group which decision is negative, i.e., Predicted Prevalence The fraction of entities within a group which were predicted as positive. Predicted Positive Rate The fraction of the entities predicted as positive that belong to a certain group. False Positive The number of entities of the group with and False Negative The number of entities of the group with and True Positive The number of entities of the group with and True Negative The number of entities of the group with and False Discovery Rate The fraction of false positives of a group within the predicted positive of the group. False Omission Rate The fraction of false negatives of a group within the predicted negative of the group. False Positive Rate The fraction of false positives of a group within the labeled negative of the group. False Negative Rate The fraction of false negatives of a group within the labeled positives of the group. In the context of public policy and social good we want to avoid providing less benefits to specific groups of entities, if the intervention is assistive, as well as, avoid hurting more specific groups, if the intervention is punitive. Therefore we define bias as a disparity measure of group metric values of a given group when compared with a reference group. This reference can be selected using different criteria. For instance, one could use the majority group (with larger size) across the groups defined by A, or the group with minimum group metric value, or the traditional approach of fixing a historically favored group (e.g ethnicity:caucasian). Each disparity metric for a given group is calculated as follows: To read about the bias metrics saved in this table, look at the Aequitas documentation . Table \"test_results.aequitas\" Column Type Collation Nullable Default model_id integer not null subset_hash character varying not null tie_breaker character varying not null evaluation_start_time timestamp without time zone not null evaluation_end_time timestamp without time zone not null matrix_uuid text parameter character varying not null attribute_name character varying not null attribute_value character varying not null total_entities integer group_label_pos integer group_label_neg integer group_size integer group_size_pct numeric prev numeric pp integer pn integer fp integer fn integer tn integer tp integer ppr numeric pprev numeric tpr numeric tnr numeric for numeric fdr numeric fpr numeric fnr numeric npv numeric precision numeric ppr_disparity numeric ppr_ref_group_value character varying pprev_disparity numeric pprev_ref_group_value character varying precision_disparity numeric precision_ref_group_value character varying fdr_disparity numeric fdr_ref_group_value character varying for_disparity numeric for_ref_group_value character varying fpr_disparity numeric fpr_ref_group_value character varying fnr_disparity numeric fnr_ref_group_value character varying tpr_disparity numeric tpr_ref_group_value character varying tnr_disparity numeric tnr_ref_group_value character varying npv_disparity numeric npv_ref_group_value character varying Statistical_Parity boolean Impact_Parity boolean FDR_Parity boolean FPR_Parity boolean FOR_Parity boolean FNR_Parity boolean TypeI_Parity boolean TypeII_Parity boolean Equalized_Odds boolean Unsupervised_Fairness boolean Supervised_Fairness boolean","title":"{test_train}_results.aequitas"},{"location":"dirtyduck/ml_governance/#test-train_resultspredictions","text":"You can think of the previous table {test, train}_results.{test, train}_predictions as a summary of individuals predictions that our model is making. But where can you find the individual predictions that our model is making? (So you can generate a list from here). And where can we find the test matrix that the predictions are based on? Let us introduce you to the results.predictions table. Here is what its first row looks like: select model_id , entity_id , as_of_date , score , label_value , matrix_uuid from test_results . predictions where model_id = 1 order by score desc ; model id entity id as of date score label value matrix uuid 1 229 2016-01-01 00:00:00 1.0 1 cd0ae68d6ace43033b49ee0390c3583e 1 355 2016-01-01 00:00:00 1.0 1 cd0ae68d6ace43033b49ee0390c3583e 1 840 2016-01-01 00:00:00 1.0 0 cd0ae68d6ace43033b49ee0390c3583e As you can see, the table contains our models' predictions for a given entity and date. And do you notice the field matrix_uuid ? Doesn't it look similar to the fields from above that gave us the names of our training matrices? In fact, it is the same. You can find the test matrix that was used to make this prediction under TRIAGE_OUTPUT_PATH/matrices/{matrix_uuid}.csv .","title":"{test, train}_results.predictions"},{"location":"dirtyduck/ml_governance/#test-train_resultsfeature_importances","text":"This tables store the feature importance of all the models.","title":"{test, train}_results.feature_importances"},{"location":"dirtyduck/ml_governance/#footnotes","text":"1 Literally from the configuration file. If you modify something it will generate a new hash. Handle with care!","title":"Footnotes"},{"location":"dirtyduck/problem_description/","text":"Description of the problem # This tutorial aims to introduce the reader to Triage , a machine learning modeling tool built by the Center for Data Science and Public Policy . We will use the well-known Chicago Food Inspections dataset 1 . We will present the two problems that Triage was built to model 2 : Resource Prioritization Systems (also known as an inspections problem ) 3 and Early Warning Systems 4 . Resource Prioritization Systems # In an ideal world, inspectors would visit every food facility, every day 5 to ensure it meets safety standards. But the real world doesn't have enough inspectors for that to happen, so the city needs to decide how to allocate its limited inspection workforce to find and remediate as many establishments with food hazards as possible. Assuming the city can inspect n n facilities in the next X X period of time, they can define the problem as: Which n n facilities will have a food violation in the following X X period of time? If our inspection workforce is really limited, we should probably just target the most serious violations. Then we'd define the problem as: Which n n facilities will have a critical or serious violation in the following X X period of time? If you want to continue to this case studie click here Early Warning Systems # Using the same dataset ( Chicago Food Inspections dataset ), facility owners or managers would pose the machine learning (ML) problem as an early warning problem. They'd like to know whether an inspector is going to visit their facility so they can prepare for it. They can define the problem as: Will my facility be inspected in the next X X period of time? If you want to continue to this case studie click here Important Note that in both case studies, resource prioritization and early warning systems we are defining a period of time in which the event will potentially happen. What do they have in common? # For either problem, X X could be a day, a week, month, a quarter, a year, 56 days, or some other time period. Without going into much detail, both problems use data where each row describes an event in which an entity was involved, and each event has a specific outcome or result. The entity for both inspection prioritizations and early warnings in this tutorial is a food facility , and the event is an inspection. But the outcome differs. For inspections the outcome is whether the inspection failed or major violation was found , while for early warning the outcome is whether the facility was inspected . How do they differ? # Besides the obvious (e.g. the label), these ML's problem formulations have very different internal structure: Fot the EIS problem all of the entities of interest in a given period of time have a label. The Inspections problem does not have that luxury. Given all of the existing entities of interest only a fraction are inspected which means that only the inspected facilities will have a label ( True/False ) since these are the only entities with a known outcome (e.g a major violation was discovered during the inspection), but all of the remaining ones will not have a label. This will be reflected, in the training matrices since you only train on the facilities that were inspected (so you will have less rows in them). Another impact will be in the metrics. You will need to be very careful about interpreting the metrics in an inspections problem. Finally, when you are designing the field validation of your model, you need to take in account selection bias . If not, you will be inspecting the same facilities over and over 6 . What's next? # Learn more about early warning systems Learn more about resource prioritization systems Several examples use this dataset, such as City of Chicago Food Inspection Forecasting , PyCon 2016 keynote: Built in Super Heroes , and PyData 2016: Forecasting critical food violations at restaurants using open data . \u21a9 It is also possible to do \"visit-level prediction\" type of ML problem. \u21a9 Examples include Predictive Enforcement of Hazardous Waste Regulations and Targeting Proactive Inspections for Lead Hazards . \u21a9 Examples include Increasing High School Graduation Rates: Early Warnings and Predictive Systems , Building Data-Driven Early Intervention Systems for Police Officers , and Data-Driven Justice Initiative: Identifying Frequent Users of Multiple Public Systems for More Effective Early Assistance . \u21a9 Defined as \"bakery, banquet hall, candy store, caterer, coffee shop, day care center (for ages less than 2), day care center (for ages 2 \u2013 6), day care center (combo, for ages less than 2 and 2 \u2013 6 combined), gas station, Golden Diner, grocery store, hospital, long term care center(nursing home), liquor store, mobile food dispenser, restaurant, paleteria, school, shelter, tavern, social club, wholesaler, or Wrigley Field Rooftop\" ( source ). \u21a9 This points is particularly acute: Imagine the scenario in which the inspections problem is crime prediction in order to send cops (inspectors)to that \"risky\" area (facilities)\u2026 \u21a9","title":"Problem description"},{"location":"dirtyduck/problem_description/#description-of-the-problem","text":"This tutorial aims to introduce the reader to Triage , a machine learning modeling tool built by the Center for Data Science and Public Policy . We will use the well-known Chicago Food Inspections dataset 1 . We will present the two problems that Triage was built to model 2 : Resource Prioritization Systems (also known as an inspections problem ) 3 and Early Warning Systems 4 .","title":"Description of the problem"},{"location":"dirtyduck/problem_description/#resource-prioritization-systems","text":"In an ideal world, inspectors would visit every food facility, every day 5 to ensure it meets safety standards. But the real world doesn't have enough inspectors for that to happen, so the city needs to decide how to allocate its limited inspection workforce to find and remediate as many establishments with food hazards as possible. Assuming the city can inspect n n facilities in the next X X period of time, they can define the problem as: Which n n facilities will have a food violation in the following X X period of time? If our inspection workforce is really limited, we should probably just target the most serious violations. Then we'd define the problem as: Which n n facilities will have a critical or serious violation in the following X X period of time? If you want to continue to this case studie click here","title":"Resource Prioritization Systems"},{"location":"dirtyduck/problem_description/#early-warning-systems","text":"Using the same dataset ( Chicago Food Inspections dataset ), facility owners or managers would pose the machine learning (ML) problem as an early warning problem. They'd like to know whether an inspector is going to visit their facility so they can prepare for it. They can define the problem as: Will my facility be inspected in the next X X period of time? If you want to continue to this case studie click here Important Note that in both case studies, resource prioritization and early warning systems we are defining a period of time in which the event will potentially happen.","title":"Early Warning Systems"},{"location":"dirtyduck/problem_description/#what-do-they-have-in-common","text":"For either problem, X X could be a day, a week, month, a quarter, a year, 56 days, or some other time period. Without going into much detail, both problems use data where each row describes an event in which an entity was involved, and each event has a specific outcome or result. The entity for both inspection prioritizations and early warnings in this tutorial is a food facility , and the event is an inspection. But the outcome differs. For inspections the outcome is whether the inspection failed or major violation was found , while for early warning the outcome is whether the facility was inspected .","title":"What do they have in common?"},{"location":"dirtyduck/problem_description/#how-do-they-differ","text":"Besides the obvious (e.g. the label), these ML's problem formulations have very different internal structure: Fot the EIS problem all of the entities of interest in a given period of time have a label. The Inspections problem does not have that luxury. Given all of the existing entities of interest only a fraction are inspected which means that only the inspected facilities will have a label ( True/False ) since these are the only entities with a known outcome (e.g a major violation was discovered during the inspection), but all of the remaining ones will not have a label. This will be reflected, in the training matrices since you only train on the facilities that were inspected (so you will have less rows in them). Another impact will be in the metrics. You will need to be very careful about interpreting the metrics in an inspections problem. Finally, when you are designing the field validation of your model, you need to take in account selection bias . If not, you will be inspecting the same facilities over and over 6 .","title":"How do they differ?"},{"location":"dirtyduck/problem_description/#whats-next","text":"Learn more about early warning systems Learn more about resource prioritization systems Several examples use this dataset, such as City of Chicago Food Inspection Forecasting , PyCon 2016 keynote: Built in Super Heroes , and PyData 2016: Forecasting critical food violations at restaurants using open data . \u21a9 It is also possible to do \"visit-level prediction\" type of ML problem. \u21a9 Examples include Predictive Enforcement of Hazardous Waste Regulations and Targeting Proactive Inspections for Lead Hazards . \u21a9 Examples include Increasing High School Graduation Rates: Early Warnings and Predictive Systems , Building Data-Driven Early Intervention Systems for Police Officers , and Data-Driven Justice Initiative: Identifying Frequent Users of Multiple Public Systems for More Effective Early Assistance . \u21a9 Defined as \"bakery, banquet hall, candy store, caterer, coffee shop, day care center (for ages less than 2), day care center (for ages 2 \u2013 6), day care center (combo, for ages less than 2 and 2 \u2013 6 combined), gas station, Golden Diner, grocery store, hospital, long term care center(nursing home), liquor store, mobile food dispenser, restaurant, paleteria, school, shelter, tavern, social club, wholesaler, or Wrigley Field Rooftop\" ( source ). \u21a9 This points is particularly acute: Imagine the scenario in which the inspections problem is crime prediction in order to send cops (inspectors)to that \"risky\" area (facilities)\u2026 \u21a9","title":"What's next?"},{"location":"dirtyduck/triage_intro/","text":"Triage # Predictive analytics projects require coordinating many tasks, such as feature generation, classifier training, evaluation, and list generation. Each of these tasks is complicated in its own right, but it also needs to be combined with the other tasks throughout the course of the project. DSaPP built triage to facilitate the creation of supervised learning models, in particular binary classification models with a strong temporal component in the data. The dataset's temporal component mainly affects two modeling steps: feature creation (you need to be careful to avoid leaking information from the future through your features ) and hyperparameter selection. triage solves both by splitting the data into temporal blocks and automating temporal cross-validation (TCC) and the feature generation. triage uses the concept of an experiment . An experiment consists of a series of steps that aim to generate a good model for predicting the label of an entity in the data set. The steps are data time-splitting , label generation , feature generation , matrix creation , model training , predictions , and model evaluation . In each of these steps, triage will handle the temporal nuances of the data. Nowadays triage will help you to select the best model ( model selection ) and it allows you to explore and understand the behavior of your models using post-modeling techniques. You need to specify (via a configuration file) how you want to split your data temporally, which combination of machine learning algorithms and their hyperparameters you'd like to use, which kinds of features you want to generate, which subsets of those features you want to try in each model, and which metrics you'd like to use to evaluate performance and provide some criteria to select the best model. An experiment run consists in fitting every combination of algorithm, hyperparameters, and feature subsets to the temporally split data and evaluating their predictive performance on future data splits using the user's metrics. triage calls a unique combination of algorithm, hyperparameters, and feature subsets a model_group and a model group fit to a specific data matrix a model . Our data typically span multiple time periods, so triage fits multiple models for each model group. triage is simple to use, but it contains a lot of complex concepts that we will try to clarify in this section. First we will explain how to run triage , and then we will create a toy experiment that helps explain triage's main concepts. Triage interface # To run a triage experiment, you need the following: A database with the data that you want to model. In this tutorial, the credentials are part of the DATABASE_URL environment variable triage installed in your environment. You can verify that triage is indeed installed if you type in bastion : triage -h An experiment config file . This is where the magic happens. We will discuss this file at length in this section of the tutorial. We are providing a docker container, bastion , that executes triage experiments. You already had the database (you were working on it the last two sections of this tutorial, remember?). So, like a real project, you just need to worry about the experiment configuration file . In the following section of the tutorial we will use a small experiment configuration file located at <../triage/experiments/simple_test_skeleton.yaml>. We will show you how to setup the experiment while explaining the inner workings of triage . We will modify the configuration file to show the effects of the configuration parameters. If you want to follow along, we suggest you copy that file and modify by yourself. You can run that experiment with: # Remember to run this in bastion NOT in your laptop! triage experiment experiments/simple_test_skeleton.yaml Every time you modify the configuration file and see the effects, you should execute the experiment again using the previous command. A simple triage experiment # A brief recap of Machine Learning # Triage helps you to run a Machine learning experiment. An experiment in this context means the use of Machine Learning to explore a dynamic system in order to do some predictions about it. Before execute the any ML experiment, you need to define some boundaries : Which are the entities that you want to study? What will you want to know about them? In DSaPP, we build ML systems that aim to have social impact, i.e. help government offices, NGOs or other agents to do their job better. \"Do their job better\" means increase their reach (e.g. identify correctly more entities with some characteristics) using more efficiently their (scarce) resources (e.g. inspectors, medics, money, etc). With this optic, the boundaries are: Cohort: Which are the entities that you want to reach? Label: What will you want to know about them? Label timespan: In what time period? Update frequency: How frequently do you want to intervene? List size: How many resources do you have to intervene? Triage's experiment configuration file structures this information. Cohorts, labels, event dates and as of dates # We will use the inspections prioritization as a narrative to help clarify these concepts: Which are the entities that you want to reach? : Active facilities, i.e. facilities that exists at the day of the planning inspections. We don't want to waste city resources (inspectors time) going to facilities that are out of business. What will you want to know about them?: Will those facilities fail the inspection? In what time period?: Will those facilities fail the inspection in the following month? How frequently do you want to intervene?: Every month. How many resources do you have to intervene?: We only have one inspector, so, one inspection per month To exemplify and explain the inner workings of triage in this scenario, we will use a subset of the semantic.events table with the following facilities (i.e. imagine that Chicago only has this three facilities): select entity_id , license_num , facility_aka , facility_type , activity_period from semantic . entities where license_num in ( 1596210 , 1874347 , 1142451 ) order by entity_id asc ; entity id license num facility aka facility type activity period 229 1596210 food 4 less grocery store [2010-01-08,) 355 1874347 mcdonalds restaurant [2010-01-12,2017-11-09) 840 1142451 jewel foodstore # 3345 grocery store [2010-01-26,) The first thing triage does when executes the experiment, is split the time that the data covers in blocks considering the time horizon for the label ( Which facilities will fail an inspection in the following month? in this scenario of inspection prioritization 1 ) . This time horizon is calculated from a set of specific dates ( as_of_date in triage parlance) that divide the blocks in past (for training the model) and future (for testing the model). The set of as_of_dates is ( mainly ) calculated from the label timespan and the update frequency 2 . The as of date is not the event date . The event date occurred when the facility was inspected. The as of date is when the planning of the future facilities to be inspected happens. triage will create those labels using information about the outcome of the event 3 , taking into account the temporal structure of the data. In our example: if a facility is inspected is an event, and whether it fails the inspection (outcome true ) or not (outcome false ). For a given entity on a given as of date , triage asks whether there's an outcome in the future time horizon. If so, triage will generate a label for that specific entity on that as of date . For this example, the label will be if given an as of date (e.g. January first, 2014), the facility will have a failed inspection in the following year. The following example hopefully will clarify the difference between outcome and label . We will focus on events (inspections) that happened in the year of 2014. select date , entity_id , ( result = 'fail' ) as outcome from semantic . events where '[2014-01-01, 2015-01-01]' :: daterange @> date and entity_id in ( 229 , 355 , 840 ) order by date asc ; date entity id outcome 2014-01-14 840 f 2014-02-04 229 f 2014-02-24 840 t 2014-03-05 840 f 2014-04-10 355 t 2014-04-15 229 f 2014-04-18 355 f 2014-05-06 840 f 2014-08-28 355 f 2014-09-19 229 f 2014-09-30 355 t 2014-10-10 355 f 2014-10-31 840 f We can observe that the facilities had several inspections, but in that timeframe 362 y 859 had failed inspections. Continuing the narrative, from the perspective of the day of 2014-01-01 ( as of date ), those facilities will have positive label . We can express that in a query and getting the labels for that as of date : select '2014-01-01' as as_of_date , entity_id , bool_or ( result = 'fail' ):: integer as label from semantic . events where '2014-01-01' :: timestamp <= date and date < '2014-01-01' :: timestamp + interval '1 year' and entity_id in ( 229 , 355 , 840 ) group by entity_id ; as of date entity id label 2014-01-01 229 0 2014-01-01 355 1 2014-01-01 840 1 Note that ee transform the label to integer, since the machine learning algorithms only work with numeric data. We also need a way to store the state of each entity. We can group entities in cohorts defined by the state. The cohort can be used to decide which facilities you want to predict on (i.e. include in the ML train/test matrices). The rationale of this comes from the need to only predict for entities in a particular state: Is the restaurant new? Is this facility on this zip code ? Is the facility \"active\"? 4 We will consider a facility as active if a given as of date is in the interval defined by the start_date and end_date . select '2018-01-01' :: date as as_of_date , entity_id , activity_period , case when activity_period @> '2018-01-01' :: date -- 2018-01-01 is as of date then 'active' :: text else 'inactive' :: text end as state from semantic . entities where entity_id in ( 229 , 355 , 840 ); as of date entity id activity period state 2018-01-01 229 [2010-01-08,) active 2018-01-01 355 [2010-01-12,2017-11-09) inactive 2018-01-01 840 [2010-01-26,) active Triage will use a simple modification of the queries that we just discussed for automate the generation of the cohorts and labels for our experiment. Experiment configuration file # The experiment configuration file is used to create the experiment object. Here, you will specify the temporal configuration, the features to be generated, the labels to learn, and the models that you want to train in your data. The configuration file is a yaml file with the following main sections: temporal_config : Temporal specification of the data, used for creating the blocks for temporal crossvalidation. cohort_config : Using the state of the entities, define (using sql ) cohorts to filter out objects that shouldn't be included in the training and prediction stages. This will generate a table call cohort_{experiment_hash} label_config : Specify (using sql ) how to generate labels from the event's outcome . A table named labels_{experiment_hash} will be created. feature_aggregation : Which spatio-temporal aggregations of the columns in the data set do you want to generate as features for the models? model_group_keys : How do you want to identify the model_group in the database (so you can run analysis on them)? grid_config : Which combination of hyperparameters and algorithms will be trained and evaluated in the data set? scoring : Which metrics will be calculated? Two of the more important (and potentially confusing) sections are temporal_config and feature_generation . We will explain them in detail in the next sections. Temporal crossvalidation # Cross validation is a common technique to select a model that generalizes well to new data. Standard cross validation randomly splits the training data into subsets, fits models on all but one, and calculates the metric of interest (e.g. precision/recall) on the one left out, rotating through the subsets and leaving each out once. You select the model that performed best across the left-out sets, and then retrain it on the complete training data. Unfortunately, standard cross validation is inappropriate for most real-world data science problems. If your data have temporal correlations, standard cross validation lets the model peek into the future, training on some future observations and testing on past observations. To avoid this problem, you should design your training and testing to mimic how your model will be used, making predictions only using the data that would be available at that time (i.e. from the past). In temporal crossvalidation, rather than randomly splitting the dataset into training and test splits, temporal cross validation splits the data by time. triage uses the timechop library for this purpose. Timechop will \"chop\" the data set in several temporal blocks. These blocks are then used for creating the features and matrices for training and evaluation of the machine learning models. Assume we want to select which restaurant (of two in our example dataset) we should inspect next year based on its higher risk of violating some condition. Also assume that the process of picking which facility is repeated every year on January 1 st 5 Following the problem description template given in Section Description of the problem to solve , the question that we'll attempt to answer is: Which facility ( n=1 n=1 ) is likely to violate some inspected condition in the following year ( X=1 X=1 )? The traditional approach in machine learning is splitting the data in training and test datasets. Train or fit the algorithm on the training data set to generate a train model and test or evaluate the model on the test data set. We will do the same here, but, with the help of timechop we will take in account the time: We will fit models on training set up to 2014-01-01 and see how well those models would have predicted 2015; fit more models on training set up to 2015-01-01 and see how well those models would have predicted 2016; and so on. That way, we choose models that have historically performed best at our task, forecasting. It\u2019s why this approach is sometimes called evaluation on a rolling forecast origin because the origin at which the prediction is made rolls forward in time. 6 The data at which the model will do the predictions is denominated as as of date in triage ( as of date = January first in our example). The length of the prediction time window (1 year) is called label span . Training and predicting with a new model as of date (every year) is the model update frequency . Because it's inefficient to calculate by hand all the as of dates or prediction points, timechop will take care of that for us. To do so, we need to specify some more constraints besides the label span and the model update frequency : What is the date range covered by our data? What is the date range in which we have information about labels? How frequently do you receive information about your entities? How far in the future you want to predict? How much of the past data do you want to use? With this information, timechop will calculate as-of train and test dates from the last date in which you have label data, using the label span in both test and train sets, plus the constraints just mentioned. In total timechop uses 11 configuration parameters 7 . There are parameters related to the boundaries of the available data set: feature_start_time : data aggregated into features begins at this point (earliest date included in features) feature_end_time : data aggregated into features is from before this point (latest date included in features) label_start_time : data aggregated into labels begins at this point (earliest event date included in any label (event date >= label start time ) label_end_time : data aggregated is from before this point (event date < label end time to be included in any label) Parameters that control the labels ' time horizon on the train and test sets: training_label_timespans : how much time is covered by training labels (e.g., outcomes in the next 3 days? 2 months? 1 year?) (training prediction span) test_label_timespans : how much time is covered by test prediction (e.g., outcomes in the next 3 days? 2 months? 1 year?) (test prediction span) These parameters will be used with the outcomes table to generate the labels . In an early warning setting, they will often have the same value. For inspections prioritization , this value typically equals test_durations and model_update_frequency . Parameters related about how much data we want to use, both in the future and in the past relative to the as-of date : test_durations : how far into the future should a model be used to make predictions (test span) NOTE : in the typical case of wanting a single prediction set immediately after model training, this should be set to 0 days For early warning problems, test_durations should equal model_update_frequency . For inspection prioritization, organizational process determines the value: how far out are you scheduling for? The equivalent of test_durations for the training matrices is max_training_histories : max_training_histories : the maximum amount of history for each entity to train on (early matrices may contain less than this time if it goes past label/feature start times). If patterns have changed significantly, models trained on recent data may outperform models trained on a much lengthier history. Finally, we should specify how many rows per entity_id in the train and test matrix: training_as_of_date_frequencies : how much time between rows for a single entity in a training matrix (list time between rows for same entity in train matrix). test_as_of_date_frequencies : how much time between rows for a single entity in a test matrix (time between rows for same entity in test matrix). The following images (we will show how to generate them later) shows the time blocks created by several temporal configurations. We will change a parameter at a time so you could see how it affects the resulting blocks. If you want to try the modifications (or your own) and generate the temporal blocks images run the following (they'll be generated in <./images/>): # Remember to run this in bastion NOT in laptop's shell! triage experiment experiments/simple_test_skeleton.yaml --show-timechop {feature, label}_{end, start}_time The image below shows these {feature, label}_start_time are equal, as are the {feature, label}_end_time . These parameters show in the image as dashed vertical black lines. This setup will be our baseline example. The plot is divided in two horizontal lines (\"Block 0\" and \"Block 1\"). Each line is divided by vertical dashed lines \u2013 the grey lines outline the boundaries of the data for features and data for labels, which in this image coincide. The black dash lines represent the beginning and the end of the test set. In \"Block 0\" those lines correspond to 2017 and 2018 , and in \"Block 1\" they correspond to 2016 and 2017 . The shaded areas (in this image there is just one per block, but you will see other examples below) represents the span of the as of dates . They start with the oldest as of date and end with the latest. Each line inside that area represents the label span. Those lines begin at the as of date . At each as of date , timechop generates each entity's features (from the past) and labels (from the future). So in the image, we will have two sets of train/test datasets. Each facility will have 13 rows in \"Block 0\" and 12 rows in \"Block 1\". The trained models will predict the label using the features calculated for that test set as of date . The single line represents the label's time horizon in testing. This is the temporal configuration that generated the previous image: temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '1y' In that configuration the date ranges of features and labels are equal, but they can be different (maybe you have more data for features that data for labels) as is shown in the following image and in their configuration parameters. temporal_config : feature_start_time : '2010-01-01' # <------- The change happened here! feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '1y' model_update_frequency From our baseline temporal_config example ( 102 ), we will change how often we want a new model, which generates more time blocks (if there are time-constrained data, obviously). temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '6month' # <------- The change happened here! training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '1y' max_training_histories With this parameter you could get a growing window for training (depicted in 110 ) or as in all the other examples, fixed training windows . temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '10y' # <------- The change happened here! _as_of_date_frequencies and test_durations temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '3month' # <------- The change happened here! test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '10y' Now, change test_as_of_date_frequencies : temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '3month' <------- The change happened here! max_training_histories : '10y' Nothing changed because the test set doesn't have \"space\" to allow more spans. The \"space\" is controlled by test_durations , so let's change it to 6month : temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '6month' <------- The change happened here! test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '10y' So, now we will move both parameters: test_durations , test_as_of_date_frequencies temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '6month' <------- The change happened here! test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '3month' <------- and also here! max_training_histories : '10y' _label_timespans temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '3month' ] <------- The change happened here! test_as_of_date_frequencies : '1month' max_training_histories : '10y' temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '3month' ] <------- The change happened here! training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '10y' That's it! Now you have the power to bend time! 8 With the time blocks defined, triage will create the labels and then the features for our train and test sets. We will discuss features in the following section. Feature generation # We will show how to create features using the experiments config file . triage uses collate for this. 9 The collate library controls the generation of features (including the imputation rules for each feature generated) using the time blocks generated by timechop . Collate helps the modeler create features based on spatio-temporal aggregations into the as of date . Collate generates SQL queries that will create features per each as of date . As before, we will try to mimic what triage does behind the scenario. Collate will help you to create features based on the following template: For a given as of date , how the aggregation function operates into a column taking into account a previous time interval and some attributes . Two possible features could be framed as: As of 2016-01-01, how many inspections has each facility had in the previous 6 months? and As of 2016-01-01, how many \"high risk\" findings has the facility had in the previous 6 months? In our data, that date range (between 2016-01-01 and 2015-07-01) looks like: select event_id , date , entity_id , risk from semantic . events where date <@ daterange (( '2016-01-01' :: date - interval '6 months' ):: date , '2016-01-01' ) and entity_id in ( 229 , 355 , 840 ) order by date asc ; event id date entity id risk 1561324 2015-07-17 840 high 1561517 2015-07-24 840 high 1562122 2015-08-12 840 high 1547403 2015-08-20 229 high 1547420 2015-08-28 229 high 1547448 2015-09-14 355 medium 1547462 2015-09-21 355 medium 1547504 2015-10-09 355 medium 1547515 2015-10-16 355 medium 1583249 2015-10-21 840 high 1583577 2015-10-28 840 high 1583932 2015-11-04 840 high We can transform those data to two features: number_of_inspections and flagged_as_high_risk : select entity_id , '2016-01-01' as as_of_date , count ( event_id ) as inspections , count ( event_id ) filter ( where risk = 'high' ) as flagged_as_high_risk from semantic . events where date <@ daterange (( '2016-01-01' :: date - interval '6 months' ):: date , '2016-01-01' ) and entity_id in ( 229 , 355 , 840 ) group by grouping sets ( entity_id ); entity id as of date inspections flagged as high risk 229 2016-01-01 2 2 355 2016-01-01 4 0 840 2016-01-01 6 6 This query is making an aggregation . Note that the previous SQL query has five parts: The filter (( risk = 'high')::int ) The aggregation function ( count() ) The name of the resulting transformation ( flagged_as_high_risk ) The context in which it is aggregated (by entity_id ) The date range (between 2016-01-01 and 6 months before) What about if we want to add proportions and totals of failed and passed inspections? select entity_id , '2016-01-01' as as_of_date , count ( event_id ) as inspections , count ( event_id ) filter ( where risk = 'high' ) as flagged_as_high_risk , count ( event_id ) filter ( where result = 'pass' ) as passed_inspections , round ( avg (( result = 'pass' ):: int ), 2 ) as proportion_of_passed_inspections , count ( event_id ) filter ( where result = 'fail' ) as failed_inspections , round ( avg (( result = 'fail' ):: int ), 2 ) as proportion_of_failed_inspections from semantic . events where date <@ daterange (( '2016-01-01' :: date - interval '6 months' ):: date , '2016-01-01' ) and entity_id in ( 229 , 355 , 840 ) group by grouping sets ( entity_id ) entity id as of date inspections flagged as high risk passed inspections proportion of passed inspections failed inspections proportion of failed inspections 229 2016-01-01 2 2 1 0.50 1 0.50 355 2016-01-01 4 0 1 0.25 2 0.50 840 2016-01-01 6 6 4 0.67 2 0.33 But what if we want to also add features for \"medium\" and \"low\" risk? And what would the query look like if we want to use several time intervals, like 3 months , 5 years , etc? What if we want to contextualize this by location? Plus we need to calculate all these features for several as of dates and manage the imputation strategy for all of them!!! You will realize that even with this simple set of features we will require very complex SQL to be constructed. But fear not. triage will automate that for us! The following blocks of code represent a snippet of triage 's configuration file related to feature aggregation. It shows the triage syntax for the inspections feature constructed above: feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates : - quantity : total : \"*\" imputation : count : type : 'zero_noflag' metrics : - 'count' intervals : [ '6month' ] groups : - 'entity_id' feature_aggregations is a yaml list 10 of feature groups construction specification or just feature group . A feature group is a way of grouping several features that share intervals and groups . triage requires the following configuration parameter for every feature group : prefix : This will be used for name of the feature created from_obj : Represents a TABLE object in PostgreSQL . You can pass a table like in the example above ( semantic.events ) or a SQL query that returns a table. We will see an example of this later. triage will use it like the FROM clause in the SQL query. knowlege_date_column : Column that indicates the date of the event. intervals : A yaml list. triage will create one feature per interval listed. groups : A yaml list of the attributes that we will use to aggregate. This will be translated to a SQL GROUP BY by triage . The last section to discuss is imputation . Imputation is very important step in the modeling, and you should carefully think about how you will impute the missing values in the feature. After deciding the best way of impute each feature, you should avoid leakage (For example, imagine that you want to impute with the mean one feature. You could have leakage if you take all the values of the column, including ones of the future to calculate the imputation). We will return to this later in this tutorial. Collate is in charge of creating the SQL agregation queries. Another way of thinking about it is that collate encapsulates the FROM part of the query ( from_obj ) as well as the GROUP BY columns ( groups ). triage ( collate ) supports two types of objects to be aggregated: aggregates and categoricals (more on this one later) 11 . The aggregates subsection represents a yaml list of features to be created. Each element on this represents a column ( quantity , in the example, the whole row * ) and an alias ( total ), defines the imputation strategy for NULLs , and the metric refers to the aggregation function to be applied to the quantity ( count ). triage will generate the following (or a very similar one), one per each combination of interval \u00d7 groups \u00d7 quantity : select metric ( quantity ) as alias from from_obj where as_of_date <@ ( as_of_date - interval , as_of_date ) group by groups With the previous configuration triage will generate 1 feature with the following name: 12 inspections_entity_id_6month_total_count All the features of that feature group (in this case only 1) will be stored in the table. features.inspections_aggregation_imputed In general the names of the generated tables are constructed as follows: schema.prefix_group_aggregation_imputed NOTE : the outputs are stored in the features schema. Inside each of those new tables, the feature name will follow this pattern: prefix_group_interval_alias_aggregation_operation If we complicate a little the above configuration adding new intervals: feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates : - # number of inspections quantity : total : \"*\" imputation : count : type : 'zero_noflag' metrics : [ 'count' ] intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' You will end with 5 new features , one for each interval (5) \u00d7 the only aggregate definition we have. Note the weird all in the intervals definition. all is the time interval between the feature_start_time and the as_of_date . triage also supports categorical objects. The following code adds a feature for the risk flag. feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates : - # number of inspections quantity : total : \"*\" imputation : count : type : 'zero_noflag' metrics : [ 'count' ] intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'risks' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero' categoricals : - column : 'risk' choice_query : 'select distinct risk from semantic.events' metrics : - 'sum' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' There are several changes. First, the imputation strategy in this new feature group is for every categorical features in that feature group (in that example only one). The next change is the type: instead of aggregates , it's categoricals . categoricals define a yaml list too. Each categorical feature needs to define a column to be aggregated and the query to get all the distinct values. With this configuration, triage will generate two tables, one per feature group . The new table will be features.risks_aggregation_imputed . This table will have more columns: intervals (5) \u00d7 groups (1) \u00d7 metric (1) \u00d7 features (1) \u00d7 number of choices returned by the query . The query: select distinct risk from semantic . events ; risk \u00a4 medium high low returns 4 possible values (including NULL ). When dealing with categorical aggregations you need to be careful. Could be the case that in some period of time, in your data, you don't have all the possible values of the categorical variable. This could cause problems down the road. Triage allows you to specify the possible values ( choices ) of the variable. Instead of using choice_query , you could use choices as follows: feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates : - # number of inspections quantity : total : \"*\" imputation : count : type : 'mean' metrics : [ 'count' ] intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'risks' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero' categoricals : - column : 'risk' choices : [ 'low' , 'medium' , 'high' ] metrics : - 'sum' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' In both cases triage will generate 20 new features, as expected. The features generated from categorical objects will have the following pattern: prefix_group_interval_column_choice_aggregation_operation So, risks_entity_id_1month_risk_medium_sum will be among our new features in the last example. As a next step, let's investigate the effect of having several elements in the groups list. feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates : - # number of inspections quantity : total : \"*\" imputation : count : type : 'mean' metrics : [ 'count' ] intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'risks' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero' categoricals : - column : 'risk' choices : [ 'low' , 'medium' , 'high' ] metrics : - 'sum' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - 'zip_code' The number of features created in the table features.risks_aggregation_imputed is now 60 ( intervals (5) \u00d7 groups (2) \u00d7 metric (2) \u00d7 features (1) \u00d7 number of choices (3). Triage will add several imputation flag (binary) columns per feature. Those columns convey information about if that particular value was imputed or not . So in the last counting we need to add 20 more columns to a grand total of 80 columns. Imputation # Triage currently supports the following imputation strategies: mean: The mean value of the feature. constant: Fill with a constant (you need to provide the constant value). zero: Same that the previous one, but the constant is zero. zero noflag : Sometimes, the absence (i.e. a NULL) doesn't mean that the value is missing, that actually means that the event didn't happen to that entity. For example a NULL in the inspections_entity_id_1month_total_count column in features.inspections_aggreagtion_imputed doesn't mean that the value is missing, it means that zero inspections happen to that facility in the last month. Henceforth, the flag column is not needed. Only for aggregates: binary mode : Takes the mode of a binary feature Only for categoricals:: null category : Just flag null values with the null category column and finally, if you are sure that is not possible to have NULLS: error: Raise an exception if ant null values are encountered. Feature groups strategies # Another interesting thing that triage controls is how many feature groups are used in the machine learning grid. This would help you to understand the effect of using different groups in the final performance of the models. In simple_test_skeleton.yaml you will find the following blocks: feature_group_definition : prefix : - 'results' - 'risks' - 'inspections' feature_group_strategies : [ 'all' ] This configuration adds to the number of model groups to be created. The possible feature group strategies are: all : All the features groups are used. leave-one-out : All the combinations of: \"All the feature groups except one are used\". leave-one-in : All the combinations of \"One feature group except the rest is used\" all-combinations : All the combinations of feature groups In order to clarify these concepts, let's use simple_test_skeleton.yaml configuration file. In it there are three feature groups: inspections , results , risks . Using all will create just one set containg all the features of the three feature groups: {inspections, results, risks} If you modify feature_group_strategies to ['leave-one-out'] : the following sets will be created: {inspections, results}, {inspections, risks}, {results, risks} Using the leave-one-in strategy: {inspections}, {results}, {risks} Finally choosing all-combinations : {inspections}, {results}, {risks}, {inspections, results} , {inspections, risks}, {results, risks}, {inspections, results, risks} Controlling the size of the tables # This section is a little technical, you can skip it if you fell like it. By default, triage will use the biggest column type for the features table ( integer , numeric , etc). This could lead to humongous tables, with sizes several hundred of gigabytes. Triage took that decision, because it doesn't know anything about the possible values of your data (e.g. Is it possible to have millions of inspections in one month? or just a few dozens?). If you are facing this difficulty, you can force triage to cast the column in the features table. Just add coltype to the aggregate/categorical block: aggregates : - quantity : total : \"*\" metrics : [ 'count' ] coltype : 'smallint' The Grid # Before applying Machine Learning to your dataset you don't know which combination of algorithm and hyperparameters will be the best given a specific matrix. Triage approaches this problem exploring a algorithm + hyperparameters + feature groups grid. At this time, this exploration is a exhaustive one, i.e. it covers the complete grid, so you would get (number of algorithms) \\times \\times (number of hyperparameters) \\times \\times (number of feature group strategies) models groups. The number of models trained is (number of model groups) \\times \\times (number of time splits). In our simple experiment the grid is very simple: grid_config : 'sklearn.dummy.DummyClassifier' : strategy : [ most_frequent ] Just one algorithm and one hyperparameter (also we have only one feature group strategy: all ), and two time splits. So we will get 2 models, 1 model group. Keep in mind that the grid is providing more than way to select a model. You can use the tables generated by the grid (see section Machine Learning Governance ) and analyze and understand your data. In other words, analyzing the results (evaluations, predictions, hyperparameter space, etc.) is like applying Data mining concepts to your data using Machine learning. We will return to this when we apply post modeling to our models. Audition # Audition is a tool for helping you select a subset of trained classifiers from a triage experiment. Often, production-scale experiments will come up with thousands of trained models, and sifting through all of those results can be time-consuming even after calculating the usual basic metrics like precision and recall. You will be facing questions as: Which metrics matter most? Should you prioritize the best metric value over time or treat recent data as most important? Is low metric variance important? The answers to questions like these may not be obvious. Audition introduces a structured, semi-automated way of filtering models based on what you consider important. Post-modeling # As the name indicates, postmodeling occurs after you have modeled (potentially) thousands of models (different hyperparameters, different time windows, different algorithms, etc), and using audition you pre selected a small number of models. Now, with the postmodeling tools you will be able to select your final model for production use. Triage's postmodeling capabilities include: Show the score distribution Compare the list generated by a set of models Compare the feature importance between a set of models Diplay the probability calibration curves Analyze the errors using a decision tree trained on the errors of the model. Cross-tab analysis Bias analysis If you want to see Audition and Postmodeling in action, we will use them after Inspections and EIS modeling . Final cleaning # In the next section we will start modeling, so it is a good idea to clean the {test, train}_results schemas and have a fresh start: select nuke_triage (); nuke triage triage was send to the oblivion. Long live to triage! triage also creates a lot of files (we will see why in the next section). Let's remove them too. rm -r /triage/matrices/* rm -r /triage/trained_models/* Where to go from here... # If you haven't done so already, our dirty duck tutorial is a good way to geet up and running with some sample data. If you're ready to get started with your own data, check out the suggested project workflow for some tips about how to iterate and tune the pipeline for your project. Footnotes # 1 Would be my restaurant inspected in the following month? in the case of an early warning case. 2 It's a little more complicated than that as we will see. 3 All events produce some outcome . In theory every event of interest in stored in a database. These events are immutable : you can't (shouldn't) change them (they already happen). 4 We could consider different states, for example: we can use the column risk as an state. Another possibility is define a new state called failed that indicates if the facility failed in the last time it was inspected. One more: you could create cohorts based on the facility_type. 5 The city in this toy example has very low resources. 6 See for example: https://robjhyndman.com/hyndsight/tscv/ 7 I know, I know. And in order to cover all the cases, we are still missing one or two parameters, but we are working on it. 8 Obscure reference to the \" Avatar: The Last Airbender \" cartoon series. I'm sorry. 9 collate is to feature generation what timechop is to date temporal splitting 10 triage uses a lot of yaml , this guide could be handy 11 Note that the name categoricals is confusing here: The original variable (i.e. a column) is categorical, the aggregate of that column is not. The same with the aggregates : The original column could be a categorical or a numeric (to be fare most of the time is a numeric column, but see the example: we are counting ), and then triage applies an aggregate that will be numeric. That is how triage named things, and yes, I know is confusing. 12 triage will generate also a new binary column that indicates if the value of the feature was imputed ( 1 ) or not ( 0 ): inspections_entity_id_6month_total_count_imp .","title":"A deeper look into triage"},{"location":"dirtyduck/triage_intro/#triage","text":"Predictive analytics projects require coordinating many tasks, such as feature generation, classifier training, evaluation, and list generation. Each of these tasks is complicated in its own right, but it also needs to be combined with the other tasks throughout the course of the project. DSaPP built triage to facilitate the creation of supervised learning models, in particular binary classification models with a strong temporal component in the data. The dataset's temporal component mainly affects two modeling steps: feature creation (you need to be careful to avoid leaking information from the future through your features ) and hyperparameter selection. triage solves both by splitting the data into temporal blocks and automating temporal cross-validation (TCC) and the feature generation. triage uses the concept of an experiment . An experiment consists of a series of steps that aim to generate a good model for predicting the label of an entity in the data set. The steps are data time-splitting , label generation , feature generation , matrix creation , model training , predictions , and model evaluation . In each of these steps, triage will handle the temporal nuances of the data. Nowadays triage will help you to select the best model ( model selection ) and it allows you to explore and understand the behavior of your models using post-modeling techniques. You need to specify (via a configuration file) how you want to split your data temporally, which combination of machine learning algorithms and their hyperparameters you'd like to use, which kinds of features you want to generate, which subsets of those features you want to try in each model, and which metrics you'd like to use to evaluate performance and provide some criteria to select the best model. An experiment run consists in fitting every combination of algorithm, hyperparameters, and feature subsets to the temporally split data and evaluating their predictive performance on future data splits using the user's metrics. triage calls a unique combination of algorithm, hyperparameters, and feature subsets a model_group and a model group fit to a specific data matrix a model . Our data typically span multiple time periods, so triage fits multiple models for each model group. triage is simple to use, but it contains a lot of complex concepts that we will try to clarify in this section. First we will explain how to run triage , and then we will create a toy experiment that helps explain triage's main concepts.","title":"Triage"},{"location":"dirtyduck/triage_intro/#triage-interface","text":"To run a triage experiment, you need the following: A database with the data that you want to model. In this tutorial, the credentials are part of the DATABASE_URL environment variable triage installed in your environment. You can verify that triage is indeed installed if you type in bastion : triage -h An experiment config file . This is where the magic happens. We will discuss this file at length in this section of the tutorial. We are providing a docker container, bastion , that executes triage experiments. You already had the database (you were working on it the last two sections of this tutorial, remember?). So, like a real project, you just need to worry about the experiment configuration file . In the following section of the tutorial we will use a small experiment configuration file located at <../triage/experiments/simple_test_skeleton.yaml>. We will show you how to setup the experiment while explaining the inner workings of triage . We will modify the configuration file to show the effects of the configuration parameters. If you want to follow along, we suggest you copy that file and modify by yourself. You can run that experiment with: # Remember to run this in bastion NOT in your laptop! triage experiment experiments/simple_test_skeleton.yaml Every time you modify the configuration file and see the effects, you should execute the experiment again using the previous command.","title":"Triage interface"},{"location":"dirtyduck/triage_intro/#a-simple-triage-experiment","text":"","title":"A simple triage experiment"},{"location":"dirtyduck/triage_intro/#a-brief-recap-of-machine-learning","text":"Triage helps you to run a Machine learning experiment. An experiment in this context means the use of Machine Learning to explore a dynamic system in order to do some predictions about it. Before execute the any ML experiment, you need to define some boundaries : Which are the entities that you want to study? What will you want to know about them? In DSaPP, we build ML systems that aim to have social impact, i.e. help government offices, NGOs or other agents to do their job better. \"Do their job better\" means increase their reach (e.g. identify correctly more entities with some characteristics) using more efficiently their (scarce) resources (e.g. inspectors, medics, money, etc). With this optic, the boundaries are: Cohort: Which are the entities that you want to reach? Label: What will you want to know about them? Label timespan: In what time period? Update frequency: How frequently do you want to intervene? List size: How many resources do you have to intervene? Triage's experiment configuration file structures this information.","title":"A brief recap of Machine Learning"},{"location":"dirtyduck/triage_intro/#cohorts-labels-event-dates-and-as-of-dates","text":"We will use the inspections prioritization as a narrative to help clarify these concepts: Which are the entities that you want to reach? : Active facilities, i.e. facilities that exists at the day of the planning inspections. We don't want to waste city resources (inspectors time) going to facilities that are out of business. What will you want to know about them?: Will those facilities fail the inspection? In what time period?: Will those facilities fail the inspection in the following month? How frequently do you want to intervene?: Every month. How many resources do you have to intervene?: We only have one inspector, so, one inspection per month To exemplify and explain the inner workings of triage in this scenario, we will use a subset of the semantic.events table with the following facilities (i.e. imagine that Chicago only has this three facilities): select entity_id , license_num , facility_aka , facility_type , activity_period from semantic . entities where license_num in ( 1596210 , 1874347 , 1142451 ) order by entity_id asc ; entity id license num facility aka facility type activity period 229 1596210 food 4 less grocery store [2010-01-08,) 355 1874347 mcdonalds restaurant [2010-01-12,2017-11-09) 840 1142451 jewel foodstore # 3345 grocery store [2010-01-26,) The first thing triage does when executes the experiment, is split the time that the data covers in blocks considering the time horizon for the label ( Which facilities will fail an inspection in the following month? in this scenario of inspection prioritization 1 ) . This time horizon is calculated from a set of specific dates ( as_of_date in triage parlance) that divide the blocks in past (for training the model) and future (for testing the model). The set of as_of_dates is ( mainly ) calculated from the label timespan and the update frequency 2 . The as of date is not the event date . The event date occurred when the facility was inspected. The as of date is when the planning of the future facilities to be inspected happens. triage will create those labels using information about the outcome of the event 3 , taking into account the temporal structure of the data. In our example: if a facility is inspected is an event, and whether it fails the inspection (outcome true ) or not (outcome false ). For a given entity on a given as of date , triage asks whether there's an outcome in the future time horizon. If so, triage will generate a label for that specific entity on that as of date . For this example, the label will be if given an as of date (e.g. January first, 2014), the facility will have a failed inspection in the following year. The following example hopefully will clarify the difference between outcome and label . We will focus on events (inspections) that happened in the year of 2014. select date , entity_id , ( result = 'fail' ) as outcome from semantic . events where '[2014-01-01, 2015-01-01]' :: daterange @> date and entity_id in ( 229 , 355 , 840 ) order by date asc ; date entity id outcome 2014-01-14 840 f 2014-02-04 229 f 2014-02-24 840 t 2014-03-05 840 f 2014-04-10 355 t 2014-04-15 229 f 2014-04-18 355 f 2014-05-06 840 f 2014-08-28 355 f 2014-09-19 229 f 2014-09-30 355 t 2014-10-10 355 f 2014-10-31 840 f We can observe that the facilities had several inspections, but in that timeframe 362 y 859 had failed inspections. Continuing the narrative, from the perspective of the day of 2014-01-01 ( as of date ), those facilities will have positive label . We can express that in a query and getting the labels for that as of date : select '2014-01-01' as as_of_date , entity_id , bool_or ( result = 'fail' ):: integer as label from semantic . events where '2014-01-01' :: timestamp <= date and date < '2014-01-01' :: timestamp + interval '1 year' and entity_id in ( 229 , 355 , 840 ) group by entity_id ; as of date entity id label 2014-01-01 229 0 2014-01-01 355 1 2014-01-01 840 1 Note that ee transform the label to integer, since the machine learning algorithms only work with numeric data. We also need a way to store the state of each entity. We can group entities in cohorts defined by the state. The cohort can be used to decide which facilities you want to predict on (i.e. include in the ML train/test matrices). The rationale of this comes from the need to only predict for entities in a particular state: Is the restaurant new? Is this facility on this zip code ? Is the facility \"active\"? 4 We will consider a facility as active if a given as of date is in the interval defined by the start_date and end_date . select '2018-01-01' :: date as as_of_date , entity_id , activity_period , case when activity_period @> '2018-01-01' :: date -- 2018-01-01 is as of date then 'active' :: text else 'inactive' :: text end as state from semantic . entities where entity_id in ( 229 , 355 , 840 ); as of date entity id activity period state 2018-01-01 229 [2010-01-08,) active 2018-01-01 355 [2010-01-12,2017-11-09) inactive 2018-01-01 840 [2010-01-26,) active Triage will use a simple modification of the queries that we just discussed for automate the generation of the cohorts and labels for our experiment.","title":"Cohorts, labels, event dates and as of dates"},{"location":"dirtyduck/triage_intro/#experiment-configuration-file","text":"The experiment configuration file is used to create the experiment object. Here, you will specify the temporal configuration, the features to be generated, the labels to learn, and the models that you want to train in your data. The configuration file is a yaml file with the following main sections: temporal_config : Temporal specification of the data, used for creating the blocks for temporal crossvalidation. cohort_config : Using the state of the entities, define (using sql ) cohorts to filter out objects that shouldn't be included in the training and prediction stages. This will generate a table call cohort_{experiment_hash} label_config : Specify (using sql ) how to generate labels from the event's outcome . A table named labels_{experiment_hash} will be created. feature_aggregation : Which spatio-temporal aggregations of the columns in the data set do you want to generate as features for the models? model_group_keys : How do you want to identify the model_group in the database (so you can run analysis on them)? grid_config : Which combination of hyperparameters and algorithms will be trained and evaluated in the data set? scoring : Which metrics will be calculated? Two of the more important (and potentially confusing) sections are temporal_config and feature_generation . We will explain them in detail in the next sections.","title":"Experiment configuration file"},{"location":"dirtyduck/triage_intro/#temporal-crossvalidation","text":"Cross validation is a common technique to select a model that generalizes well to new data. Standard cross validation randomly splits the training data into subsets, fits models on all but one, and calculates the metric of interest (e.g. precision/recall) on the one left out, rotating through the subsets and leaving each out once. You select the model that performed best across the left-out sets, and then retrain it on the complete training data. Unfortunately, standard cross validation is inappropriate for most real-world data science problems. If your data have temporal correlations, standard cross validation lets the model peek into the future, training on some future observations and testing on past observations. To avoid this problem, you should design your training and testing to mimic how your model will be used, making predictions only using the data that would be available at that time (i.e. from the past). In temporal crossvalidation, rather than randomly splitting the dataset into training and test splits, temporal cross validation splits the data by time. triage uses the timechop library for this purpose. Timechop will \"chop\" the data set in several temporal blocks. These blocks are then used for creating the features and matrices for training and evaluation of the machine learning models. Assume we want to select which restaurant (of two in our example dataset) we should inspect next year based on its higher risk of violating some condition. Also assume that the process of picking which facility is repeated every year on January 1 st 5 Following the problem description template given in Section Description of the problem to solve , the question that we'll attempt to answer is: Which facility ( n=1 n=1 ) is likely to violate some inspected condition in the following year ( X=1 X=1 )? The traditional approach in machine learning is splitting the data in training and test datasets. Train or fit the algorithm on the training data set to generate a train model and test or evaluate the model on the test data set. We will do the same here, but, with the help of timechop we will take in account the time: We will fit models on training set up to 2014-01-01 and see how well those models would have predicted 2015; fit more models on training set up to 2015-01-01 and see how well those models would have predicted 2016; and so on. That way, we choose models that have historically performed best at our task, forecasting. It\u2019s why this approach is sometimes called evaluation on a rolling forecast origin because the origin at which the prediction is made rolls forward in time. 6 The data at which the model will do the predictions is denominated as as of date in triage ( as of date = January first in our example). The length of the prediction time window (1 year) is called label span . Training and predicting with a new model as of date (every year) is the model update frequency . Because it's inefficient to calculate by hand all the as of dates or prediction points, timechop will take care of that for us. To do so, we need to specify some more constraints besides the label span and the model update frequency : What is the date range covered by our data? What is the date range in which we have information about labels? How frequently do you receive information about your entities? How far in the future you want to predict? How much of the past data do you want to use? With this information, timechop will calculate as-of train and test dates from the last date in which you have label data, using the label span in both test and train sets, plus the constraints just mentioned. In total timechop uses 11 configuration parameters 7 . There are parameters related to the boundaries of the available data set: feature_start_time : data aggregated into features begins at this point (earliest date included in features) feature_end_time : data aggregated into features is from before this point (latest date included in features) label_start_time : data aggregated into labels begins at this point (earliest event date included in any label (event date >= label start time ) label_end_time : data aggregated is from before this point (event date < label end time to be included in any label) Parameters that control the labels ' time horizon on the train and test sets: training_label_timespans : how much time is covered by training labels (e.g., outcomes in the next 3 days? 2 months? 1 year?) (training prediction span) test_label_timespans : how much time is covered by test prediction (e.g., outcomes in the next 3 days? 2 months? 1 year?) (test prediction span) These parameters will be used with the outcomes table to generate the labels . In an early warning setting, they will often have the same value. For inspections prioritization , this value typically equals test_durations and model_update_frequency . Parameters related about how much data we want to use, both in the future and in the past relative to the as-of date : test_durations : how far into the future should a model be used to make predictions (test span) NOTE : in the typical case of wanting a single prediction set immediately after model training, this should be set to 0 days For early warning problems, test_durations should equal model_update_frequency . For inspection prioritization, organizational process determines the value: how far out are you scheduling for? The equivalent of test_durations for the training matrices is max_training_histories : max_training_histories : the maximum amount of history for each entity to train on (early matrices may contain less than this time if it goes past label/feature start times). If patterns have changed significantly, models trained on recent data may outperform models trained on a much lengthier history. Finally, we should specify how many rows per entity_id in the train and test matrix: training_as_of_date_frequencies : how much time between rows for a single entity in a training matrix (list time between rows for same entity in train matrix). test_as_of_date_frequencies : how much time between rows for a single entity in a test matrix (time between rows for same entity in test matrix). The following images (we will show how to generate them later) shows the time blocks created by several temporal configurations. We will change a parameter at a time so you could see how it affects the resulting blocks. If you want to try the modifications (or your own) and generate the temporal blocks images run the following (they'll be generated in <./images/>): # Remember to run this in bastion NOT in laptop's shell! triage experiment experiments/simple_test_skeleton.yaml --show-timechop {feature, label}_{end, start}_time The image below shows these {feature, label}_start_time are equal, as are the {feature, label}_end_time . These parameters show in the image as dashed vertical black lines. This setup will be our baseline example. The plot is divided in two horizontal lines (\"Block 0\" and \"Block 1\"). Each line is divided by vertical dashed lines \u2013 the grey lines outline the boundaries of the data for features and data for labels, which in this image coincide. The black dash lines represent the beginning and the end of the test set. In \"Block 0\" those lines correspond to 2017 and 2018 , and in \"Block 1\" they correspond to 2016 and 2017 . The shaded areas (in this image there is just one per block, but you will see other examples below) represents the span of the as of dates . They start with the oldest as of date and end with the latest. Each line inside that area represents the label span. Those lines begin at the as of date . At each as of date , timechop generates each entity's features (from the past) and labels (from the future). So in the image, we will have two sets of train/test datasets. Each facility will have 13 rows in \"Block 0\" and 12 rows in \"Block 1\". The trained models will predict the label using the features calculated for that test set as of date . The single line represents the label's time horizon in testing. This is the temporal configuration that generated the previous image: temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '1y' In that configuration the date ranges of features and labels are equal, but they can be different (maybe you have more data for features that data for labels) as is shown in the following image and in their configuration parameters. temporal_config : feature_start_time : '2010-01-01' # <------- The change happened here! feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '1y' model_update_frequency From our baseline temporal_config example ( 102 ), we will change how often we want a new model, which generates more time blocks (if there are time-constrained data, obviously). temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '6month' # <------- The change happened here! training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '1y' max_training_histories With this parameter you could get a growing window for training (depicted in 110 ) or as in all the other examples, fixed training windows . temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '10y' # <------- The change happened here! _as_of_date_frequencies and test_durations temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '3month' # <------- The change happened here! test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '10y' Now, change test_as_of_date_frequencies : temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '3month' <------- The change happened here! max_training_histories : '10y' Nothing changed because the test set doesn't have \"space\" to allow more spans. The \"space\" is controlled by test_durations , so let's change it to 6month : temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '6month' <------- The change happened here! test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '10y' So, now we will move both parameters: test_durations , test_as_of_date_frequencies temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '6month' <------- The change happened here! test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '3month' <------- and also here! max_training_histories : '10y' _label_timespans temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '1y' ] training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '3month' ] <------- The change happened here! test_as_of_date_frequencies : '1month' max_training_histories : '10y' temporal_config : feature_start_time : '2014-01-01' feature_end_time : '2018-01-01' label_start_time : '2014-01-02' label_end_time : '2018-01-01' model_update_frequency : '1y' training_label_timespans : [ '3month' ] <------- The change happened here! training_as_of_date_frequencies : '1month' test_durations : '0d' test_label_timespans : [ '1y' ] test_as_of_date_frequencies : '1month' max_training_histories : '10y' That's it! Now you have the power to bend time! 8 With the time blocks defined, triage will create the labels and then the features for our train and test sets. We will discuss features in the following section.","title":"Temporal crossvalidation"},{"location":"dirtyduck/triage_intro/#feature-generation","text":"We will show how to create features using the experiments config file . triage uses collate for this. 9 The collate library controls the generation of features (including the imputation rules for each feature generated) using the time blocks generated by timechop . Collate helps the modeler create features based on spatio-temporal aggregations into the as of date . Collate generates SQL queries that will create features per each as of date . As before, we will try to mimic what triage does behind the scenario. Collate will help you to create features based on the following template: For a given as of date , how the aggregation function operates into a column taking into account a previous time interval and some attributes . Two possible features could be framed as: As of 2016-01-01, how many inspections has each facility had in the previous 6 months? and As of 2016-01-01, how many \"high risk\" findings has the facility had in the previous 6 months? In our data, that date range (between 2016-01-01 and 2015-07-01) looks like: select event_id , date , entity_id , risk from semantic . events where date <@ daterange (( '2016-01-01' :: date - interval '6 months' ):: date , '2016-01-01' ) and entity_id in ( 229 , 355 , 840 ) order by date asc ; event id date entity id risk 1561324 2015-07-17 840 high 1561517 2015-07-24 840 high 1562122 2015-08-12 840 high 1547403 2015-08-20 229 high 1547420 2015-08-28 229 high 1547448 2015-09-14 355 medium 1547462 2015-09-21 355 medium 1547504 2015-10-09 355 medium 1547515 2015-10-16 355 medium 1583249 2015-10-21 840 high 1583577 2015-10-28 840 high 1583932 2015-11-04 840 high We can transform those data to two features: number_of_inspections and flagged_as_high_risk : select entity_id , '2016-01-01' as as_of_date , count ( event_id ) as inspections , count ( event_id ) filter ( where risk = 'high' ) as flagged_as_high_risk from semantic . events where date <@ daterange (( '2016-01-01' :: date - interval '6 months' ):: date , '2016-01-01' ) and entity_id in ( 229 , 355 , 840 ) group by grouping sets ( entity_id ); entity id as of date inspections flagged as high risk 229 2016-01-01 2 2 355 2016-01-01 4 0 840 2016-01-01 6 6 This query is making an aggregation . Note that the previous SQL query has five parts: The filter (( risk = 'high')::int ) The aggregation function ( count() ) The name of the resulting transformation ( flagged_as_high_risk ) The context in which it is aggregated (by entity_id ) The date range (between 2016-01-01 and 6 months before) What about if we want to add proportions and totals of failed and passed inspections? select entity_id , '2016-01-01' as as_of_date , count ( event_id ) as inspections , count ( event_id ) filter ( where risk = 'high' ) as flagged_as_high_risk , count ( event_id ) filter ( where result = 'pass' ) as passed_inspections , round ( avg (( result = 'pass' ):: int ), 2 ) as proportion_of_passed_inspections , count ( event_id ) filter ( where result = 'fail' ) as failed_inspections , round ( avg (( result = 'fail' ):: int ), 2 ) as proportion_of_failed_inspections from semantic . events where date <@ daterange (( '2016-01-01' :: date - interval '6 months' ):: date , '2016-01-01' ) and entity_id in ( 229 , 355 , 840 ) group by grouping sets ( entity_id ) entity id as of date inspections flagged as high risk passed inspections proportion of passed inspections failed inspections proportion of failed inspections 229 2016-01-01 2 2 1 0.50 1 0.50 355 2016-01-01 4 0 1 0.25 2 0.50 840 2016-01-01 6 6 4 0.67 2 0.33 But what if we want to also add features for \"medium\" and \"low\" risk? And what would the query look like if we want to use several time intervals, like 3 months , 5 years , etc? What if we want to contextualize this by location? Plus we need to calculate all these features for several as of dates and manage the imputation strategy for all of them!!! You will realize that even with this simple set of features we will require very complex SQL to be constructed. But fear not. triage will automate that for us! The following blocks of code represent a snippet of triage 's configuration file related to feature aggregation. It shows the triage syntax for the inspections feature constructed above: feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates : - quantity : total : \"*\" imputation : count : type : 'zero_noflag' metrics : - 'count' intervals : [ '6month' ] groups : - 'entity_id' feature_aggregations is a yaml list 10 of feature groups construction specification or just feature group . A feature group is a way of grouping several features that share intervals and groups . triage requires the following configuration parameter for every feature group : prefix : This will be used for name of the feature created from_obj : Represents a TABLE object in PostgreSQL . You can pass a table like in the example above ( semantic.events ) or a SQL query that returns a table. We will see an example of this later. triage will use it like the FROM clause in the SQL query. knowlege_date_column : Column that indicates the date of the event. intervals : A yaml list. triage will create one feature per interval listed. groups : A yaml list of the attributes that we will use to aggregate. This will be translated to a SQL GROUP BY by triage . The last section to discuss is imputation . Imputation is very important step in the modeling, and you should carefully think about how you will impute the missing values in the feature. After deciding the best way of impute each feature, you should avoid leakage (For example, imagine that you want to impute with the mean one feature. You could have leakage if you take all the values of the column, including ones of the future to calculate the imputation). We will return to this later in this tutorial. Collate is in charge of creating the SQL agregation queries. Another way of thinking about it is that collate encapsulates the FROM part of the query ( from_obj ) as well as the GROUP BY columns ( groups ). triage ( collate ) supports two types of objects to be aggregated: aggregates and categoricals (more on this one later) 11 . The aggregates subsection represents a yaml list of features to be created. Each element on this represents a column ( quantity , in the example, the whole row * ) and an alias ( total ), defines the imputation strategy for NULLs , and the metric refers to the aggregation function to be applied to the quantity ( count ). triage will generate the following (or a very similar one), one per each combination of interval \u00d7 groups \u00d7 quantity : select metric ( quantity ) as alias from from_obj where as_of_date <@ ( as_of_date - interval , as_of_date ) group by groups With the previous configuration triage will generate 1 feature with the following name: 12 inspections_entity_id_6month_total_count All the features of that feature group (in this case only 1) will be stored in the table. features.inspections_aggregation_imputed In general the names of the generated tables are constructed as follows: schema.prefix_group_aggregation_imputed NOTE : the outputs are stored in the features schema. Inside each of those new tables, the feature name will follow this pattern: prefix_group_interval_alias_aggregation_operation If we complicate a little the above configuration adding new intervals: feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates : - # number of inspections quantity : total : \"*\" imputation : count : type : 'zero_noflag' metrics : [ 'count' ] intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' You will end with 5 new features , one for each interval (5) \u00d7 the only aggregate definition we have. Note the weird all in the intervals definition. all is the time interval between the feature_start_time and the as_of_date . triage also supports categorical objects. The following code adds a feature for the risk flag. feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates : - # number of inspections quantity : total : \"*\" imputation : count : type : 'zero_noflag' metrics : [ 'count' ] intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'risks' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero' categoricals : - column : 'risk' choice_query : 'select distinct risk from semantic.events' metrics : - 'sum' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' There are several changes. First, the imputation strategy in this new feature group is for every categorical features in that feature group (in that example only one). The next change is the type: instead of aggregates , it's categoricals . categoricals define a yaml list too. Each categorical feature needs to define a column to be aggregated and the query to get all the distinct values. With this configuration, triage will generate two tables, one per feature group . The new table will be features.risks_aggregation_imputed . This table will have more columns: intervals (5) \u00d7 groups (1) \u00d7 metric (1) \u00d7 features (1) \u00d7 number of choices returned by the query . The query: select distinct risk from semantic . events ; risk \u00a4 medium high low returns 4 possible values (including NULL ). When dealing with categorical aggregations you need to be careful. Could be the case that in some period of time, in your data, you don't have all the possible values of the categorical variable. This could cause problems down the road. Triage allows you to specify the possible values ( choices ) of the variable. Instead of using choice_query , you could use choices as follows: feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates : - # number of inspections quantity : total : \"*\" imputation : count : type : 'mean' metrics : [ 'count' ] intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'risks' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero' categoricals : - column : 'risk' choices : [ 'low' , 'medium' , 'high' ] metrics : - 'sum' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' In both cases triage will generate 20 new features, as expected. The features generated from categorical objects will have the following pattern: prefix_group_interval_column_choice_aggregation_operation So, risks_entity_id_1month_risk_medium_sum will be among our new features in the last example. As a next step, let's investigate the effect of having several elements in the groups list. feature_aggregations : - prefix : 'inspections' from_obj : 'semantic.events' knowledge_date_column : 'date' aggregates : - # number of inspections quantity : total : \"*\" imputation : count : type : 'mean' metrics : [ 'count' ] intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - prefix : 'risks' from_obj : 'semantic.events' knowledge_date_column : 'date' categoricals_imputation : sum : type : 'zero' categoricals : - column : 'risk' choices : [ 'low' , 'medium' , 'high' ] metrics : - 'sum' intervals : [ '1month' , '3month' , '6month' , '1y' , 'all' ] groups : - 'entity_id' - 'zip_code' The number of features created in the table features.risks_aggregation_imputed is now 60 ( intervals (5) \u00d7 groups (2) \u00d7 metric (2) \u00d7 features (1) \u00d7 number of choices (3). Triage will add several imputation flag (binary) columns per feature. Those columns convey information about if that particular value was imputed or not . So in the last counting we need to add 20 more columns to a grand total of 80 columns.","title":"Feature generation"},{"location":"dirtyduck/triage_intro/#imputation","text":"Triage currently supports the following imputation strategies: mean: The mean value of the feature. constant: Fill with a constant (you need to provide the constant value). zero: Same that the previous one, but the constant is zero. zero noflag : Sometimes, the absence (i.e. a NULL) doesn't mean that the value is missing, that actually means that the event didn't happen to that entity. For example a NULL in the inspections_entity_id_1month_total_count column in features.inspections_aggreagtion_imputed doesn't mean that the value is missing, it means that zero inspections happen to that facility in the last month. Henceforth, the flag column is not needed. Only for aggregates: binary mode : Takes the mode of a binary feature Only for categoricals:: null category : Just flag null values with the null category column and finally, if you are sure that is not possible to have NULLS: error: Raise an exception if ant null values are encountered.","title":"Imputation"},{"location":"dirtyduck/triage_intro/#feature-groups-strategies","text":"Another interesting thing that triage controls is how many feature groups are used in the machine learning grid. This would help you to understand the effect of using different groups in the final performance of the models. In simple_test_skeleton.yaml you will find the following blocks: feature_group_definition : prefix : - 'results' - 'risks' - 'inspections' feature_group_strategies : [ 'all' ] This configuration adds to the number of model groups to be created. The possible feature group strategies are: all : All the features groups are used. leave-one-out : All the combinations of: \"All the feature groups except one are used\". leave-one-in : All the combinations of \"One feature group except the rest is used\" all-combinations : All the combinations of feature groups In order to clarify these concepts, let's use simple_test_skeleton.yaml configuration file. In it there are three feature groups: inspections , results , risks . Using all will create just one set containg all the features of the three feature groups: {inspections, results, risks} If you modify feature_group_strategies to ['leave-one-out'] : the following sets will be created: {inspections, results}, {inspections, risks}, {results, risks} Using the leave-one-in strategy: {inspections}, {results}, {risks} Finally choosing all-combinations : {inspections}, {results}, {risks}, {inspections, results} , {inspections, risks}, {results, risks}, {inspections, results, risks}","title":"Feature groups strategies"},{"location":"dirtyduck/triage_intro/#controlling-the-size-of-the-tables","text":"This section is a little technical, you can skip it if you fell like it. By default, triage will use the biggest column type for the features table ( integer , numeric , etc). This could lead to humongous tables, with sizes several hundred of gigabytes. Triage took that decision, because it doesn't know anything about the possible values of your data (e.g. Is it possible to have millions of inspections in one month? or just a few dozens?). If you are facing this difficulty, you can force triage to cast the column in the features table. Just add coltype to the aggregate/categorical block: aggregates : - quantity : total : \"*\" metrics : [ 'count' ] coltype : 'smallint'","title":"Controlling the size of the tables"},{"location":"dirtyduck/triage_intro/#the-grid","text":"Before applying Machine Learning to your dataset you don't know which combination of algorithm and hyperparameters will be the best given a specific matrix. Triage approaches this problem exploring a algorithm + hyperparameters + feature groups grid. At this time, this exploration is a exhaustive one, i.e. it covers the complete grid, so you would get (number of algorithms) \\times \\times (number of hyperparameters) \\times \\times (number of feature group strategies) models groups. The number of models trained is (number of model groups) \\times \\times (number of time splits). In our simple experiment the grid is very simple: grid_config : 'sklearn.dummy.DummyClassifier' : strategy : [ most_frequent ] Just one algorithm and one hyperparameter (also we have only one feature group strategy: all ), and two time splits. So we will get 2 models, 1 model group. Keep in mind that the grid is providing more than way to select a model. You can use the tables generated by the grid (see section Machine Learning Governance ) and analyze and understand your data. In other words, analyzing the results (evaluations, predictions, hyperparameter space, etc.) is like applying Data mining concepts to your data using Machine learning. We will return to this when we apply post modeling to our models.","title":"The Grid"},{"location":"dirtyduck/triage_intro/#audition","text":"Audition is a tool for helping you select a subset of trained classifiers from a triage experiment. Often, production-scale experiments will come up with thousands of trained models, and sifting through all of those results can be time-consuming even after calculating the usual basic metrics like precision and recall. You will be facing questions as: Which metrics matter most? Should you prioritize the best metric value over time or treat recent data as most important? Is low metric variance important? The answers to questions like these may not be obvious. Audition introduces a structured, semi-automated way of filtering models based on what you consider important.","title":"Audition"},{"location":"dirtyduck/triage_intro/#post-modeling","text":"As the name indicates, postmodeling occurs after you have modeled (potentially) thousands of models (different hyperparameters, different time windows, different algorithms, etc), and using audition you pre selected a small number of models. Now, with the postmodeling tools you will be able to select your final model for production use. Triage's postmodeling capabilities include: Show the score distribution Compare the list generated by a set of models Compare the feature importance between a set of models Diplay the probability calibration curves Analyze the errors using a decision tree trained on the errors of the model. Cross-tab analysis Bias analysis If you want to see Audition and Postmodeling in action, we will use them after Inspections and EIS modeling .","title":"Post-modeling"},{"location":"dirtyduck/triage_intro/#final-cleaning","text":"In the next section we will start modeling, so it is a good idea to clean the {test, train}_results schemas and have a fresh start: select nuke_triage (); nuke triage triage was send to the oblivion. Long live to triage! triage also creates a lot of files (we will see why in the next section). Let's remove them too. rm -r /triage/matrices/* rm -r /triage/trained_models/*","title":"Final cleaning"},{"location":"dirtyduck/triage_intro/#where-to-go-from-here","text":"If you haven't done so already, our dirty duck tutorial is a good way to geet up and running with some sample data. If you're ready to get started with your own data, check out the suggested project workflow for some tips about how to iterate and tune the pipeline for your project.","title":"Where to go from here..."},{"location":"dirtyduck/triage_intro/#footnotes","text":"1 Would be my restaurant inspected in the following month? in the case of an early warning case. 2 It's a little more complicated than that as we will see. 3 All events produce some outcome . In theory every event of interest in stored in a database. These events are immutable : you can't (shouldn't) change them (they already happen). 4 We could consider different states, for example: we can use the column risk as an state. Another possibility is define a new state called failed that indicates if the facility failed in the last time it was inspected. One more: you could create cohorts based on the facility_type. 5 The city in this toy example has very low resources. 6 See for example: https://robjhyndman.com/hyndsight/tscv/ 7 I know, I know. And in order to cover all the cases, we are still missing one or two parameters, but we are working on it. 8 Obscure reference to the \" Avatar: The Last Airbender \" cartoon series. I'm sorry. 9 collate is to feature generation what timechop is to date temporal splitting 10 triage uses a lot of yaml , this guide could be handy 11 Note that the name categoricals is confusing here: The original variable (i.e. a column) is categorical, the aggregate of that column is not. The same with the aggregates : The original column could be a categorical or a numeric (to be fare most of the time is a numeric column, but see the example: we are counting ), and then triage applies an aggregate that will be numeric. That is how triage named things, and yes, I know is confusing. 12 triage will generate also a new binary column that indicates if the value of the feature was imputed ( 1 ) or not ( 0 ): inspections_entity_id_6month_total_count_imp .","title":"Footnotes"},{"location":"dirtyduck/who_is_this_tutorial_for/","text":"Who is this tutorial for? # We created this tutorial with two roles in mind: A data scientist/ML practitioner who wants to focus in the problem at his/her hands, not in the nitty-gritty detail about how to configure and setup a Machine learning pipeline, Model governance, Model selection, etc. A policy maker with a little of technical background that wants to learn how to pose his/her policy problem as a Machine Learning problem.","title":"Who is this tutorial for?"},{"location":"dirtyduck/who_is_this_tutorial_for/#who-is-this-tutorial-for","text":"We created this tutorial with two roles in mind: A data scientist/ML practitioner who wants to focus in the problem at his/her hands, not in the nitty-gritty detail about how to configure and setup a Machine learning pipeline, Model governance, Model selection, etc. A policy maker with a little of technical background that wants to learn how to pose his/her policy problem as a Machine Learning problem.","title":"Who is this tutorial for?"},{"location":"experiments/algorithm/","text":"Experiment Algorithm Deep Dive # This guide's purpose is to provide familiarity of the inner workings of a Triage Experiment to people with some experience in data science and Python. A Triage Experiment is a highly structured way of defining the experimentation phase of a data science project. To those wondering whether this Experiment structure is flexible enough to fit their needs, this should help. 1. Temporal Validation Setup # First, the given temporal_config section in the experiment definition is transformed into train and test splits, including as_of_times for each matrix. We create these splits by figuring out the latest reasonable split time from the inputs, and moving backwards in time at the rate of the given model_update_frequency , until we get to the earliest reasonable split time. For each split, we create as_of_times by moving either backwards from the split time towards the max_training_history (for train matrices) or forwards from the split time towards the test_duration (for test matrices) at the provided data_frequency . Many of these configured values may be lists, in which case we generate the cross-product of all the possible values and generate more splits. For a more detailed look at the temporal validation logic, see Temporal Validation Deep Dive . The train and test splits themselves are not used until the Building Matrices section, but a flat list of all computed as_of_times for all matrices needed in the experiment is used in the next section, Transforming Data . 2. Transforming Data # With all of the as_of_times for this Experiment now computed, it's now possible to transform the input data into features and labels as of all the required times. Labels # The Experiment populates a 'labels' table using the following input: 1. A query, provided by the user in the configuration file, that generates entity_ids and outcomes for a given as_of_date and label_timespan. Each as_of_date and label_timespan defined in temporal config For instance, an inspections-style query (for the given timespan, return the entity and outcome of any matching inspections) would look like: select events.entity_id, bool_or(outcome::bool)::integer as outcome from events where '{as_of_date}' <= outcome_date and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id This binary labels table is scoped to the entire Experiment, so all as_of_time (computed in step 1) and label_timespan (taken straight from temporal_config ) combinations are present. Additionally, the 'label_name' and 'label_type' are also recorded with each row in the table. The name of the labels table is based on both the name of the label and a hash of the label query (e.g labels_failedviolation_a0b1c2d3 ), so any prior experiments that shared both the name and query will be able to reuse the labels table. If the 'replace' flag was sent, for each as_of_time and label_timespan , the labels table is queried to check if any rows exist that match. If any such rows exist, the labels query for that date and timespan is not run. At this point, the 'labels' table may not have entries for all entities and dates that need to be in a given matrix. How these rows have their labels represented is up to the configured include_missing_labels_in_train_as value in the experiment. This value is not processed when we generate the labels table, but later on when the matrix is built (see 'Retrieving Data and Saving Completed Matrix') Cohort Table # The Experiment keeps track of the which entities are in the cohort on any given date. Similarly to the labels table, the experiment populates a cohort table using the following input: A query, provided by the user in the configuration file, that generates entity_ids for a given as_of_date. Each as_of_date as defined in temporal config This cohort table is scoped to the entire Experiment, so all as_of_times (computed in step 1) are present. The name of the cohort table is based on both the name of the cohort and a hash of the cohort query (e.g cohort_permitted_a0b1c2d3 ), so any prior experiments that shared both the name and query will be able to reuse the cohort table. If the 'replace' flag was sent, for each as_of_time , the cohort table is queried to check if any rows exist that match. If any such rows exist, the cohort query for that date is not run. Features # Each provided feature_aggregation configures the creation and population of several feature tables in the 'features' schema: one for each of the groups specified in the config, one that merges the groups together into one table, and one that fills in null values from the merged table with imputed values based on imputation config. Generating Aggregation SQL To generate the SQL that creates the pre-imputation table, the Experiment assembles building blocks from the feature aggregation config, as well as the experiment's list of as_of_times : from_obj represents, well, the object of the FROM clause in the SQL query. Often this is just a table, but can be configured to be a subquery. This holds all the data that we want to aggregate into features Each as_of_time in the experiment and interval in the feature_aggregation is combined with the knowledge_date_column to create a WHERE clause representing a valid window of events to aggregate in the from_obj : e.g ( where {knowledge_date_column} >= {as_of_time} - interval {interval} ) Each aggregate , categorical , or array_categorical represents a SELECT clause. For aggregates, the quantity is a column or SQL expression representing a numeric quantity present in the from_obj , and the metrics are any number of aggregate functions we want to use. The aggregate function is applied to the quantity. Each group is a column applied to the GROUP BY clause. Generally this is 'entity_id', but higher-level groupings (for instance, 'zip_code') can be used as long as they can be rolled up to 'entity_id'. By default the query is joined with the cohort table to remove unnecessary rows. If features_ignore_cohort is passed to the Experiment this is not done. So a simplified version of a typical query would look like: SELECT {group}, {metric}({quantity}) FROM {from_obj} JOIN {cohort_table} ON ( {cohort_table.entity_id} = {from_obj.entity_id} AND {cohort_table.date} = {as_of_time} ) WHERE {knowledge_date_column} >= {as_of_time} - interval {interval} GROUP BY {group} Writing Group-wide Feature Tables For each as_of_time , the results from the generated query are written to a table whose name is prefixed with the prefix , and suffixed with the group . For instance, if the configuration specifies zipcode-level aggregates and entity-level aggregates, there will be a table for each, keyed on its group plus the as_of_date. Merging into Aggregation-wide Feature Tables Each generated group table is combined into one representing the whole aggregation with a left join. Given that the groups originally came from the same table (the from_obj of the aggregation) and therefore we know the zipcode for each entity, what we do now is create a table that would be keyed on entity and as_of_date, and contain all entity-level and zipcode-level aggregates from both tables. This aggregation-level table represents all of the features in the aggregation, pre-imputation. Its output location is generally {prefix}_aggregation Imputing Values A table that looks similar, but with imputed values is created. The cohort table from above is passed into collate as the comprehensive set of entities and dates for which output should be generated, regardless if they exist in the from_obj . Each feature column has an imputation rule, inherited from some level of the feature definition. The imputation rules that are based on data (e.g. mean ) use the rows from the as_of_time to produce the imputed value. In addition, each column that needs imputation has an imputation flag column created, which contains a boolean flagging which rows were imputed or not. Since the values of these columns are redundant for most aggregate functions that look at a given timespan's worth of data (they will be imputed only if zero events in their timespan are seen), only one imputation flag column per timespan is created. An exception to this are some statistical functions that require not one, but two values, like standard deviation and variance. These boolean imputation flags are not merged in with the others. Its output location is generally {prefix}_aggregation_imputed Recap # At this point, we have at least three tables that are used to populate matrices: labels_{labelname}_{labelqueryhash} with computed labels for each date cohort_{cohortname}_{cohortqueryhash} with the cohort for each date A features.{prefix}_aggregation_imputed table for each feature aggregation present in the experiment config. 3. Building Matrices # At this point, we have to build actual train and test matrices that can be processed by machine learning algorithms, save at the user's specified path, either on the local filesystem or s3 depending on the scheme portion of the path (e.g. s3://bucket-name/project_directory ) First we have to figure out exactly what matrices we have to build. The split definitions from step 1 are a good start -- they are our train and test splits -- but sometimes we also want to test different subsets of the data, like feature groups (e.g. 'how does using group of features A perform against using all features?'). So there's a layer of iteration we introduce for each split, that may produce many more matrices. What do we iterate over? * Feature List - All subsets of features that the user wants to cycle through. This is the end result of the feature group generation and mixing process, which is described more below. * Cohorts - In theory we can take in different cohorts and iterate in the same experiment. This is not fully implemented, so in reality we just use the one cohort that is passed in the cohort_config * Label names - In theory we can take in different labels (e.g. complaints, sustained complaints) in the same experiment. Right now there is no support for multiple label names, but the label name used is configurable through the optional 'label_config'->'name' config value * Label types - In theory we can take in different label types (e.g. binary) in the same experiment. Right now this isn't done, there is one label type and it is hardcoded as 'binary'. Feature Lists # How do we arrive at the feature lists? There are two pieces of config that are used: feature group_definition and feature_group_strategies . Feature group definitions are just ways to define logical blocks of features, most often features that come from the same source, or describing a particular type of event. These groups within the experiment as a list of feature names, representing some subset of all potential features for the experiment. Feature group strategies are ways to take feature groups and mix them together in various ways. The feature group strategies take these subsets of features and convert them into another list of subsets of features, which is the final list iterated over to create different matrices. Feature Group Definition Feature groups, at present, can be defined as either a prefix (the prefix of the feature name), a table (the feature table that the feature resides in), or all (all features). Each argument is passed as a list, and each entry in the list is interpreted as a group. So, a feature group config of {'table': ['complaints_aggregate_imputed', 'incidents_aggregate_imputed']} would result in two feature groups: one with all the features in complaints_aggregate_imputed , and one with all the features in incidents_aggregate_imputed . Note that this requires a bit of knowledge on the user's part of how the feature table names will be constructed. prefix works on the prefix of the feature name as it exists in the database. So this also requires some knowledge of how these get created. The general format is: {aggregation_prefix}_{group}_{timeperiod}_{quantity} , so with some knowledge the user can create groups with the aggregation's configured prefix (common), or the aggregations configured prefix + group (in case they want to compare, for instance, zip-code level features versus entity level features). all , with a single value of True , will include a feature group with all defined features. If no feature group definition is sent, this is the default. Either way, at the end of this process the experiment will be aware of some list of feature groups, even if the list is just length 1 with all features as one group. Feature Group Mixing A few basic feature group mixing strategies are implemented: leave-one-in , leave-one-out , and all . These are sent in the experiment definition as a list, so different strategies can be tried in the same experiment. Each included strategy will be applied to the list of feature groups from the previous step, to convert them into For instance, 'leave-one-in' will cycle through each feature group, and for each one create a list of features that just represents that feature group, so for some matrices we would only use features from that particular group. leave-one-out does the opposite, for each feature group creating a list of features that includes all other feature groups but that one. all just creates a list of features that represents all feature groups together. Iteration and Matrix Creation # At this point, matrices are created by looping through all train/test splits and data subsets (e.g. feature groups, state definitions), grabbing the data corresponding to each from the database, and assembling that data into a design matrix that is saved along with the metadata that defines it. As an example, if the experiment defines 3 train/test splits (one test per train in this example, for simplicity), 3 feature groups that are mixed using the 'leave-one-out' and 'all' strategies, and 1 state definition, we'll expect 18 matrices to be saved: 9 splits after multiplying the time splits by the feature groups, and each one creating a train and test matrix. Associating Matrices with Experiment # After all matrices for the Experiment are defined but before any are built, the Experiment is associated with each Matrix in the database through the model_metadata.experiment_matrices table. This means that whether or not the Experiment has to end up building a matrix, after the fact a user can query the database to see if it used said matrix. Retrieving Data and Saving Completed Matrix Each matrix that has to be built (i.e. has not been built by some prior experiment) is built by retrieving its data out of the database. How do we get the data for an individual matrix out of the database? Create an entity-date table for this specific matrix. There is some logic applied to decide what rows show up. There are two possible sets of rows that could show up. all valid entity dates . These dates come from the entity-date-state table for the experiment (populated using the rules defined in the 'cohort_config'), filtered down to the entity-date pairs that match both the state filter and the list of as-of-dates for this matrix . all labeled entity dates . These dates consist of all the valid entity dates from above, that also have an entry in the labels table. If the matrix is a test matrix, all valid entity dates will be present. If the matrix is a train matrix, whether or not valid but unlabeled examples show up is decided by the include_missing_labels_in_train_as configuration value. If it is present in any form, these labels will be in the matrix. Otherwise, they will be filtered out. Write features data from tables to disk in CSV format using a COPY command, table by table. Each table is joined with the matrix-specific entity-date table to only include the desired rows. Write labels data to disk in CSV format using a COPY command. These labels will consist of the rows in the matrix-specific entity-date table left joined to the labels table. Rows not present in the labels table will have their label filled in (either True or False) based on the value of the include_missing_labels_in_train_as configuration key. Merge the features and labels CSV files horizontally, in pandas. They are expected to be of the same shape, which is enforced by the entity-date table. The resulting matrix is indexed on entity_id and as_of_date , and then saved to disk (in CSV format, more formats to come) along with its metadata: time, feature, label, index, and state information. along with any user metadata the experiment config specified. The filename is decided by a hash of this metadata, and the metadata is saved in a YAML file with the same hash and directory. The metadata is additionally added to a database table 'matrices'. Matrix metadata reference: - Train matrix temporal info - Test matrix temporal info - Feature, label, index, cohort, user metadata Recap # At this point, all finished matrices and metadata will be saved under the project_path supplied by the user to the Experiment constructor, in the subdirectory matrices . 4. Running Models # The last phase of an Experiment run uses the completed design matrices to train, test, and evaluate classifiers. This procedure writes a lot of metadata to the 3 schemas: 'model_metadata', 'train_results', and 'test_results'. Associating Models with Experiment # Every combination of training matrix + classifier + hyperparameter is considered a Model. Before any Models are trained, the Experiment is associated with each Model in the database through the model_metadata.experiment_models table. This means that whether or not the Experiment has to end up training a model, after the fact a user can query the database to see if it used said model. Train # Each matrix marked for training is sent through the configured grid in the experiment's grid_config . This works much like the scikit-learn ParameterGrid (and in fact uses it on the backend). It cycles through all of the classifiers and hyperparameter combinations contained herein, and calls .fit() with that train matrix. Any classifier that adheres to the scikit-learn .fit/.transform interface and is available in the Python environment will work here, whether it is a standard scikit-learn classifier, a third-party library like XGBoost, or a custom-built one in the calling repository (for instance, one that implements the problem domain's baseline heuristic algorithm for comparison). Metadata about the trained classifier is written to the model_metadata.models Postgres table. The trained model is saved to a filename with the model hash (see Model Hash section below). Model Groups Each model is assigned a 'model group'. A model group represents a number of trained classifiers that we want to treat as equivalent by some criteria. By default, this is aimed at defining models which are equivalent across time splits, to make analyzing model stability easier. This default is accomplished with a set of 'model group keys' that includes data about the classifier (module, hyperparameters), temporal intervals used to create the train matrix (label timespan, training history, as-of-date frequency), and metadata describing the data in the train matrix (features and feature groups, label name, cohort name). The user can override this set of model_group_keys in the experiment definition, with all of the default information plus other matrix metadata at their disposal (See end of 'Retrieving Data and Saving Completed Matrix' section for more about matrix metadata). This data is stored in the model_metadata.model_groups table, along with a model_group_id that is used as a foreign key in the model_metadata.models table. Model Hash Each trained model is assigned a hash, for the purpose of uniquely defining and caching the model. This hash is based on the training matrix metadata, classifier path, hyperparameters (except those which concern execution and do not affect results of the classifier, such as n_jobs ), and the given project path for the Experiment. This hash can be found in each row of the model_metadata.models table. It is enforced as a unique key in the table. Global Feature Importance The training phase also writes global feature importances to the database, in the train_results.feature_importances table. A few methods are queried to attempt to compute feature importances: * The bulk of these are computed using the trained model's .feature_importances_ attribute, if it exists. * For sklearn's SVC models with a linear kernel, the model's .coef_.squeeze() is used. * For sklearn's LogisticRegression models, np.exp(model.coef_).squeeze() is used. * Otherwise, no feature importances are written. Test Matrix # For each test matrix, predictions, individual importances, and the user-specified testing evaluation metrics are written to the 'test_results' schema. For each train matrix, predictions and the user-specified training evaluation metrics are written to the 'train_results' schema. Predictions The trained model's prediction probabilities ( predict_proba() ) are computed both for the matrix it was trained on and any testing matrices. The predictions for the training matrix are saved in train_results.predictions and those for the testing matrices are saved in the test_results.predictions . More specifically, predict_proba returns the probabilities for each label (false and true), but in this case only the probabilities for the true label are saved in the {train or test}_predictions table. The entity_id and as_of_date are retrieved from the matrix's index, and stored in the database table along with the probability score, label value (if it has one), as well as other metadata. Individual Feature Importance # Feature importances (of a configurable number of top features, defaulting to 5) for each prediction are computed and written to the test_results.individual_importances table. Right now, there are no sophisticated calculation methods integrated into the experiment; simply the top 5 global feature importances for the model are copied to the individual_importances table. Metrics # Triage allows for the computation of both testing set and training set evaluation metrics. Evaluation metrics, such as precision and recall at various thresholds, are written to either the train_results.evaluations table or the test_results.evaluations . Triage defines a number of Evaluation Metrics metrics that can be addressed by name in the experiment definition, along with a list of thresholds and/or other parameters (such as the 'beta' value for fbeta) to iterate through. Thresholding is done either via absolute value (top k) or percentile by sorting the predictions and labels by the row's predicted probability score, with ties broken in some way (see next paragraph), and assigning the predicted value as True for those above the threshold. Note that the percentile thresholds are in terms of the population percentage, not a cutoff threshold for the predicted probability. A few different versions of tiebreaking are implemented to deal with the nuances of thresholding, and each result is written to the evaluations table for each metric score, along with some related statistics: worst_value - Ordering by the label ascending. This has the effect of as many predicted negatives making it above thresholds as possible, thus producing the worst possible score. best_value - Ordering by the label descending. This has the effect of as many predicted positives making it above thresholds as possible, thus producing the best possible score. stochastic_value - If the worst_value and best_value are not the same (as defined by the floating point tolerance at catwalk.evaluation.RELATIVE_TOLERANCE), the sorting/thresholding/evaluation will be redone many times, and the mean of all these trials is written to this column. Otherwise, the worst_value is written here num_sort_trials - If trials are needed to produce the stochastic_value , the number of trials taken is written here. Otherwise this will be 0 standard_deviation - If trials are needed to produce the stochastic_value , the standard deviation of these trials is written here. Otherwise this will be 0 Sometimes test matrices may not have labels for every row, so it's worth mentioning here how that is handled and interacts with thresholding. Rows with missing labels are not considered in the metric calculations, and if some of these rows are in the top k of the test matrix, no more rows are taken from the rest of the list for consideration. So if the experiment is calculating precision at the top 100 rows, and 40 of the top 100 rows are missing a label, the precision will actually be calculated on the 60 of the top 100 rows that do have a label. To make the results of this more transparent for users, a few extra pieces of metadata are written to the evaluations table for each metric score. num_labeled_examples - The number of rows in the test matrix that have labels num_labeled_above_threshold - The number of rows above the configured threshold for this metric score that have labels num_positive_labels - The number of positive labels in the test matrix Triage supports performing a bias audit using the Aequitas library, if a bias_audit_config is passed in configuration. This is handled first through creating a 'protected groups'table which retrieves the configured protected group information for each member of the cohort, and the time that this protected group information was first known. This table is named using a hash of the bias audit configuration, so data can be reused across experiments as long as the bias configuration does not change. A bias audit is performed alongside metric calculation time for each model that is built, on both the train and test matrices, and each subset. This is very similar to the evaluations table schema, in that for each slice of data that has evaluation metrics generated for it, also receives a bias audit. The change is that thresholds are not borrowed from the evaluation configuration, as aequitas audits are computationally expensive and large threshold grids are common in Triage experiments; the bias audit has its evaluation thresholds configured in the bias_audit_config . All data from the bias audit is saved to either the train_results.aequitas or test_results.aequitas tables. Triage also supports evaluating a model on a subset of the predictions made. This is done by passing a subset query in the prediction config. The model evaluator will then subset the predictions on valid entity-date pairs for the given model and will calculate metrics for the subset, re-applying thresholds as necessary to the predictions in the subset. Subset definitions are stored in the model_metadata.subsets table, and the evaluations are stored in the evaluations tables. A hash of the subset configuration identifies subset evaluations and links the subsets table. Recap # At this point, the 'model_metadata', 'train_results', and 'test_results' database schemas are fully populated with data about models, model groups, predictions, feature importances, and evaluation metrics for the researcher to query. In addition, the trained model pickle files are saved in the configured project path. The experiment is considered finished.","title":"Experiment Algorithm"},{"location":"experiments/algorithm/#experiment-algorithm-deep-dive","text":"This guide's purpose is to provide familiarity of the inner workings of a Triage Experiment to people with some experience in data science and Python. A Triage Experiment is a highly structured way of defining the experimentation phase of a data science project. To those wondering whether this Experiment structure is flexible enough to fit their needs, this should help.","title":"Experiment Algorithm Deep Dive"},{"location":"experiments/algorithm/#1-temporal-validation-setup","text":"First, the given temporal_config section in the experiment definition is transformed into train and test splits, including as_of_times for each matrix. We create these splits by figuring out the latest reasonable split time from the inputs, and moving backwards in time at the rate of the given model_update_frequency , until we get to the earliest reasonable split time. For each split, we create as_of_times by moving either backwards from the split time towards the max_training_history (for train matrices) or forwards from the split time towards the test_duration (for test matrices) at the provided data_frequency . Many of these configured values may be lists, in which case we generate the cross-product of all the possible values and generate more splits. For a more detailed look at the temporal validation logic, see Temporal Validation Deep Dive . The train and test splits themselves are not used until the Building Matrices section, but a flat list of all computed as_of_times for all matrices needed in the experiment is used in the next section, Transforming Data .","title":"1. Temporal Validation Setup"},{"location":"experiments/algorithm/#2-transforming-data","text":"With all of the as_of_times for this Experiment now computed, it's now possible to transform the input data into features and labels as of all the required times.","title":"2. Transforming Data"},{"location":"experiments/algorithm/#labels","text":"The Experiment populates a 'labels' table using the following input: 1. A query, provided by the user in the configuration file, that generates entity_ids and outcomes for a given as_of_date and label_timespan. Each as_of_date and label_timespan defined in temporal config For instance, an inspections-style query (for the given timespan, return the entity and outcome of any matching inspections) would look like: select events.entity_id, bool_or(outcome::bool)::integer as outcome from events where '{as_of_date}' <= outcome_date and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id This binary labels table is scoped to the entire Experiment, so all as_of_time (computed in step 1) and label_timespan (taken straight from temporal_config ) combinations are present. Additionally, the 'label_name' and 'label_type' are also recorded with each row in the table. The name of the labels table is based on both the name of the label and a hash of the label query (e.g labels_failedviolation_a0b1c2d3 ), so any prior experiments that shared both the name and query will be able to reuse the labels table. If the 'replace' flag was sent, for each as_of_time and label_timespan , the labels table is queried to check if any rows exist that match. If any such rows exist, the labels query for that date and timespan is not run. At this point, the 'labels' table may not have entries for all entities and dates that need to be in a given matrix. How these rows have their labels represented is up to the configured include_missing_labels_in_train_as value in the experiment. This value is not processed when we generate the labels table, but later on when the matrix is built (see 'Retrieving Data and Saving Completed Matrix')","title":"Labels"},{"location":"experiments/algorithm/#cohort-table","text":"The Experiment keeps track of the which entities are in the cohort on any given date. Similarly to the labels table, the experiment populates a cohort table using the following input: A query, provided by the user in the configuration file, that generates entity_ids for a given as_of_date. Each as_of_date as defined in temporal config This cohort table is scoped to the entire Experiment, so all as_of_times (computed in step 1) are present. The name of the cohort table is based on both the name of the cohort and a hash of the cohort query (e.g cohort_permitted_a0b1c2d3 ), so any prior experiments that shared both the name and query will be able to reuse the cohort table. If the 'replace' flag was sent, for each as_of_time , the cohort table is queried to check if any rows exist that match. If any such rows exist, the cohort query for that date is not run.","title":"Cohort Table"},{"location":"experiments/algorithm/#features","text":"Each provided feature_aggregation configures the creation and population of several feature tables in the 'features' schema: one for each of the groups specified in the config, one that merges the groups together into one table, and one that fills in null values from the merged table with imputed values based on imputation config.","title":"Features"},{"location":"experiments/algorithm/#recap","text":"At this point, we have at least three tables that are used to populate matrices: labels_{labelname}_{labelqueryhash} with computed labels for each date cohort_{cohortname}_{cohortqueryhash} with the cohort for each date A features.{prefix}_aggregation_imputed table for each feature aggregation present in the experiment config.","title":"Recap"},{"location":"experiments/algorithm/#3-building-matrices","text":"At this point, we have to build actual train and test matrices that can be processed by machine learning algorithms, save at the user's specified path, either on the local filesystem or s3 depending on the scheme portion of the path (e.g. s3://bucket-name/project_directory ) First we have to figure out exactly what matrices we have to build. The split definitions from step 1 are a good start -- they are our train and test splits -- but sometimes we also want to test different subsets of the data, like feature groups (e.g. 'how does using group of features A perform against using all features?'). So there's a layer of iteration we introduce for each split, that may produce many more matrices. What do we iterate over? * Feature List - All subsets of features that the user wants to cycle through. This is the end result of the feature group generation and mixing process, which is described more below. * Cohorts - In theory we can take in different cohorts and iterate in the same experiment. This is not fully implemented, so in reality we just use the one cohort that is passed in the cohort_config * Label names - In theory we can take in different labels (e.g. complaints, sustained complaints) in the same experiment. Right now there is no support for multiple label names, but the label name used is configurable through the optional 'label_config'->'name' config value * Label types - In theory we can take in different label types (e.g. binary) in the same experiment. Right now this isn't done, there is one label type and it is hardcoded as 'binary'.","title":"3. Building Matrices"},{"location":"experiments/algorithm/#feature-lists","text":"How do we arrive at the feature lists? There are two pieces of config that are used: feature group_definition and feature_group_strategies . Feature group definitions are just ways to define logical blocks of features, most often features that come from the same source, or describing a particular type of event. These groups within the experiment as a list of feature names, representing some subset of all potential features for the experiment. Feature group strategies are ways to take feature groups and mix them together in various ways. The feature group strategies take these subsets of features and convert them into another list of subsets of features, which is the final list iterated over to create different matrices.","title":"Feature Lists"},{"location":"experiments/algorithm/#iteration-and-matrix-creation","text":"At this point, matrices are created by looping through all train/test splits and data subsets (e.g. feature groups, state definitions), grabbing the data corresponding to each from the database, and assembling that data into a design matrix that is saved along with the metadata that defines it. As an example, if the experiment defines 3 train/test splits (one test per train in this example, for simplicity), 3 feature groups that are mixed using the 'leave-one-out' and 'all' strategies, and 1 state definition, we'll expect 18 matrices to be saved: 9 splits after multiplying the time splits by the feature groups, and each one creating a train and test matrix.","title":"Iteration and Matrix Creation"},{"location":"experiments/algorithm/#associating-matrices-with-experiment","text":"After all matrices for the Experiment are defined but before any are built, the Experiment is associated with each Matrix in the database through the model_metadata.experiment_matrices table. This means that whether or not the Experiment has to end up building a matrix, after the fact a user can query the database to see if it used said matrix.","title":"Associating Matrices with Experiment"},{"location":"experiments/algorithm/#recap_1","text":"At this point, all finished matrices and metadata will be saved under the project_path supplied by the user to the Experiment constructor, in the subdirectory matrices .","title":"Recap"},{"location":"experiments/algorithm/#4-running-models","text":"The last phase of an Experiment run uses the completed design matrices to train, test, and evaluate classifiers. This procedure writes a lot of metadata to the 3 schemas: 'model_metadata', 'train_results', and 'test_results'.","title":"4. Running Models"},{"location":"experiments/algorithm/#associating-models-with-experiment","text":"Every combination of training matrix + classifier + hyperparameter is considered a Model. Before any Models are trained, the Experiment is associated with each Model in the database through the model_metadata.experiment_models table. This means that whether or not the Experiment has to end up training a model, after the fact a user can query the database to see if it used said model.","title":"Associating Models with Experiment"},{"location":"experiments/algorithm/#train","text":"Each matrix marked for training is sent through the configured grid in the experiment's grid_config . This works much like the scikit-learn ParameterGrid (and in fact uses it on the backend). It cycles through all of the classifiers and hyperparameter combinations contained herein, and calls .fit() with that train matrix. Any classifier that adheres to the scikit-learn .fit/.transform interface and is available in the Python environment will work here, whether it is a standard scikit-learn classifier, a third-party library like XGBoost, or a custom-built one in the calling repository (for instance, one that implements the problem domain's baseline heuristic algorithm for comparison). Metadata about the trained classifier is written to the model_metadata.models Postgres table. The trained model is saved to a filename with the model hash (see Model Hash section below).","title":"Train"},{"location":"experiments/algorithm/#test-matrix","text":"For each test matrix, predictions, individual importances, and the user-specified testing evaluation metrics are written to the 'test_results' schema. For each train matrix, predictions and the user-specified training evaluation metrics are written to the 'train_results' schema.","title":"Test Matrix"},{"location":"experiments/algorithm/#individual-feature-importance","text":"Feature importances (of a configurable number of top features, defaulting to 5) for each prediction are computed and written to the test_results.individual_importances table. Right now, there are no sophisticated calculation methods integrated into the experiment; simply the top 5 global feature importances for the model are copied to the individual_importances table.","title":"Individual Feature Importance"},{"location":"experiments/algorithm/#metrics","text":"Triage allows for the computation of both testing set and training set evaluation metrics. Evaluation metrics, such as precision and recall at various thresholds, are written to either the train_results.evaluations table or the test_results.evaluations . Triage defines a number of Evaluation Metrics metrics that can be addressed by name in the experiment definition, along with a list of thresholds and/or other parameters (such as the 'beta' value for fbeta) to iterate through. Thresholding is done either via absolute value (top k) or percentile by sorting the predictions and labels by the row's predicted probability score, with ties broken in some way (see next paragraph), and assigning the predicted value as True for those above the threshold. Note that the percentile thresholds are in terms of the population percentage, not a cutoff threshold for the predicted probability. A few different versions of tiebreaking are implemented to deal with the nuances of thresholding, and each result is written to the evaluations table for each metric score, along with some related statistics: worst_value - Ordering by the label ascending. This has the effect of as many predicted negatives making it above thresholds as possible, thus producing the worst possible score. best_value - Ordering by the label descending. This has the effect of as many predicted positives making it above thresholds as possible, thus producing the best possible score. stochastic_value - If the worst_value and best_value are not the same (as defined by the floating point tolerance at catwalk.evaluation.RELATIVE_TOLERANCE), the sorting/thresholding/evaluation will be redone many times, and the mean of all these trials is written to this column. Otherwise, the worst_value is written here num_sort_trials - If trials are needed to produce the stochastic_value , the number of trials taken is written here. Otherwise this will be 0 standard_deviation - If trials are needed to produce the stochastic_value , the standard deviation of these trials is written here. Otherwise this will be 0 Sometimes test matrices may not have labels for every row, so it's worth mentioning here how that is handled and interacts with thresholding. Rows with missing labels are not considered in the metric calculations, and if some of these rows are in the top k of the test matrix, no more rows are taken from the rest of the list for consideration. So if the experiment is calculating precision at the top 100 rows, and 40 of the top 100 rows are missing a label, the precision will actually be calculated on the 60 of the top 100 rows that do have a label. To make the results of this more transparent for users, a few extra pieces of metadata are written to the evaluations table for each metric score. num_labeled_examples - The number of rows in the test matrix that have labels num_labeled_above_threshold - The number of rows above the configured threshold for this metric score that have labels num_positive_labels - The number of positive labels in the test matrix Triage supports performing a bias audit using the Aequitas library, if a bias_audit_config is passed in configuration. This is handled first through creating a 'protected groups'table which retrieves the configured protected group information for each member of the cohort, and the time that this protected group information was first known. This table is named using a hash of the bias audit configuration, so data can be reused across experiments as long as the bias configuration does not change. A bias audit is performed alongside metric calculation time for each model that is built, on both the train and test matrices, and each subset. This is very similar to the evaluations table schema, in that for each slice of data that has evaluation metrics generated for it, also receives a bias audit. The change is that thresholds are not borrowed from the evaluation configuration, as aequitas audits are computationally expensive and large threshold grids are common in Triage experiments; the bias audit has its evaluation thresholds configured in the bias_audit_config . All data from the bias audit is saved to either the train_results.aequitas or test_results.aequitas tables. Triage also supports evaluating a model on a subset of the predictions made. This is done by passing a subset query in the prediction config. The model evaluator will then subset the predictions on valid entity-date pairs for the given model and will calculate metrics for the subset, re-applying thresholds as necessary to the predictions in the subset. Subset definitions are stored in the model_metadata.subsets table, and the evaluations are stored in the evaluations tables. A hash of the subset configuration identifies subset evaluations and links the subsets table.","title":"Metrics"},{"location":"experiments/algorithm/#recap_2","text":"At this point, the 'model_metadata', 'train_results', and 'test_results' database schemas are fully populated with data about models, model groups, predictions, feature importances, and evaluation metrics for the researcher to query. In addition, the trained model pickle files are saved in the configured project path. The experiment is considered finished.","title":"Recap"},{"location":"experiments/architecture/","text":"Experiment Architecture # mermaid.initialize({startOnLoad:true}); This document is aimed at people wishing to contribute to Triage development. It explains the design and architecture of the Experiment class. Dependency Graphs # For a general overview of how the parts of an experiment depend on each other, refer to the graphs below. Experiment (high-level) # graph TD TC[Timechop] subgraph Architect LG[Label Generator] EDG[Entity-Date Generator] FG[\"Feature Generator (+ feature groups)\"] MB[Matrix Builder] end subgraph Catwalk, per-model MT[Model Trainer] PR[Predictor] PG[Protected Group Generator] EV[Model Evaluator] end TC \u2192 LG TC \u2192 EDG TC \u2192 FG LG \u2192 MB EDG \u2192 MB FG \u2192 MB MB \u2192 MT MB \u2192 PR MT \u2192 PR EDG \u2192 PG PG \u2192 EV PR \u2192 EV The FeatureGenerator section above hides some details to make the overall graph flow more concise. To support feature grouping, there are more operations that happen between feature table creation and matrix building. The relevant section of the dependency graph is expanded below, along with the output that each pair of components sends between each other within the arrow Feature Dependency Details # graph TD TC[Timechop] FG[Feature Generator] FDG[Feature Dictionary Generator] FGC[Feature Group Creator] FGM[Feature Group Mixer] PL[Planner] MB[Matrix Builder] TC -- as-of-dates \u2192 FG FG -- feature tables \u2192 FDG FDG -- master feature dictionary \u2192 FGC FGC -- feature groups \u2192 FGM FGM -- recombined feature groups \u2192 PL TC -- time splits \u2192 PL FG -- feature tables \u2192 MB PL -- matrix build tasks \u2192 MB Component List and Input/Output # These are where the interesting data science work is done. Timechop (temporal cross-validation) Architect (design matrix creation) Entity-Date_ Table Generator Label Generator Feature Generator Feature Dictionary Creator Feature Group Creator Feature Group Mixer Planner Matrix Builder Catwalk (modeling) Model Train/Tester Model Grouper Model Trainer Predictor Protected Group Table Generator Model Evaluator Individual Importance Calculator Timechop # Timechop does the necessary temporal math to set up temporal cross-validation. It 'chops' time according to config into train-test split definitions, which other components use. Input temporal_config in experiment config Output Time splits containing temporal cross-validation definition, including each as_of_date to be included in the matrices in each time split Entity-Date Table Generator # The EntityDateTableGenerator manages entity-date tables (including cohort and subset tables) by running the configured query for a number of different as_of_dates . Input All unique as_of_dates needed by matrices in the experiment, as provided by Timechop query and name from cohort_config or subsets in the scoring section in an experiment config entity-date table name that the caller wants to use Output An entity-date table in the database, consisting of entity ids and dates Label Generator # The LabelGenerator manages a labels table by running the configured label query for a number of different as_of_dates and label_timespans . Input All unique as_of_dates and label_timespans , needed by matrices in the experiment, as provided by Timechop query and name from label_config in experiment config Output A labels table in the database, consisting of entity ids, dates, and boolean labels Feature Generator # The FeatureGenerator manages a number of features tables by converting the configured feature_aggregations into collate.Spacetime objects, and then running the queries generated by collate . For each feature_aggregation , it runs a few passes: Optionally, convert a complex from object (e.g. the FROM part of the configured aggregation query) into an indexed table for speed. Create a number of empty tables at different GROUP BY levels (e.g. entity_id , zip_code ). and run inserts individually for each as_of_date . These inserts are split up into individual tasks and parallelized for speed. Roll up the GROUP BY tables from step 1 to the entity_id level with a single LEFT JOIN query. Use the cohort table to find all members of the cohort not present in the table from step 2 and create a new table with all members of the cohort, null values filled in with values based on the rules in the feature_aggregations config. Input All unique as_of_dates needed by matrices in the experiment, and the start time for features, as provided by Timechop The populated cohort table, as provided by Entity-Date Table Generator feature_aggregations in experiment config Output Populated feature tables in the database, one for each feature_aggregation Feature Dictionary Creator # Summarizes the feature tables created by FeatureGenerator into a dictionary more easily usable for feature grouping and serialization purposes. Does this by querying the database's information_schema . Input Names of feature tables and the index of each table, as provided by Feature Generator Output A master feature dictionary, consisting of each populated feature table and all of its feature column names. Feature Group Creator # Creates feature groups by taking the configured feature grouping rules and applying them to the master feature dictionary, to create a collection of smaller feature dictionaries. Input Master feature dictionary, as provided by Feature Dictionary Creator feature_group_definition in experiment config Output List of feature dictionaries, each representing one feature group Feature Group Mixer # Combines feature groups into new ones based on the configured rules (e.g. leave-one-out , leave-one-in ). Input List of feature dictionaries, as provided by Feature Group Creator feature_group_strategies in experiment config Output List of feature dictionaries, each representing one or more feature groups. Planner # Mixes time split definitions and feature groups to create the master list of matrices that are required for modeling to proceed. Input List of feature dictionaries, as provided by Feature Group Mixer List of matrix split definitions, as provided by Timechop user_metadata , in experiment config feature_start_time from temporal_config in experiment config cohort name from cohort_config in experiment config label name from cohort_config in experiment config Output List of serializable matrix build tasks, consisting of everything needed to build a single matrix: list of as-of-dates a label name a label type a feature dictionary matrix uuid matrix metadata matrix type (train or test) Matrix Builder # Takes matrix build tasks from the Planner and builds them if they don't already exist. Input A matrix build task, as provided by Planner include_missing_labels_in_train_as from label_config in experiment config The experiment's MatrixStorageEngine Output The built matrix saved in the MatrixStorageEngine A row describing the matrix saved in the database's model_metadata.matrices table. ModelTrainTester # A meta-component of sorts. Encompasses all of the other catwalk components. Input One temporal split, as provided by Timechop grid_config in experiment config Fully configured ModelTrainer , Predictor , ModelEvaluator , Individual Importance Calculator objects Output All of its components are run, resulting in trained models, predictions, evaluation metrics, and individual importances ModelGrouper # Assigns a model group to each model based on its metadata. Input model_group_keys in experiment config All the data about a particular model neded to decide a model group for the model: classifier name, hyperparameter list, and matrix metadata, as provided by ModelTrainer Output a model group id corresponding to a row in the model_metadata.model_groups table, either a matching one that already existed in the table or one that it autoprovisioned. ModelTrainer # Trains a model, stores it, and saves its metadata (including model group information and feature importances) to the database. Each model to be trained is expressed as a serializable task so that it can be parallelized. Input an instance of the ModelGrouper class. the experiment's ModelStorageEngine a MatrixStore object an importable classifier path and a set of hyperparameters Output - a row in the database's model_metadata.model_groups table, the model_metadata.models table, and rows in train_results.feature_importances for each feature. - the trained model persisted in the ModelStorageEngine Predictor # Generates predictions for a given model and matrix, both returning them for immediate use and saving them to the database. Input The experiment's Model Storage Engine A model id corresponding to a row from the database A MatrixStore object Output The predictions as an array Each prediction saved to the database, unless configured not to. The table they are stored in depends on which type of matrix it is (e.g. test_results.predictions or train_results.predictions ) Protected Group Table Generator # Generates a table containing protected group attributes (e.g. race, sex, age). Input A cohort table name and its configuration's unique hash Bias audit configuration, specifically a from object (either a table or query), and column names in the from object for protected attributes, knowledge date, and entity id. A name for the protected groups table Output A protected groups table, containing all rows from the cohort and any protected group information present in the from object, as well as the cohort hash so multiple cohorts can live in the same table. ModelEvaluator # Generates evaluation metrics for a given model and matrix over the entire matrix and for any subsets. Input scoring in experiment config array of predictions the MatrixStore and model_id that the predictions were generated from the subset to be evaluated (or None for the whole matrix) the reference group and thresholding rules from bias_audit_config in experiment config the protected group generator object (for retrieving protected group data) Output A row in the database for each evaluation metric for each subset. The table they are stored in depends on which type of matrix it is (e.g. test_results.evaluations or train_results.evaluations ). A row in the database for each Aequitas bias report. Either test_results.aequitas or train_results.aequitas . Individual Importance Calculator # Generates the top n feature importances for each entity in a given model. Input individual_importance_config in experiment config. model id a MatrixStore object for a test matrix an as-of-date Output rows in the test_results.individual_importances table for the model, date, and matrix based on the configured method and number of top features per entity. General Class Design # The Experiment class is designed to have all work done by component objects that reside as attributes on the instance. The purpose of this is to maximize the reuse potential of the components outside of the Experiment, as well as avoid excessive class inheritance within the Experiment. The inheritance tree of the Experiment is reserved for execution concerns , such as switching between singlethreaded, multiprocess, or cluster execution. To enable these different execution contexts without excessive duplicated code, the components that cover computationally or memory-intensive work generally implement methods to generate a collection of serializable tasks to perform later, on either that same object or perhaps another one running in another process or machine. The subclasses of Experiment then differentiate themselves by implementing methods to execute a collection of these tasks using their preferred method of execution, whether it be a simple loop, a process pool, or a cluster. The components are created and experiment configuration is bound to them at Experiment construction time, so that the instance methods can have concise call signatures that only cover the information passed by other components mid-experiment. Data reuse/replacement is handled within components. The Experiment generally just hands the replace flag to each component at object construction, and at runtime each component uses that and determines whether or not the needed work has already been done. I'm trying to find some behavior. Where does it reside? # If you're looking to change behavior of the Experiment, When possible, the logic resides in one of the components and hopefully the component list above should be helpful at finding the lines between components. Logic that specifically relates to parallel execution is in one of the experiment subclasses (see parallelization section below). Everything else is in the Experiment base class . This is where the public interface ( .run() ) resides, and follows a template method pattern to define the skeleton of the Experiment: instantating components based on experiment configuration and runtime inputs, and passing output from one component to another. I want to add a new option. Where should I put it? # Generally, the experiment configuration is where any new options go that change any data science-related functionality; in other words, if you could conceivably get better precision from the change, it should make it into experiment configuration. This is so the hashed experiment config is meaningful and the experiment can be audited by looking at the experiment configuration rather than requiring the perusal of custom code. The blind spot in this is, of course, the state of the database, which can always change results, but it's useful for database state to continue to be the only exception to this rule. On the other hand, new options that affect only runtime concerns (e.g. performance boosts) should go as arguments to the Experiment. For instance, changing the number of cores to use for matrix building, or telling it to skip predictions won't change the answer you're looking for; options like these just help you potentially get to the answer faster. Once an experiment is completed, runtime flags like these should be totally safe to ignore in analysis. Storage Abstractions # Another important part of enabling different execution contexts is being able to pass large, persisted objects (e.g. matrices or models) by reference to another process or cluster. To achieve this, as well as provide the ability to configure different storage mediums (e.g. S3) and formats (e,g, HDF) without changes to the Experiment class, all references to these large objects within any components are handled through an abstraction layer. Matrix Storage # All interactions with individual matrices and their bundled metadata are handled through MatrixStore objects. The storage medium is handled through a base Store object that is an attribute of the MatrixStore . The storage format is handled through inheritance on the MatrixStore : Each subclass, such as CSVMatrixStore or HDFMatrixStore , implements the necessary methods ( save , load , head_of_matrix ) to properly persist or load a matrix from its storage. In addition, the MatrixStore provides a variety of methods to retrieve data from either the base matrix itself or its metadata. For instance (this is not meant to be a complete list): matrix - the raw matrix metadata - the raw metadata dictionary exists - whether or not it exists in storage columns - the column list labels - the label column uuid - the matrix's UUID as_of_dates - the matrix's list of as-of-dates One MatrixStorageEngine exists at the Experiment level, and roughly corresponds with a directory wherever matrices are stored. Its only interface is to provide a MatrixStore object given a matrix UUID. Model Storage # Model storage is handled similarly to matrix storage, although the interactions with it are far simpler so there is no single-model class akin to the MatrixStore . One ModelStorageEngine exists at the Experiment level, configured with the Experiment's storage medium, and through it trained models can be saved or loaded. The ModelStorageEngine uses joblib to save and load compressed pickles of the model. Miscellaneous Project Storage # Both the ModelStorageEngine and MatrixStorageEngine are based on a more general storage abstraction that is suitable for any other auxiliary objects (e.g. graph images) that need to be stored. That is the ProjectStorage object, which roughly corresponds to a directory on some storage medium where we store everything. One of these exists as an Experiment attribute, and its interface .get_store can be used to persist or load whatever is needed. Parallelization/Subclassing Details # In the Class Design section above, we introduced tasks for parallelization and subclassing for execution changes. In this section, we expand on these to help provide a new guide to working with these. Currently there are three methods that must be implemented by subclasses of Experiment in order to be fully functional. Abstract Methods # process_query_tasks - Run feature generation queries. Receives a list of tasks. each task actually represents a table and is split into three lists of queries to enable the implementation to avoid deadlocks: prepare (table creation), inserts (a collection of INSERT INTO SELECT queries), and finalize (indexing). prepare needs to be run before the inserts and finalize is best run after the inserts, so it is advised that only the inserts are parallelized. The subclass should run each individual batch of queries by calling self.feature_generator.run_commands([list of queries]) , which will run all of the queries serially, so the implementation can send a batch of queries to each worker instead of having each individual query be on a new worker. process_matrix_build_tasks - Run matrix build tasks (that assume all the necessary label/cohort/feature tables have been built). Receives a dictionary of tasks. Each key is a matrix UUID, and each value is a dictionary that has all the necessary keyword arguments to call self.matrix_builder.build_matrix to build one matrix. process_train_test_batches - Run model train/test task batches (that assume all matrices are built). Receives a list of triage.component.catwalk.TaskBatch objects, each of which has a list of tasks, a description of those tasks, and whether or not that batch is safe to run in parallel. Within this, each task is a dictionary that has all the necessary keyword arguments to call self.model_train_tester.process_task to train and test one model. Each task covers model training, prediction (on both test and train matrices), model evaluation (on both test and train matrices), and saving of global and individual feature importances. Reference Implementations # SingleThreadedExperiment is a barebones implementation that runs everything serially. MultiCoreExperiment utilizes local multiprocessing to run tasks through a worker pool. Reading this is helpful to see the minimal implementation needed for some parallelization. RQExperiment - utilizes an RQ worker cluster to allow the tasks to be parallelized either locally or distributed to other. Does not take care of spawning a cluster or any other infrastructural concerns: it expects that the cluster is running somewhere and is reading from the same Redis instance that is passed to the RQExperiment . The RQExperiment simply enqueues tasks and waits for them to be completed. Reading this is helpful as a simple example of how to enable distributed computing.","title":"Experiment Architecture"},{"location":"experiments/architecture/#experiment-architecture","text":"mermaid.initialize({startOnLoad:true}); This document is aimed at people wishing to contribute to Triage development. It explains the design and architecture of the Experiment class.","title":"Experiment Architecture"},{"location":"experiments/architecture/#dependency-graphs","text":"For a general overview of how the parts of an experiment depend on each other, refer to the graphs below.","title":"Dependency Graphs"},{"location":"experiments/architecture/#experiment-high-level","text":"graph TD TC[Timechop] subgraph Architect LG[Label Generator] EDG[Entity-Date Generator] FG[\"Feature Generator (+ feature groups)\"] MB[Matrix Builder] end subgraph Catwalk, per-model MT[Model Trainer] PR[Predictor] PG[Protected Group Generator] EV[Model Evaluator] end TC \u2192 LG TC \u2192 EDG TC \u2192 FG LG \u2192 MB EDG \u2192 MB FG \u2192 MB MB \u2192 MT MB \u2192 PR MT \u2192 PR EDG \u2192 PG PG \u2192 EV PR \u2192 EV The FeatureGenerator section above hides some details to make the overall graph flow more concise. To support feature grouping, there are more operations that happen between feature table creation and matrix building. The relevant section of the dependency graph is expanded below, along with the output that each pair of components sends between each other within the arrow","title":"Experiment (high-level)"},{"location":"experiments/architecture/#feature-dependency-details","text":"graph TD TC[Timechop] FG[Feature Generator] FDG[Feature Dictionary Generator] FGC[Feature Group Creator] FGM[Feature Group Mixer] PL[Planner] MB[Matrix Builder] TC -- as-of-dates \u2192 FG FG -- feature tables \u2192 FDG FDG -- master feature dictionary \u2192 FGC FGC -- feature groups \u2192 FGM FGM -- recombined feature groups \u2192 PL TC -- time splits \u2192 PL FG -- feature tables \u2192 MB PL -- matrix build tasks \u2192 MB","title":"Feature Dependency Details"},{"location":"experiments/architecture/#component-list-and-inputoutput","text":"These are where the interesting data science work is done. Timechop (temporal cross-validation) Architect (design matrix creation) Entity-Date_ Table Generator Label Generator Feature Generator Feature Dictionary Creator Feature Group Creator Feature Group Mixer Planner Matrix Builder Catwalk (modeling) Model Train/Tester Model Grouper Model Trainer Predictor Protected Group Table Generator Model Evaluator Individual Importance Calculator","title":"Component List and Input/Output"},{"location":"experiments/architecture/#timechop","text":"Timechop does the necessary temporal math to set up temporal cross-validation. It 'chops' time according to config into train-test split definitions, which other components use. Input temporal_config in experiment config Output Time splits containing temporal cross-validation definition, including each as_of_date to be included in the matrices in each time split","title":"Timechop"},{"location":"experiments/architecture/#entity-date-table-generator","text":"The EntityDateTableGenerator manages entity-date tables (including cohort and subset tables) by running the configured query for a number of different as_of_dates . Input All unique as_of_dates needed by matrices in the experiment, as provided by Timechop query and name from cohort_config or subsets in the scoring section in an experiment config entity-date table name that the caller wants to use Output An entity-date table in the database, consisting of entity ids and dates","title":"Entity-Date Table Generator"},{"location":"experiments/architecture/#label-generator","text":"The LabelGenerator manages a labels table by running the configured label query for a number of different as_of_dates and label_timespans . Input All unique as_of_dates and label_timespans , needed by matrices in the experiment, as provided by Timechop query and name from label_config in experiment config Output A labels table in the database, consisting of entity ids, dates, and boolean labels","title":"Label Generator"},{"location":"experiments/architecture/#feature-generator","text":"The FeatureGenerator manages a number of features tables by converting the configured feature_aggregations into collate.Spacetime objects, and then running the queries generated by collate . For each feature_aggregation , it runs a few passes: Optionally, convert a complex from object (e.g. the FROM part of the configured aggregation query) into an indexed table for speed. Create a number of empty tables at different GROUP BY levels (e.g. entity_id , zip_code ). and run inserts individually for each as_of_date . These inserts are split up into individual tasks and parallelized for speed. Roll up the GROUP BY tables from step 1 to the entity_id level with a single LEFT JOIN query. Use the cohort table to find all members of the cohort not present in the table from step 2 and create a new table with all members of the cohort, null values filled in with values based on the rules in the feature_aggregations config. Input All unique as_of_dates needed by matrices in the experiment, and the start time for features, as provided by Timechop The populated cohort table, as provided by Entity-Date Table Generator feature_aggregations in experiment config Output Populated feature tables in the database, one for each feature_aggregation","title":"Feature Generator"},{"location":"experiments/architecture/#feature-dictionary-creator","text":"Summarizes the feature tables created by FeatureGenerator into a dictionary more easily usable for feature grouping and serialization purposes. Does this by querying the database's information_schema . Input Names of feature tables and the index of each table, as provided by Feature Generator Output A master feature dictionary, consisting of each populated feature table and all of its feature column names.","title":"Feature Dictionary Creator"},{"location":"experiments/architecture/#feature-group-creator","text":"Creates feature groups by taking the configured feature grouping rules and applying them to the master feature dictionary, to create a collection of smaller feature dictionaries. Input Master feature dictionary, as provided by Feature Dictionary Creator feature_group_definition in experiment config Output List of feature dictionaries, each representing one feature group","title":"Feature Group Creator"},{"location":"experiments/architecture/#feature-group-mixer","text":"Combines feature groups into new ones based on the configured rules (e.g. leave-one-out , leave-one-in ). Input List of feature dictionaries, as provided by Feature Group Creator feature_group_strategies in experiment config Output List of feature dictionaries, each representing one or more feature groups.","title":"Feature Group Mixer"},{"location":"experiments/architecture/#planner","text":"Mixes time split definitions and feature groups to create the master list of matrices that are required for modeling to proceed. Input List of feature dictionaries, as provided by Feature Group Mixer List of matrix split definitions, as provided by Timechop user_metadata , in experiment config feature_start_time from temporal_config in experiment config cohort name from cohort_config in experiment config label name from cohort_config in experiment config Output List of serializable matrix build tasks, consisting of everything needed to build a single matrix: list of as-of-dates a label name a label type a feature dictionary matrix uuid matrix metadata matrix type (train or test)","title":"Planner"},{"location":"experiments/architecture/#matrix-builder","text":"Takes matrix build tasks from the Planner and builds them if they don't already exist. Input A matrix build task, as provided by Planner include_missing_labels_in_train_as from label_config in experiment config The experiment's MatrixStorageEngine Output The built matrix saved in the MatrixStorageEngine A row describing the matrix saved in the database's model_metadata.matrices table.","title":"Matrix Builder"},{"location":"experiments/architecture/#modeltraintester","text":"A meta-component of sorts. Encompasses all of the other catwalk components. Input One temporal split, as provided by Timechop grid_config in experiment config Fully configured ModelTrainer , Predictor , ModelEvaluator , Individual Importance Calculator objects Output All of its components are run, resulting in trained models, predictions, evaluation metrics, and individual importances","title":"ModelTrainTester"},{"location":"experiments/architecture/#modelgrouper","text":"Assigns a model group to each model based on its metadata. Input model_group_keys in experiment config All the data about a particular model neded to decide a model group for the model: classifier name, hyperparameter list, and matrix metadata, as provided by ModelTrainer Output a model group id corresponding to a row in the model_metadata.model_groups table, either a matching one that already existed in the table or one that it autoprovisioned.","title":"ModelGrouper"},{"location":"experiments/architecture/#modeltrainer","text":"Trains a model, stores it, and saves its metadata (including model group information and feature importances) to the database. Each model to be trained is expressed as a serializable task so that it can be parallelized. Input an instance of the ModelGrouper class. the experiment's ModelStorageEngine a MatrixStore object an importable classifier path and a set of hyperparameters Output - a row in the database's model_metadata.model_groups table, the model_metadata.models table, and rows in train_results.feature_importances for each feature. - the trained model persisted in the ModelStorageEngine","title":"ModelTrainer"},{"location":"experiments/architecture/#predictor","text":"Generates predictions for a given model and matrix, both returning them for immediate use and saving them to the database. Input The experiment's Model Storage Engine A model id corresponding to a row from the database A MatrixStore object Output The predictions as an array Each prediction saved to the database, unless configured not to. The table they are stored in depends on which type of matrix it is (e.g. test_results.predictions or train_results.predictions )","title":"Predictor"},{"location":"experiments/architecture/#protected-group-table-generator","text":"Generates a table containing protected group attributes (e.g. race, sex, age). Input A cohort table name and its configuration's unique hash Bias audit configuration, specifically a from object (either a table or query), and column names in the from object for protected attributes, knowledge date, and entity id. A name for the protected groups table Output A protected groups table, containing all rows from the cohort and any protected group information present in the from object, as well as the cohort hash so multiple cohorts can live in the same table.","title":"Protected Group Table Generator"},{"location":"experiments/architecture/#modelevaluator","text":"Generates evaluation metrics for a given model and matrix over the entire matrix and for any subsets. Input scoring in experiment config array of predictions the MatrixStore and model_id that the predictions were generated from the subset to be evaluated (or None for the whole matrix) the reference group and thresholding rules from bias_audit_config in experiment config the protected group generator object (for retrieving protected group data) Output A row in the database for each evaluation metric for each subset. The table they are stored in depends on which type of matrix it is (e.g. test_results.evaluations or train_results.evaluations ). A row in the database for each Aequitas bias report. Either test_results.aequitas or train_results.aequitas .","title":"ModelEvaluator"},{"location":"experiments/architecture/#individual-importance-calculator","text":"Generates the top n feature importances for each entity in a given model. Input individual_importance_config in experiment config. model id a MatrixStore object for a test matrix an as-of-date Output rows in the test_results.individual_importances table for the model, date, and matrix based on the configured method and number of top features per entity.","title":"Individual Importance Calculator"},{"location":"experiments/architecture/#general-class-design","text":"The Experiment class is designed to have all work done by component objects that reside as attributes on the instance. The purpose of this is to maximize the reuse potential of the components outside of the Experiment, as well as avoid excessive class inheritance within the Experiment. The inheritance tree of the Experiment is reserved for execution concerns , such as switching between singlethreaded, multiprocess, or cluster execution. To enable these different execution contexts without excessive duplicated code, the components that cover computationally or memory-intensive work generally implement methods to generate a collection of serializable tasks to perform later, on either that same object or perhaps another one running in another process or machine. The subclasses of Experiment then differentiate themselves by implementing methods to execute a collection of these tasks using their preferred method of execution, whether it be a simple loop, a process pool, or a cluster. The components are created and experiment configuration is bound to them at Experiment construction time, so that the instance methods can have concise call signatures that only cover the information passed by other components mid-experiment. Data reuse/replacement is handled within components. The Experiment generally just hands the replace flag to each component at object construction, and at runtime each component uses that and determines whether or not the needed work has already been done.","title":"General Class Design"},{"location":"experiments/architecture/#im-trying-to-find-some-behavior-where-does-it-reside","text":"If you're looking to change behavior of the Experiment, When possible, the logic resides in one of the components and hopefully the component list above should be helpful at finding the lines between components. Logic that specifically relates to parallel execution is in one of the experiment subclasses (see parallelization section below). Everything else is in the Experiment base class . This is where the public interface ( .run() ) resides, and follows a template method pattern to define the skeleton of the Experiment: instantating components based on experiment configuration and runtime inputs, and passing output from one component to another.","title":"I'm trying to find some behavior. Where does it reside?"},{"location":"experiments/architecture/#i-want-to-add-a-new-option-where-should-i-put-it","text":"Generally, the experiment configuration is where any new options go that change any data science-related functionality; in other words, if you could conceivably get better precision from the change, it should make it into experiment configuration. This is so the hashed experiment config is meaningful and the experiment can be audited by looking at the experiment configuration rather than requiring the perusal of custom code. The blind spot in this is, of course, the state of the database, which can always change results, but it's useful for database state to continue to be the only exception to this rule. On the other hand, new options that affect only runtime concerns (e.g. performance boosts) should go as arguments to the Experiment. For instance, changing the number of cores to use for matrix building, or telling it to skip predictions won't change the answer you're looking for; options like these just help you potentially get to the answer faster. Once an experiment is completed, runtime flags like these should be totally safe to ignore in analysis.","title":"I want to add a new option. Where should I put it?"},{"location":"experiments/architecture/#storage-abstractions","text":"Another important part of enabling different execution contexts is being able to pass large, persisted objects (e.g. matrices or models) by reference to another process or cluster. To achieve this, as well as provide the ability to configure different storage mediums (e.g. S3) and formats (e,g, HDF) without changes to the Experiment class, all references to these large objects within any components are handled through an abstraction layer.","title":"Storage Abstractions"},{"location":"experiments/architecture/#matrix-storage","text":"All interactions with individual matrices and their bundled metadata are handled through MatrixStore objects. The storage medium is handled through a base Store object that is an attribute of the MatrixStore . The storage format is handled through inheritance on the MatrixStore : Each subclass, such as CSVMatrixStore or HDFMatrixStore , implements the necessary methods ( save , load , head_of_matrix ) to properly persist or load a matrix from its storage. In addition, the MatrixStore provides a variety of methods to retrieve data from either the base matrix itself or its metadata. For instance (this is not meant to be a complete list): matrix - the raw matrix metadata - the raw metadata dictionary exists - whether or not it exists in storage columns - the column list labels - the label column uuid - the matrix's UUID as_of_dates - the matrix's list of as-of-dates One MatrixStorageEngine exists at the Experiment level, and roughly corresponds with a directory wherever matrices are stored. Its only interface is to provide a MatrixStore object given a matrix UUID.","title":"Matrix Storage"},{"location":"experiments/architecture/#model-storage","text":"Model storage is handled similarly to matrix storage, although the interactions with it are far simpler so there is no single-model class akin to the MatrixStore . One ModelStorageEngine exists at the Experiment level, configured with the Experiment's storage medium, and through it trained models can be saved or loaded. The ModelStorageEngine uses joblib to save and load compressed pickles of the model.","title":"Model Storage"},{"location":"experiments/architecture/#miscellaneous-project-storage","text":"Both the ModelStorageEngine and MatrixStorageEngine are based on a more general storage abstraction that is suitable for any other auxiliary objects (e.g. graph images) that need to be stored. That is the ProjectStorage object, which roughly corresponds to a directory on some storage medium where we store everything. One of these exists as an Experiment attribute, and its interface .get_store can be used to persist or load whatever is needed.","title":"Miscellaneous Project Storage"},{"location":"experiments/architecture/#parallelizationsubclassing-details","text":"In the Class Design section above, we introduced tasks for parallelization and subclassing for execution changes. In this section, we expand on these to help provide a new guide to working with these. Currently there are three methods that must be implemented by subclasses of Experiment in order to be fully functional.","title":"Parallelization/Subclassing Details"},{"location":"experiments/architecture/#abstract-methods","text":"process_query_tasks - Run feature generation queries. Receives a list of tasks. each task actually represents a table and is split into three lists of queries to enable the implementation to avoid deadlocks: prepare (table creation), inserts (a collection of INSERT INTO SELECT queries), and finalize (indexing). prepare needs to be run before the inserts and finalize is best run after the inserts, so it is advised that only the inserts are parallelized. The subclass should run each individual batch of queries by calling self.feature_generator.run_commands([list of queries]) , which will run all of the queries serially, so the implementation can send a batch of queries to each worker instead of having each individual query be on a new worker. process_matrix_build_tasks - Run matrix build tasks (that assume all the necessary label/cohort/feature tables have been built). Receives a dictionary of tasks. Each key is a matrix UUID, and each value is a dictionary that has all the necessary keyword arguments to call self.matrix_builder.build_matrix to build one matrix. process_train_test_batches - Run model train/test task batches (that assume all matrices are built). Receives a list of triage.component.catwalk.TaskBatch objects, each of which has a list of tasks, a description of those tasks, and whether or not that batch is safe to run in parallel. Within this, each task is a dictionary that has all the necessary keyword arguments to call self.model_train_tester.process_task to train and test one model. Each task covers model training, prediction (on both test and train matrices), model evaluation (on both test and train matrices), and saving of global and individual feature importances.","title":"Abstract Methods"},{"location":"experiments/architecture/#reference-implementations","text":"SingleThreadedExperiment is a barebones implementation that runs everything serially. MultiCoreExperiment utilizes local multiprocessing to run tasks through a worker pool. Reading this is helpful to see the minimal implementation needed for some parallelization. RQExperiment - utilizes an RQ worker cluster to allow the tasks to be parallelized either locally or distributed to other. Does not take care of spawning a cluster or any other infrastructural concerns: it expects that the cluster is running somewhere and is reading from the same Redis instance that is passed to the RQExperiment . The RQExperiment simply enqueues tasks and waits for them to be completed. Reading this is helpful as a simple example of how to enable distributed computing.","title":"Reference Implementations"},{"location":"experiments/cohort-labels/","text":"Cohort and Label Deep Dive # This document is intended at providing a deep dive into the concepts of cohorts and labels as they apply to Triage. For context, reading the Triage section of the Dirty Duck tutorial may be helpful before reading this document. Temporal Validation Refresher # Triage uses temporal validation to select models because the real-world problems that Triage is built for tend to evolve or change over time. Picking a date range to train on and a date range afterwards to test on ensures that we don't leak data from the future into our models that wouldn't be available in a real-world deployment scenario. Because of this, we often talk in Triage about the as-of-date : all models trained by Triage are associated with an as-of-date , which means that all the data that goes into the model is only included if it was known about before that date . The matrix used to train the model may have multiple as-of-dates , and the most recent is referred to as the model's as-of-date so it's easy to see the cutoff date for data included in the model. For more on temporal validation, see the relevant section in Dirty Duck . What are Cohorts and Labels in Triage? # This document assumes that the reader is familiar with the concept of a machine learning target variable and will focus on explaining what is unique to Triage. A cohort is the population used used for modeling on a given as-of-date . This is expressed as a list of entities . An entity is simply the object of prediction, such as a facility to inspect or a patient coming in for a visit. Early warning systems tend to include their entire population (or at least a large subset of it) in the cohort at any given date, while appointment-based problems may only include in a date's cohort the people who are scheduled for an appointment on that date. A label is the binary target variable for a member of the cohort at a given as-of-date and a given label timespan. For instance, in an inspection prioritization problem the question being asked may be 'what facilities are at high risk of having a failed inspection in the next 6 months?' For this problem, the label_timespan is 6 months. There may be multiple label timespans tested in the same experiment, in which case there could be multiple labels for an entity and date. In addition, multiple label definitions are often tested against each other, such as \"any inspection failures\" vs \"inspection failures with serious issues\". Both labels and cohorts are defined in Triage's experiment configuration using SQL queries, with the variables ( as_of_date , label_timespan ) given as placeholders. This allows the definitions to be given in a concise manner while allowing the temporal configuration defined elsewhere in the experiment to produce the actual list of dates and timespans that are calculated during the experiment. Cohort Definition and Examples # The cohort is configured with a query that returns a unique list of entity_id s given an as_of_date , and it runs the query for each as_of_date that is produced by your temporal config. You tell Triage where to place each as_of_date with a placeholder surrounded by brackets: {as_of_date} . Note 1 # The as_of_date is parsed as a timestamp in the database, which Postgres defaults to midnight at the beginning of the date in question . It's important to consider how this is used for feature generation. Features are only included if they are known about before this timestamp . So features will be only included for an as_of_date if they are known about before that as_of_date . If you want to work around this (e.g for visit-level problems in which you want to intake data on the day of the visit and make predictions using that data the same day ), you can move your cohort up a day. The time splitting in Triage is designed for day granularity so approaches to train up to a specific hour and test at another hour of the same day are not supported. Note 2 # Triage expects all entity ids to be integers. Note 3 # Triage expects the cohort to be a unique list of entity ids. Throughout the cohort example queries you will see distinct(entity_id) used to ensure this. Example: Inspections # Let's say I am prioritizing the inspection of food service facilities such as restaurants, caterers or grocery stores. One simple definition of a cohort for facility inspection would be to include any facilities that have active permits in the last year in the cohort. Assume that these permits are contained in a table, named permits , with the facility's id, a start date, and an end date of the permit. Inspections Cohort Source Table entity_id start_date end_date 25 2016-01-01 2016-02-01 44 2016-01-01 2016-02-01 25 2016-02-01 2016-03-01 Triage expects the cohort query passed to it to return a unique list of entity_id s given an as_of_date , and it runs the query for each as_of_date that is produced by your temporal config. You tell Triage where to place each as_of_date with a placeholder surrounded by brackets: {as_of_date} . An example query that implements the 'past year' definition would be: select distinct(entity_id) from permits where tsrange(start_date, end_date, '[]') @> {as_of_date} Running this query using the as_of_date '2017-01-15' would return both entity ids 25 and 44. Running it with '2017-02-15' would return only entity id 25. Running it with '2017-03-15' would return no rows. Inspections Cohort Config The way this looks in an Experiment configuration YAML is as follows: cohort_config: query: | select distinct(entity_id) from permits where tsrange(start_time, end_time, '[]') @> {as_of_date} name: 'permits_in_last_year' The name key is optional. Part of its purpose is to help you organize different cohorts in your configuration, but it is also included in each matrix's metadata file to help you keep them straight afterwards. Example: Early Intervention # An example of an early intervention system is identifying people at risk of recidivism so they can receive extra support to encourage positive outcomes. This example defines the cohort as everybody who has been released from jail within the last three years. It does this by querying an events table for events of type 'release'. Early Intervention Cohort Source Table entity_id event_type knowledge_date 25 booking 2016-02-01 44 booking 2016-02-01 25 release 2016-03-01 Early Intervention Cohort Config cohort_config: query: | SELECT distinct(entity_id) FROM events WHERE event_type = 'release' AND knowledge_date <@ daterange(('{as_of_date}'::date - '3 years'::interval)::date, '{as_of_date}'::date) name: 'booking_last_3_years' Example: Visits # Another problem type we may want to model is visit/appointment level modeling. An example would be a health clinic that wants to figure out which patients on a given day who are most at risk for developing diabetes within some time period but don't currently have it. Visits Cohort Source Tables Here we actually define two tables: an appointments table that contains the appointment schedule, and a diabetes diagnoses table that contains positive diabetes diagnoses. appointments entity_id appointment_date 25 2016-02-01 44 2016-02-01 25 2016-03-01 diabetes_diagnoses entity_id diagnosis_date 44 2015-02-01 86 2012-06-01 Visits Cohort Config The cohort config here queries the visits table for the next day, and excludes those who have a diabetes diagnosis at some point in the past. There's a twist: a day is subtracted from the as-of-date. Why? We may be collecting useful data during the appointment about whether or not they will develop diabetes, and we may want to use this data as features. Because the as-of-date refers to the timestamp at the beginning of the day ( see note 1 ), if the as-of-date and appointment date match up exactly we won't be able to use those features. So, appointments show up in the next day's as-of-date. Whether or not this is correct depends on the feasability of generating a prediction during the visit to use this data, which depends on the deployment plans for the system. If data entry and prediction can only happen nightly, you can't expect to use data from the visit in features and would change the as-of-date to match the appointment_date. cohort_config: query: | select distinct(entity_id) from appointments where appointment_date = ('{as_of_date}'::date - interval '1 days')::date and not exists( select entity_id from diabetes_diagnoses where entity_id = appointments.entity_id and as_of_date < '{as_of_date}' group by entity_id) group by entity_id name: 'visit_day_no_previous_diabetes' Testing Cohort Configuration # If you want to test out a cohort query without running an entire experiment, there are a few ways, and the easiest way depends on how much of the rest of the experiment you have configured. Option 1: You have not started writing an experiment config file yet . If you just want to test your query with a hardcoded list of dates as Triage does it (including as-of-date interpolation), you can instantiate the EntityDateTableGenerator with the query and run it for those dates. This skips any temporal config, so you don't have to worry about temporal config: from triage.component.architect.entity_date_table_generators import EntityDateTableGenerator from triage import create_engine from datetime import datetime EntityDateTableGenerator ( query = \"select entity_id from permits where tsrange(start_time, end_time, '[]') @> {as_of_date} \" , db_engine = create_engine ( ... ), entity_date_table_name = \"my_test_cohort_table\" ) . generate_entity_date_table ([ datetime ( 2016 , 1 , 1 ), datetime ( 2016 , 2 , 1 ), datetime ( 2016 , 3 , 1 )]) Running this will generate a table with the name you gave it ( my_test_cohort_table ), populated with the cohort for that list of dates. You can inspect this table in your SQL browser of choice. Option 2: You have an experiment config file that includes temporal config, and want to look at the cohort in isolation in the database . If you want to actually create the cohort for each date that results from your temporal config, you can go as far as instantiating an Experiment and telling it to generate the cohort. from triage.experiments import SingleThreadedExperiment from triage import create_engine import yaml with open ( '<your_experiment_config.yaml>' ) as fd : experiment_config = yaml . load ( fd ) experiment = SingleThreadedExperiment ( experiment_config = experiment_config , db_engine = create_engine ( ... ), project_path = './' ) experiment . generate_cohort () print ( experiment . cohort_table_name ) This will generate the entire cohort needed for your experiment. The table name is autogenerated by the Experiment, and you can retrieve it using the cohort_table_name attribute of the Experiment. Here, as in option 1, you can look at the data in your SQL browser of choice. These options should be enough to test your cohort in isolation. How the cohort shows up in matrices is also dependent on its interaction with the labels, and later we'll show how to test that. Label Definition and Examples # The labels table works similarly to the cohort table: you give it a query with a placeholder for an as-of-date. However, the label query has one more dependency: a label timespan For instance, if you are inspecting buildings for violations, a label timespan of 6 months translates into a label of 'will this building have a violation in the next 6 months?'. These label timespans are generated by your temporal configuration as well and you may have multiple in a single experiment, so what you send Triage in your label query is also a placeholder. Note: The label query is expected by Triage to return only one row per entity id for a given as-of-date/label timespan combination. Missing Labels # Since the cohort has its own definition query, separate from the label query, we have to consider the possibility that not every entity in the cohort is present in the label query, and how to deal with these missing labels. The label value in the train matrix in these cases is controlled by a flag in the label config: include_missing_labels_in_train_as . If you omit the flag, they show up as missing. This is common for inspections problems, wherein you really don't know a suitable label. The facility wasn't inspected, so you really don't know what the label is. This makes evaluation a bit more complicated, as some of the facilities with high risk scores may have no labels. But this is a common tradeoff in inspections problems. If you set it to True, that means that all of the rows have positive label. What does this mean? It depends on what exactly your label query is, but a common use would be to model early warning problems of dropouts, in which the absence of an event (e.g. a school enrollment event) is the positive label. If you set it to False, that means that all of these rows have a negative label. A common use for this would be in early warning problems of adverse events, in which the presence of an event (e.g. excessive use of force by a police officer) is the positive label. Example: Inspections # Inspections Label Source Table To translate this into our restaurant example above, consider a source table named 'inspections' that contains information about inspections. A simplified version of this table may look like: entity_id date result 25 2016-01-05 pass 44 2016-01-04 fail 25 2016-02-04 fail The entity id is the same as the cohort above: it identifies the restaurant. The date is just the date that the inspection happened, and the result is a string 'pass'/'fail' stating whether or not the restaurant passed the inspection. Inspections Label Config In constructing the label query, we have to consider the note above that we want to return only one row for a given entity id. The easiest way to do this, given that this query is run per as-of-date, is to group by the entity id and aggregate all the matched events somehow. In this case, a sensible definition is that we want any failed inspections to trigger a positive label. So if there is one pass and one fail that falls under the label timespan , the label should be True. bool_or is a handy Postgres aggregation function that does this. A query to find any failed inspections would be written in an experiment YAML config as follows: label_config : query : | select entity_id, bool_or(result = 'fail')::integer as outcome from inspections where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id name : 'failed_inspection' Example: Early Intervention # Early Intervention Label Source Table We reuse the generic events table used in the early intervention cohort section. Early Intervention Label Config We would like to assign a True label to everybody who is booked into jail within the label timespan. Note the include_missing_labels_in_train_as value: False . Anybody who does not show up in this query can be assumed to not have been booked into jail, so they can be assigned a False label. label_config: query: | SELECT entity_id, bool_or(CASE WHEN event_type = 'booking' THEN TRUE END)::integer AS outcome FROM events WHERE knowledge_date <@ daterange('{as_of_date}'::date, ('{as_of_date}'::date + interval '{label_timespan}')::date) GROUP BY entity_id include_missing_labels_in_train_as: False name: 'booking' Example: Visits # Visits Label Source Table We reuse the diabetes_diagnoses table from the cohort section. Visits Label Config We would like to identify people who are diagnosed with diabetes within a certain label_timespan after the given as-of-date . Note that include_missing_labels_in_train_as is False here as well. Any diagnoses would show up here, so the lack of any results from this query would remove all ambiguity. label_config: query: | select entity_id, 1 as outcome from diabetes_diagnoses where as_of_date <@ daterange('{as_of_date}' :: date, ('{as_of_date}' :: date + interval '{label_timespan}') :: date) group by entity_id include_missing_labels_in_train_as: False name: 'diabetes' Note: If you broadened the scope of this diabetes problem to concern not just diabetes diagnoses but having diabetes in general, and you had access to both positive and negative diabetes tests, you might avoid setting include_missing_labels_in_train_as , similar to the inspections problem, to more completely take into account the possibility that a person may or may not have diabetes. Testing Label Configuration # If you want to test out a label query without running a whole experiment, you can test it out similarly to the cohort section above. Option 1: You have not started writing an experiment config file yet . If you just want to test your label query with a hardcoded list of dates as Triage does it (including as-of-date interpolation), you can instantiate the LabelGenerator with the query and run it for those dates. This skips any temporal config, so you don't have to worry about temporal config: from triage.component.architect.label_generators import LabelGenerator from triage import create_engine from datetime import datetime LabelGenerator ( query = \"select entity_id, bool_or(result='fail')::integer as outcome from inspections where ' {as_of_date} '::timestamp <= date and date < ' {as_of_date} '::timestamp + interval ' {label_timespan} ' group by entity_id\" db_engine = create_engine ( ... ), ) . generate_all_labels ( labels_table = 'test_labels' , as_of_dates = [ datetime ( 2016 , 1 , 1 ), datetime ( 2016 , 2 , 1 ), datetime ( 2016 , 3 , 1 )], label_timespans = [ '3 month' ], ) Running this will generate a table with the name you gave it ( test_labels ), populated with the labels for that list of dates. You can inspect this table in your SQL browser of choice. Option 2: You have an experiment config file that includes temporal config, and want to look at the labels in isolation in the database . If you want to actually create the labels for each date that results from your temporal config, you can go as far as instantiating an Experiment and telling it to generate the labels. from triage.experiments import SingleThreadedExperiment from triage import create_engine import yaml with open ( '<your_experiment_config.yaml>' ) as fd : experiment_config = yaml . load ( fd ) experiment = SingleThreadedExperiment ( experiment_config = experiment_config , db_engine = create_engine ( ... ), project_path = './' ) experiment . generate_labels () print ( experiment . labels_table_name ) This will generate the labels for each as-of-date in your experiment. The table name is autogenerated by the Experiment, and you can retrieve it using the labels_table_name attribute of the Experiment. Here, as in option 1, you can look at the data in your SQL browser of choice. These options should be enough to test your labels in isolation. How the labels shows up in matrices is also dependent on its interaction with the cohort, and later we'll show how to test that. Combining Cohorts and Labels to make Matrices # Looking at the cohort and labels tables in isolation doesn't quite get you the whole picture. They are combined with features to make matrices, and some of the functionality (e.g. include_missing_labels_in_train_as ) isn't applied until the matrices are made for performance/database disk space purposes. How does this work? Let's look at some example cohort and label tables. Cohort # entity_id as_of_date 25 2016-01-01 44 2016-01-01 25 2016-02-01 44 2016-02-01 25 2016-03-01 44 2016-03-01 60 2016-03-01 Label # entity_id as_of_date label 25 2016-01-01 True 25 2016-02-01 False 44 2016-02-01 True 25 2016-03-01 False 44 2016-03-01 True 60 2016-03-01 True Above we observe three total cohorts, on 2016-01-01 , 2016-02-01 , and 2016-03-01 . The first two cohorts have two entities each and the last one has a new third entity. For the first cohort, only one of the entities has an explicitly defined label (meaning the label query didn't return anything for them on that date). For simplicity's sake, we are going to assume only one matrix is created that includes all of these cohorts. Depending on the experiment's temporal configuration, there may be one, many, or all dates in a matrix, but the details here are outside of the scope of this document. In general, the index of the matrix is created using a left join in SQL: The cohort table is the left table, and the labels table is the right table, and they are joined on entity id/as of date. So all of the rows that are in the cohort but not the labels table (in this case, just entity 44/date 2016-01-01) will initially have a null label. The final contents of the matrix, however, depend on the include_missing_labels_in_train_as setting. Inspections-Style (preserve missing labels as null) # If include_missing_labels_in_train_as is not set, Triage treats it as a truly missing label. The final matrix will look like: entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... null 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True Early Warning Style (missing means False) # If include_missing_labels_in_train_as is set to False, Triage treats the absence of a label row as a False label. The final matrix will look like: entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... False 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True Dropout Style (missing means True) # If include_missing_labels_in_train_as is set to True, Triage treats the absence of a label row as a True label. The final matrix will look like: entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... True 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True If you would like to test how your cohort and label combine to make matrices, you can tell Triage to generate matrices and then inspect the matrices. To do this, we assume that you have your cohort and label defined in an experiment config file, as well as temporal config. The last piece needed to make matrices is some kind of features. Of course, the features aren't our main focus here, so let's use a placeholder feature that should create very quickly. config_version: 'v6' temporal_config: feature_start_time: '2010-01-04' feature_end_time: '2018-03-01' label_start_time: '2015-02-01' label_end_time: '2018-03-01' model_update_frequency: '1y' training_label_timespans: ['1month'] training_as_of_date_frequencies: '1month' test_durations: '1month' test_label_timespans: ['1month'] test_as_of_date_frequencies: '1month' max_training_histories: '5y' cohort_config: query: | select distinct(entity_id) from permits where tsrange(start_time, end_time, '[]') @> {as_of_date} name: 'permits_in_last_year' label_config: query: | select entity_id, bool_or(result = 'fail')::integer as outcome from inspections where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id name: 'failed_inspection' feature_aggregations: - prefix: 'test' from_obj: 'permits' knowledge_date_column: 'date' aggregates_imputation: all: type: 'zero_noflag' aggregates: [{quantity: '1', metrics: ['sum']}] intervals: ['3month'] groups: ['entity_id'] The above feature aggregation should just create a feature with the value 1 for each entity, but what's important here is that it's a valid feature config that allows us to make complete matrices. To make matrices using all of this configuration, you can run: from triage.experiments import SingleThreadedExperiment from triage import create_engine import yaml with open ( '<your_experiment_config.yaml>' ) as fd : experiment_config = yaml . load ( fd ) experiment = SingleThreadedExperiment ( experiment_config = experiment_config , db_engine = create_engine ( ... ), project_path = './' ) experiment . generate_matrices () The matrix generation process will run all of the cohort/label/feature generation above, and then save matrices to your project_path's matrices directory. By default, these are CSVs and should have a few columns: 'entity_id', 'date', 'test_1_sum', and 'failed_inspection'. The 'entity_id' and 'date' columns represent the index of this matrix, and 'failed_inspection' is the label. Each of these CSV files has a YAML file starting with the same hash representing metadata about that matrix. If you want to look for just the train matrices to inspect the results of the include_missing_labels_in_train_as flag, try this command (assuming you can use bash): $ grep \"matrix_type: train\" *.yaml 3343ebf255af6dbb5204a60a4390c7e1.yaml:matrix_type: train 6ee3cd406f00f0f47999513ef5d49e3f.yaml:matrix_type: train 74e2a246e9f6360124b96bea3115e01f.yaml:matrix_type: train a29c9579aa67e5a75b2f814d906e5867.yaml:matrix_type: train a558fae39238d101a66f9d2602a409e6.yaml:matrix_type: train f5bb7bd8f251a2978944ba2b82866153.yaml:matrix_type: train You can then open up those files and ensure that the labels for each entity_id/date pair match what you expect. Wrapup # Cohorts and Labels require a lot of care to define correctly as they constitute a large part of the problem framing. Even if you leave all of your feature generation the same, you can completely change the problem you're modeling by changing the label and cohort. Testing your cohort and label config can give you confidence that you're framing the problem the way you expect.","title":"Cohort and Label Deep Dive"},{"location":"experiments/cohort-labels/#cohort-and-label-deep-dive","text":"This document is intended at providing a deep dive into the concepts of cohorts and labels as they apply to Triage. For context, reading the Triage section of the Dirty Duck tutorial may be helpful before reading this document.","title":"Cohort and Label Deep Dive"},{"location":"experiments/cohort-labels/#temporal-validation-refresher","text":"Triage uses temporal validation to select models because the real-world problems that Triage is built for tend to evolve or change over time. Picking a date range to train on and a date range afterwards to test on ensures that we don't leak data from the future into our models that wouldn't be available in a real-world deployment scenario. Because of this, we often talk in Triage about the as-of-date : all models trained by Triage are associated with an as-of-date , which means that all the data that goes into the model is only included if it was known about before that date . The matrix used to train the model may have multiple as-of-dates , and the most recent is referred to as the model's as-of-date so it's easy to see the cutoff date for data included in the model. For more on temporal validation, see the relevant section in Dirty Duck .","title":"Temporal Validation Refresher"},{"location":"experiments/cohort-labels/#what-are-cohorts-and-labels-in-triage","text":"This document assumes that the reader is familiar with the concept of a machine learning target variable and will focus on explaining what is unique to Triage. A cohort is the population used used for modeling on a given as-of-date . This is expressed as a list of entities . An entity is simply the object of prediction, such as a facility to inspect or a patient coming in for a visit. Early warning systems tend to include their entire population (or at least a large subset of it) in the cohort at any given date, while appointment-based problems may only include in a date's cohort the people who are scheduled for an appointment on that date. A label is the binary target variable for a member of the cohort at a given as-of-date and a given label timespan. For instance, in an inspection prioritization problem the question being asked may be 'what facilities are at high risk of having a failed inspection in the next 6 months?' For this problem, the label_timespan is 6 months. There may be multiple label timespans tested in the same experiment, in which case there could be multiple labels for an entity and date. In addition, multiple label definitions are often tested against each other, such as \"any inspection failures\" vs \"inspection failures with serious issues\". Both labels and cohorts are defined in Triage's experiment configuration using SQL queries, with the variables ( as_of_date , label_timespan ) given as placeholders. This allows the definitions to be given in a concise manner while allowing the temporal configuration defined elsewhere in the experiment to produce the actual list of dates and timespans that are calculated during the experiment.","title":"What are Cohorts and Labels in Triage?"},{"location":"experiments/cohort-labels/#cohort-definition-and-examples","text":"The cohort is configured with a query that returns a unique list of entity_id s given an as_of_date , and it runs the query for each as_of_date that is produced by your temporal config. You tell Triage where to place each as_of_date with a placeholder surrounded by brackets: {as_of_date} .","title":"Cohort Definition and Examples"},{"location":"experiments/cohort-labels/#note-1","text":"The as_of_date is parsed as a timestamp in the database, which Postgres defaults to midnight at the beginning of the date in question . It's important to consider how this is used for feature generation. Features are only included if they are known about before this timestamp . So features will be only included for an as_of_date if they are known about before that as_of_date . If you want to work around this (e.g for visit-level problems in which you want to intake data on the day of the visit and make predictions using that data the same day ), you can move your cohort up a day. The time splitting in Triage is designed for day granularity so approaches to train up to a specific hour and test at another hour of the same day are not supported.","title":"Note 1"},{"location":"experiments/cohort-labels/#note-2","text":"Triage expects all entity ids to be integers.","title":"Note 2"},{"location":"experiments/cohort-labels/#note-3","text":"Triage expects the cohort to be a unique list of entity ids. Throughout the cohort example queries you will see distinct(entity_id) used to ensure this.","title":"Note 3"},{"location":"experiments/cohort-labels/#example-inspections","text":"Let's say I am prioritizing the inspection of food service facilities such as restaurants, caterers or grocery stores. One simple definition of a cohort for facility inspection would be to include any facilities that have active permits in the last year in the cohort. Assume that these permits are contained in a table, named permits , with the facility's id, a start date, and an end date of the permit.","title":"Example: Inspections"},{"location":"experiments/cohort-labels/#example-early-intervention","text":"An example of an early intervention system is identifying people at risk of recidivism so they can receive extra support to encourage positive outcomes. This example defines the cohort as everybody who has been released from jail within the last three years. It does this by querying an events table for events of type 'release'.","title":"Example: Early Intervention"},{"location":"experiments/cohort-labels/#example-visits","text":"Another problem type we may want to model is visit/appointment level modeling. An example would be a health clinic that wants to figure out which patients on a given day who are most at risk for developing diabetes within some time period but don't currently have it.","title":"Example: Visits"},{"location":"experiments/cohort-labels/#testing-cohort-configuration","text":"If you want to test out a cohort query without running an entire experiment, there are a few ways, and the easiest way depends on how much of the rest of the experiment you have configured. Option 1: You have not started writing an experiment config file yet . If you just want to test your query with a hardcoded list of dates as Triage does it (including as-of-date interpolation), you can instantiate the EntityDateTableGenerator with the query and run it for those dates. This skips any temporal config, so you don't have to worry about temporal config: from triage.component.architect.entity_date_table_generators import EntityDateTableGenerator from triage import create_engine from datetime import datetime EntityDateTableGenerator ( query = \"select entity_id from permits where tsrange(start_time, end_time, '[]') @> {as_of_date} \" , db_engine = create_engine ( ... ), entity_date_table_name = \"my_test_cohort_table\" ) . generate_entity_date_table ([ datetime ( 2016 , 1 , 1 ), datetime ( 2016 , 2 , 1 ), datetime ( 2016 , 3 , 1 )]) Running this will generate a table with the name you gave it ( my_test_cohort_table ), populated with the cohort for that list of dates. You can inspect this table in your SQL browser of choice. Option 2: You have an experiment config file that includes temporal config, and want to look at the cohort in isolation in the database . If you want to actually create the cohort for each date that results from your temporal config, you can go as far as instantiating an Experiment and telling it to generate the cohort. from triage.experiments import SingleThreadedExperiment from triage import create_engine import yaml with open ( '<your_experiment_config.yaml>' ) as fd : experiment_config = yaml . load ( fd ) experiment = SingleThreadedExperiment ( experiment_config = experiment_config , db_engine = create_engine ( ... ), project_path = './' ) experiment . generate_cohort () print ( experiment . cohort_table_name ) This will generate the entire cohort needed for your experiment. The table name is autogenerated by the Experiment, and you can retrieve it using the cohort_table_name attribute of the Experiment. Here, as in option 1, you can look at the data in your SQL browser of choice. These options should be enough to test your cohort in isolation. How the cohort shows up in matrices is also dependent on its interaction with the labels, and later we'll show how to test that.","title":"Testing Cohort Configuration"},{"location":"experiments/cohort-labels/#label-definition-and-examples","text":"The labels table works similarly to the cohort table: you give it a query with a placeholder for an as-of-date. However, the label query has one more dependency: a label timespan For instance, if you are inspecting buildings for violations, a label timespan of 6 months translates into a label of 'will this building have a violation in the next 6 months?'. These label timespans are generated by your temporal configuration as well and you may have multiple in a single experiment, so what you send Triage in your label query is also a placeholder. Note: The label query is expected by Triage to return only one row per entity id for a given as-of-date/label timespan combination.","title":"Label Definition and Examples"},{"location":"experiments/cohort-labels/#missing-labels","text":"Since the cohort has its own definition query, separate from the label query, we have to consider the possibility that not every entity in the cohort is present in the label query, and how to deal with these missing labels. The label value in the train matrix in these cases is controlled by a flag in the label config: include_missing_labels_in_train_as . If you omit the flag, they show up as missing. This is common for inspections problems, wherein you really don't know a suitable label. The facility wasn't inspected, so you really don't know what the label is. This makes evaluation a bit more complicated, as some of the facilities with high risk scores may have no labels. But this is a common tradeoff in inspections problems. If you set it to True, that means that all of the rows have positive label. What does this mean? It depends on what exactly your label query is, but a common use would be to model early warning problems of dropouts, in which the absence of an event (e.g. a school enrollment event) is the positive label. If you set it to False, that means that all of these rows have a negative label. A common use for this would be in early warning problems of adverse events, in which the presence of an event (e.g. excessive use of force by a police officer) is the positive label.","title":"Missing Labels"},{"location":"experiments/cohort-labels/#example-inspections_1","text":"","title":"Example: Inspections"},{"location":"experiments/cohort-labels/#example-early-intervention_1","text":"","title":"Example: Early Intervention"},{"location":"experiments/cohort-labels/#example-visits_1","text":"","title":"Example: Visits"},{"location":"experiments/cohort-labels/#testing-label-configuration","text":"If you want to test out a label query without running a whole experiment, you can test it out similarly to the cohort section above. Option 1: You have not started writing an experiment config file yet . If you just want to test your label query with a hardcoded list of dates as Triage does it (including as-of-date interpolation), you can instantiate the LabelGenerator with the query and run it for those dates. This skips any temporal config, so you don't have to worry about temporal config: from triage.component.architect.label_generators import LabelGenerator from triage import create_engine from datetime import datetime LabelGenerator ( query = \"select entity_id, bool_or(result='fail')::integer as outcome from inspections where ' {as_of_date} '::timestamp <= date and date < ' {as_of_date} '::timestamp + interval ' {label_timespan} ' group by entity_id\" db_engine = create_engine ( ... ), ) . generate_all_labels ( labels_table = 'test_labels' , as_of_dates = [ datetime ( 2016 , 1 , 1 ), datetime ( 2016 , 2 , 1 ), datetime ( 2016 , 3 , 1 )], label_timespans = [ '3 month' ], ) Running this will generate a table with the name you gave it ( test_labels ), populated with the labels for that list of dates. You can inspect this table in your SQL browser of choice. Option 2: You have an experiment config file that includes temporal config, and want to look at the labels in isolation in the database . If you want to actually create the labels for each date that results from your temporal config, you can go as far as instantiating an Experiment and telling it to generate the labels. from triage.experiments import SingleThreadedExperiment from triage import create_engine import yaml with open ( '<your_experiment_config.yaml>' ) as fd : experiment_config = yaml . load ( fd ) experiment = SingleThreadedExperiment ( experiment_config = experiment_config , db_engine = create_engine ( ... ), project_path = './' ) experiment . generate_labels () print ( experiment . labels_table_name ) This will generate the labels for each as-of-date in your experiment. The table name is autogenerated by the Experiment, and you can retrieve it using the labels_table_name attribute of the Experiment. Here, as in option 1, you can look at the data in your SQL browser of choice. These options should be enough to test your labels in isolation. How the labels shows up in matrices is also dependent on its interaction with the cohort, and later we'll show how to test that.","title":"Testing Label Configuration"},{"location":"experiments/cohort-labels/#combining-cohorts-and-labels-to-make-matrices","text":"Looking at the cohort and labels tables in isolation doesn't quite get you the whole picture. They are combined with features to make matrices, and some of the functionality (e.g. include_missing_labels_in_train_as ) isn't applied until the matrices are made for performance/database disk space purposes. How does this work? Let's look at some example cohort and label tables.","title":"Combining Cohorts and Labels to make Matrices"},{"location":"experiments/cohort-labels/#cohort","text":"entity_id as_of_date 25 2016-01-01 44 2016-01-01 25 2016-02-01 44 2016-02-01 25 2016-03-01 44 2016-03-01 60 2016-03-01","title":"Cohort"},{"location":"experiments/cohort-labels/#label","text":"entity_id as_of_date label 25 2016-01-01 True 25 2016-02-01 False 44 2016-02-01 True 25 2016-03-01 False 44 2016-03-01 True 60 2016-03-01 True Above we observe three total cohorts, on 2016-01-01 , 2016-02-01 , and 2016-03-01 . The first two cohorts have two entities each and the last one has a new third entity. For the first cohort, only one of the entities has an explicitly defined label (meaning the label query didn't return anything for them on that date). For simplicity's sake, we are going to assume only one matrix is created that includes all of these cohorts. Depending on the experiment's temporal configuration, there may be one, many, or all dates in a matrix, but the details here are outside of the scope of this document. In general, the index of the matrix is created using a left join in SQL: The cohort table is the left table, and the labels table is the right table, and they are joined on entity id/as of date. So all of the rows that are in the cohort but not the labels table (in this case, just entity 44/date 2016-01-01) will initially have a null label. The final contents of the matrix, however, depend on the include_missing_labels_in_train_as setting.","title":"Label"},{"location":"experiments/cohort-labels/#inspections-style-preserve-missing-labels-as-null","text":"If include_missing_labels_in_train_as is not set, Triage treats it as a truly missing label. The final matrix will look like: entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... null 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True","title":"Inspections-Style (preserve missing labels as null)"},{"location":"experiments/cohort-labels/#early-warning-style-missing-means-false","text":"If include_missing_labels_in_train_as is set to False, Triage treats the absence of a label row as a False label. The final matrix will look like: entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... False 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True","title":"Early Warning Style (missing means False)"},{"location":"experiments/cohort-labels/#dropout-style-missing-means-true","text":"If include_missing_labels_in_train_as is set to True, Triage treats the absence of a label row as a True label. The final matrix will look like: entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... True 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True If you would like to test how your cohort and label combine to make matrices, you can tell Triage to generate matrices and then inspect the matrices. To do this, we assume that you have your cohort and label defined in an experiment config file, as well as temporal config. The last piece needed to make matrices is some kind of features. Of course, the features aren't our main focus here, so let's use a placeholder feature that should create very quickly. config_version: 'v6' temporal_config: feature_start_time: '2010-01-04' feature_end_time: '2018-03-01' label_start_time: '2015-02-01' label_end_time: '2018-03-01' model_update_frequency: '1y' training_label_timespans: ['1month'] training_as_of_date_frequencies: '1month' test_durations: '1month' test_label_timespans: ['1month'] test_as_of_date_frequencies: '1month' max_training_histories: '5y' cohort_config: query: | select distinct(entity_id) from permits where tsrange(start_time, end_time, '[]') @> {as_of_date} name: 'permits_in_last_year' label_config: query: | select entity_id, bool_or(result = 'fail')::integer as outcome from inspections where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id name: 'failed_inspection' feature_aggregations: - prefix: 'test' from_obj: 'permits' knowledge_date_column: 'date' aggregates_imputation: all: type: 'zero_noflag' aggregates: [{quantity: '1', metrics: ['sum']}] intervals: ['3month'] groups: ['entity_id'] The above feature aggregation should just create a feature with the value 1 for each entity, but what's important here is that it's a valid feature config that allows us to make complete matrices. To make matrices using all of this configuration, you can run: from triage.experiments import SingleThreadedExperiment from triage import create_engine import yaml with open ( '<your_experiment_config.yaml>' ) as fd : experiment_config = yaml . load ( fd ) experiment = SingleThreadedExperiment ( experiment_config = experiment_config , db_engine = create_engine ( ... ), project_path = './' ) experiment . generate_matrices () The matrix generation process will run all of the cohort/label/feature generation above, and then save matrices to your project_path's matrices directory. By default, these are CSVs and should have a few columns: 'entity_id', 'date', 'test_1_sum', and 'failed_inspection'. The 'entity_id' and 'date' columns represent the index of this matrix, and 'failed_inspection' is the label. Each of these CSV files has a YAML file starting with the same hash representing metadata about that matrix. If you want to look for just the train matrices to inspect the results of the include_missing_labels_in_train_as flag, try this command (assuming you can use bash): $ grep \"matrix_type: train\" *.yaml 3343ebf255af6dbb5204a60a4390c7e1.yaml:matrix_type: train 6ee3cd406f00f0f47999513ef5d49e3f.yaml:matrix_type: train 74e2a246e9f6360124b96bea3115e01f.yaml:matrix_type: train a29c9579aa67e5a75b2f814d906e5867.yaml:matrix_type: train a558fae39238d101a66f9d2602a409e6.yaml:matrix_type: train f5bb7bd8f251a2978944ba2b82866153.yaml:matrix_type: train You can then open up those files and ensure that the labels for each entity_id/date pair match what you expect.","title":"Dropout Style (missing means True)"},{"location":"experiments/cohort-labels/#wrapup","text":"Cohorts and Labels require a lot of care to define correctly as they constitute a large part of the problem framing. Even if you leave all of your feature generation the same, you can completely change the problem you're modeling by changing the label and cohort. Testing your cohort and label config can give you confidence that you're framing the problem the way you expect.","title":"Wrapup"},{"location":"experiments/defining/","text":"Defining an Experiment # This doc is coming soon. In the meantime, check out: Example Experiment Definition for an overview of the different sections of an experiment definition. Dirty Duck for a beginning-to-end walkthrough to Triage, include deep dives on creating experiment configuration.","title":"Defining an Experiment"},{"location":"experiments/defining/#defining-an-experiment","text":"This doc is coming soon. In the meantime, check out: Example Experiment Definition for an overview of the different sections of an experiment definition. Dirty Duck for a beginning-to-end walkthrough to Triage, include deep dives on creating experiment configuration.","title":"Defining an Experiment"},{"location":"experiments/feature-testing/","text":"Testing a Feature Aggregation # Developing features for Triage experiments can be a daunting task. There are a lot of things to configure, a small amount of configuration can result in a ton of SQL, and it can take a long time to validate your feature configuration in the context of an Experiment being run on real data. To speed up the process of iterating on features, you can run a list of feature aggregations, without imputation, on just one as-of-date. This functionality can be accessed through the triage command line tool or called directly from code (say, in a Jupyter notebook) using the FeatureGenerator component. Using Triage CLI # The command-line interface for testing features takes in two arguments: - An experiment config file. Refer to the example_experiment_config.yaml 's feature_aggregations section. It consists of a YAML list, with one or more feature_aggregation rows present. - An as-of-date. This should be in the format 2016-01-01 . Example: triage experiment featuretest example/config/experiment.yaml 2016-01-01 All given feature aggregations will be processed for the given date. You will see a bunch of queries pass by in your terminal, populating tables in the features_test schema which you can inspect afterwards. Using Python Code # If you'd like to call this from a notebook or from any other Python code, the arguments look similar but are a bit different. You have to supply your own sqlalchemy database engine to create a 'FeatureGenerator' object, and then call the create_features_before_imputation method with your feature config as a list of dictionaries, along with an as-of-date as a string. Make sure your logging level is set to INFO if you want to see all of the queries. from triage.component.architect.feature_generators import FeatureGenerator from triage.util.db import create_engine import logging import yaml logging.basicConfig(level=logging.INFO) # create a db_engine db_url = 'your db url here' db_engine = create_engine(db_url) feature_config = [{ 'prefix': 'aprefix', 'aggregates': [ { 'quantity': 'quantity_one', 'metrics': ['sum', 'count'], ], 'categoricals': [ { 'column': 'cat_one', 'choices': ['good', 'bad'], 'metrics': ['sum'] }, ], 'groups': ['entity_id', 'zip_code'], 'intervals': ['all'], 'knowledge_date_column': 'knowledge_date', 'from_obj': 'data' }] FeatureGenerator(db_engine, 'features_test').create_features_before_imputation( feature_aggregation_config=feature_config, feature_dates=['2016-01-01'] )","title":"Testing Feature Configuration"},{"location":"experiments/feature-testing/#testing-a-feature-aggregation","text":"Developing features for Triage experiments can be a daunting task. There are a lot of things to configure, a small amount of configuration can result in a ton of SQL, and it can take a long time to validate your feature configuration in the context of an Experiment being run on real data. To speed up the process of iterating on features, you can run a list of feature aggregations, without imputation, on just one as-of-date. This functionality can be accessed through the triage command line tool or called directly from code (say, in a Jupyter notebook) using the FeatureGenerator component.","title":"Testing a Feature Aggregation"},{"location":"experiments/feature-testing/#using-triage-cli","text":"The command-line interface for testing features takes in two arguments: - An experiment config file. Refer to the example_experiment_config.yaml 's feature_aggregations section. It consists of a YAML list, with one or more feature_aggregation rows present. - An as-of-date. This should be in the format 2016-01-01 . Example: triage experiment featuretest example/config/experiment.yaml 2016-01-01 All given feature aggregations will be processed for the given date. You will see a bunch of queries pass by in your terminal, populating tables in the features_test schema which you can inspect afterwards.","title":"Using Triage CLI"},{"location":"experiments/feature-testing/#using-python-code","text":"If you'd like to call this from a notebook or from any other Python code, the arguments look similar but are a bit different. You have to supply your own sqlalchemy database engine to create a 'FeatureGenerator' object, and then call the create_features_before_imputation method with your feature config as a list of dictionaries, along with an as-of-date as a string. Make sure your logging level is set to INFO if you want to see all of the queries. from triage.component.architect.feature_generators import FeatureGenerator from triage.util.db import create_engine import logging import yaml logging.basicConfig(level=logging.INFO) # create a db_engine db_url = 'your db url here' db_engine = create_engine(db_url) feature_config = [{ 'prefix': 'aprefix', 'aggregates': [ { 'quantity': 'quantity_one', 'metrics': ['sum', 'count'], ], 'categoricals': [ { 'column': 'cat_one', 'choices': ['good', 'bad'], 'metrics': ['sum'] }, ], 'groups': ['entity_id', 'zip_code'], 'intervals': ['all'], 'knowledge_date_column': 'knowledge_date', 'from_obj': 'data' }] FeatureGenerator(db_engine, 'features_test').create_features_before_imputation( feature_aggregation_config=feature_config, feature_dates=['2016-01-01'] )","title":"Using Python Code"},{"location":"experiments/features/","text":"Feature Generation Recipe Book # This document is a collection of 'collate' aggregate features that we have found useful to create in Triage that may not be apparent at first. For an introduction to feature generation in Triage, refer to Dirty Duck Feature Generation Age # You can calculate age from a date of birth column using the collate_date special variable. This variable is marked as a placeholder in the feature quantity input, but is replaced with each as-of-date when features are being calculated. Combined with the Postgres age function, this calculates a person's age at each as-of-date as a feature. For this example, let's assume you have a column called 'dob' that is a timestamp (or anything that can be cast to a date) in your source table. The feature_aggregation 's quantity would be: EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE)) If Triage is calculating this for the as-of-date '2016-01-01', it will internally expand the collate_date out to: EXTRACT(YEAR FROM AGE('2016-01-01'::DATE, dob::DATE)) In context, a feature aggregate that uses age may look more like: aggregates: - # age in years quantity: age: \"EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE))\" metrics: ['max'] Here, we call the feature 'age' and since everything in collate is defined as an aggregate, we pick 'max'; Any records for the same person and as-of-date should have the same 'dob', so there are many aggregates you can use that will arrive at the same answer (e.g. 'min', 'avg'). In these cases 'max' is the standard aggregate metric of choice in Triage.","title":"Feature Generation Recipe Book"},{"location":"experiments/features/#feature-generation-recipe-book","text":"This document is a collection of 'collate' aggregate features that we have found useful to create in Triage that may not be apparent at first. For an introduction to feature generation in Triage, refer to Dirty Duck Feature Generation","title":"Feature Generation Recipe Book"},{"location":"experiments/features/#age","text":"You can calculate age from a date of birth column using the collate_date special variable. This variable is marked as a placeholder in the feature quantity input, but is replaced with each as-of-date when features are being calculated. Combined with the Postgres age function, this calculates a person's age at each as-of-date as a feature. For this example, let's assume you have a column called 'dob' that is a timestamp (or anything that can be cast to a date) in your source table. The feature_aggregation 's quantity would be: EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE)) If Triage is calculating this for the as-of-date '2016-01-01', it will internally expand the collate_date out to: EXTRACT(YEAR FROM AGE('2016-01-01'::DATE, dob::DATE)) In context, a feature aggregate that uses age may look more like: aggregates: - # age in years quantity: age: \"EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE))\" metrics: ['max'] Here, we call the feature 'age' and since everything in collate is defined as an aggregate, we pick 'max'; Any records for the same person and as-of-date should have the same 'dob', so there are many aggregates you can use that will arrive at the same answer (e.g. 'min', 'avg'). In these cases 'max' is the standard aggregate metric of choice in Triage.","title":"Age"},{"location":"experiments/prediction-ranking/","text":"Prediction Ranking # The predictions tables in the train_results and test_results schemas contain several different flavors of rankings, covering absolute vs percentile ranking and whether or not ties exist. Ranking columns # Column name Behavior rank_abs_with_ties Absolute ranking, with ties. Ranks will skip after a set of ties, so if two entities are tied at rank 3, the next entity after them will have rank 5. rank_pct_with_ties Percentile ranking, with ties. Percentiles will skip after a set of ties, so if two entities out of ten are tied at 0.1 (tenth percentile), the next entity after them will have 0.3 (thirtieth percentile). At most five decimal places. rank_abs_no_ties Absolute ranking, with no ties. Ties are broken according to a configured choice: 'best', 'worst', or 'random', which is recorded in the prediction_metadata table rank_pct_no_ties Percentile ranking, with no ties. Ties are broken according to a configured choice: 'best', 'worst', or 'random', which is recorded in the prediction_metadata table. At most five decimal places. Viewing prediction metadata # The prediction_metadata table contains information about how ties were broken. There is one row per model/matrix combination. For each model and matrix, it records: tiebreaker_ordering - The tiebreaker ordering rule (e.g. 'random', 'best', 'worst') used for the corresponding predictions. random_seed - The random seed, if 'random' was the ordering used. Otherwise None predictions_saved - Whether or not predictions were saved. If it's false, you won't expect to find any predictions, but the row is inserted as a record that the prediction was performed. There is one prediction_metadata table in each of the train_results , test_results schemas (in other words, wherever there is a companion predictions table). Backfilling ranks for old predictions # Prediction ranking is new to Triage, so you may have old Triage runs that have no prediction ranks that you would like to backfill. To do this, you can use the Predictor class' update_db_with_ranks method to backfill ranks. This example fills rankings for test predictions, but you can replace TestMatrixType with TrainMatrixType to rank train predictions (provided such predictions already exist) from triage.component.catwalk import Predictor from triage.component.catwalk.storage import TestMatrixType predictor = Predictor ( db_engine =... , rank_order = 'worst' , model_storage_engine = None , ) predictor . update_db_with_ranks ( model_id =... , # model id of some model with test predictions for the companion matrix matrix_uuid =... , # matrix uuid of some matrix with test predictions for the companion model matrix_type = TestMatrixType , ) Subsequent runs # If you run Triage Experiments with replace=False , and you change nothing except for the rank_tiebreaker in experiment config, ranking will be redone and the row in prediction_metadata updated. You don't have to run a full experiment if that's all you want to do; you could follow the directions for backfilling ranks above, which will redo the ranking for an individual model/matrix pair. However, changing the rank_tiebreaker in experiment config and re-running the experiment is a handy way of redoing all of them if that's what is useful.","title":"Prediction Ranking"},{"location":"experiments/prediction-ranking/#prediction-ranking","text":"The predictions tables in the train_results and test_results schemas contain several different flavors of rankings, covering absolute vs percentile ranking and whether or not ties exist.","title":"Prediction Ranking"},{"location":"experiments/prediction-ranking/#ranking-columns","text":"Column name Behavior rank_abs_with_ties Absolute ranking, with ties. Ranks will skip after a set of ties, so if two entities are tied at rank 3, the next entity after them will have rank 5. rank_pct_with_ties Percentile ranking, with ties. Percentiles will skip after a set of ties, so if two entities out of ten are tied at 0.1 (tenth percentile), the next entity after them will have 0.3 (thirtieth percentile). At most five decimal places. rank_abs_no_ties Absolute ranking, with no ties. Ties are broken according to a configured choice: 'best', 'worst', or 'random', which is recorded in the prediction_metadata table rank_pct_no_ties Percentile ranking, with no ties. Ties are broken according to a configured choice: 'best', 'worst', or 'random', which is recorded in the prediction_metadata table. At most five decimal places.","title":"Ranking columns"},{"location":"experiments/prediction-ranking/#viewing-prediction-metadata","text":"The prediction_metadata table contains information about how ties were broken. There is one row per model/matrix combination. For each model and matrix, it records: tiebreaker_ordering - The tiebreaker ordering rule (e.g. 'random', 'best', 'worst') used for the corresponding predictions. random_seed - The random seed, if 'random' was the ordering used. Otherwise None predictions_saved - Whether or not predictions were saved. If it's false, you won't expect to find any predictions, but the row is inserted as a record that the prediction was performed. There is one prediction_metadata table in each of the train_results , test_results schemas (in other words, wherever there is a companion predictions table).","title":"Viewing prediction metadata"},{"location":"experiments/prediction-ranking/#backfilling-ranks-for-old-predictions","text":"Prediction ranking is new to Triage, so you may have old Triage runs that have no prediction ranks that you would like to backfill. To do this, you can use the Predictor class' update_db_with_ranks method to backfill ranks. This example fills rankings for test predictions, but you can replace TestMatrixType with TrainMatrixType to rank train predictions (provided such predictions already exist) from triage.component.catwalk import Predictor from triage.component.catwalk.storage import TestMatrixType predictor = Predictor ( db_engine =... , rank_order = 'worst' , model_storage_engine = None , ) predictor . update_db_with_ranks ( model_id =... , # model id of some model with test predictions for the companion matrix matrix_uuid =... , # matrix uuid of some matrix with test predictions for the companion model matrix_type = TestMatrixType , )","title":"Backfilling ranks for old predictions"},{"location":"experiments/prediction-ranking/#subsequent-runs","text":"If you run Triage Experiments with replace=False , and you change nothing except for the rank_tiebreaker in experiment config, ranking will be redone and the row in prediction_metadata updated. You don't have to run a full experiment if that's all you want to do; you could follow the directions for backfilling ranks above, which will redo the ranking for an individual model/matrix pair. However, changing the rank_tiebreaker in experiment config and re-running the experiment is a handy way of redoing all of them if that's what is useful.","title":"Subsequent runs"},{"location":"experiments/running/","text":"Running an Experiment # Prerequisites # To use a Triage experiment, you first need: Python 3.5 A PostgreSQL database with your source data (events, geographical data, etc) loaded. Ample space on an available disk (or S3) to store the needed matrices and models for your experiment An experiment definition (see Defining an Experiment ) You may run a Triage experiment two ways: through the Triage command line tool, or through instantiating an Experiment object in your own Python code and running it. The rest of this document will introduce experiment runs of increasing complexity, through both the CLI and Python interfaces. Simple Example # To run an experiment, you need to tell triage at a minimum where to find the experiment file (in YAML format), and how to connect to the database, In this simple example, we're assuming that the experiment will be run with only one process, and that the matrices and models should be stored on the local filesystem. CLI # The Triage CLI defaults database connection information to a file stored in 'database.yaml', so with this you can omit any mention of the database. In addition, if you leave out the project path. In addition, the 'project path' (where matrices and models are stored) defaults to the current working directory. So this is the simplest possible invocation: triage experiment example/config/experiment.yaml If you have the database information stored somewhere else, you may pass it to the top-level 'triage' command: triage -d mydbconfig.yaml experiment example/config/experiment.yaml Assuming you want the matrices and models stored somewhere else, pass it as the --project-path : triage -d mydbconfig.yaml experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' Python # When running an experiment in Python, the database information is passed in the form of a SQLAlchemy database engine, and the experiment information is passed as a dictionary rather as YAML specifically. from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment ( config = experiment_config , # a dictionary db_engine = create_engine ( ... ), # http://docs.sqlalchemy.org/en/latest/core/engines.html project_path = '/path/to/directory/to/save/data' ) experiment . run () Either way you run it, you are likely to see a bunch of log output. Once the feature/cohor/label/matrix building is done and the experiment has moved onto modeling, check out the model_metadata.models and test_results.evaluations tables as data starts to come in. You'll see the simple models (Decision Trees, Scaled Logistic Regression, baselines) populate first, followed by your big models, followed by the rest. You can start to look at the simple model results first to get a handle on what basic classifiers can do for your feature space while you wait for the Random Forests to run. Multicore example # Triage also offers the ability to locally parallelize both CPU-heavy and database-heavy tasks. Triage uses the pebble library to perform both of these, but they are separately configurable as the database tasks will more likely be bounded by the number of connections/cores available on the database server instead of the number of cores available on the experiment running machine. CLI # The Triage CLI allows parallelization to be specified through the --n-processes and --n-db-processes parameters. triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --n-db-processes 4 --n-processes 8 Python # In Python, you can use the MultiCoreExperiment instead of the SingleThreadedExperiment , and similarly pass the n_processes and n_db_processes parameters. We also recommend using triage.create_engine . It will create a serializable version of the engine that will be fully reconstructed in multiprocess contexts. If you pass a regular SQLAlchemy engine, in these contexts the engine will be reconstructed with the database URL only , which may cancel other settings you have used to configure your engine. from triage.experiments import MultiCoreExperiment from triage import create_engine experiment = MultiCoreExperiment ( config = experiment_config , # a dictionary db_engine = create_engine ( ... ), project_path = '/path/to/directory/to/save/data' , n_db_processes = 4 , n_processes = 8 , ) experiment . run () The pebble library offers an interface around Python3's concurrent.futures module that adds in a very helpful tool: watching for killed subprocesses . Model training (and sometimes, matrix building) can be a memory-hungry task, and Triage can not guarantee that the operating system you're running on won't kill the worker processes in a way that prevents them from reporting back to the parent Experiment process. With Pebble, this occurrence is caught like a regular Exception, which allows the Process pool to recover and include the information in the Experiment's log. Using S3 to store matrices and models # Triage can operate on different storage engines for matrices and models, and besides the standard filesystem engine comes with S3 support out of the box. To use this, just use the s3:// scheme for your project_path (this is similar for both Python and the CLI). CLI # triage experiment example/config/experiment.yaml --project-path 's3://bucket/directory/to/save/data' Python # from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment ( config = experiment_config , # a dictionary db_engine = create_engine ( ... ), project_path = 's3://bucket/directory/to/save/data' ) experiment . run () Validating an Experiment # Configuring an experiment is complex, and running an experiment can take a long time as data scales up. If there are any misconfigured values, it's going to help out a lot to figure out what they are before we run the Experiment. So when you have completed your experiment config and want to test it out, it's best to validate the Experiment first. If any problems are detectable in your Experiment, either in configuration or the database tables referenced by it, this method will throw an exception. For instance, if I refer to the cat_complaints table in a feature aggregation but it doesn't exist, I'll see something like this: *** ValueError: from_obj query does not run. from_obj: \"cat_complaints\" Full error: (psycopg2.ProgrammingError) relation \"cat_complaints\" does not exist LINE 1: explain select * from cat_complaints ^ [SQL: 'explain select * from cat_complaints'] CLI # The CLI, by default, validates before running. You can tweak this behavior, and make it not validate, or make it only validate. triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --no-validate triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --validate-only Python The python interface will also validate by default when running an experiment. If you would prefer to skip this step, you can pass skip_validation=True when constructing your experiment. You can also run this validation step directly. Experiments expose a validate method that can be run as needed. Experiment instantiation doesn't change from the run examples at all. experiment . validate () By default, the validate method will stop as soon as it encounters an error ('strict' mode). If you would like it to validate each section without stopping (i.e. if you have only written part of the experiment configuration), call validate(strict=False) and all of the errors will be changed to warnings. We'd like to add more validations for common misconfiguration problems over time. If you got an unexpected error that turned out to be related to a confusing configuration value, help us out by adding to the validation module and submitting a pull request! Restarting an Experiment # If an experiment fails for any reason, you can restart it. By default, all work will be recreated. This includes label queries, feature queries, matrix building, model training, etc. However, if you pass the replace=False keyword argument, the Experiment will reuse what work it can. Cohort Table: The Experiment refers to a cohort table namespaced by the cohort name and a hash of the cohort query, and in that way allows you to reuse cohorts between different experiments if their label names and queries are identical. When referring to this table, it will check on an as-of-date level whether or not there are any existing rows for that date, and skip the cohort query for that date if so. For this reason, it is not aware of specific entities or source events so if the source data has changed, ensure that replace is set to True. Labels Table: The Experiment refers to a labels table namespaced by the label name and a hash of the label query, and in that way allows you to reuse labels between different experiments if their label names and queries are identical. When referring to this table, it will check on a per- as_of_date / label timespan level whether or not there are any existing rows, and skip the label query if so. For this reason, it is not aware of specific entities or source events so if the label query has changed or the source data has changed, ensure that replace is set to True. Features Tables: The Experiment will check on a per-table basis whether or not it exists and contains rows for the entire cohort, and skip the feature generation if so. It does not look at the column list for the feature table or inspect the feature data itself. So, if you have modified any source data that affects a feature aggregation, or added any columns to that aggregation, you won't want to set replace to False. However, it is cohort-and-date aware so you can change around your cohort and temporal configuration safely. Matrix Building: Each matrix's metadata is hashed to create a unique id. If a file exists in storage with that hash, it will be reused. Model Training: Each model's metadata (which includes its train matrix's hash) is hashed to create a unique id. If a file exists in storage with that hash, it will be reused. CLI # triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --replace Python # from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment ( config = experiment_config , # a dictionary db_engine = create_engine ( ... ), project_path = 's3://bucket/directory/to/save/data' , replace = True ) experiment . run () Optimizing an Experiment # Skipping Prediction Syncing # By default, the Experiment will save predictions to the database. This can take a long time if your test matrices have a lot of rows, and isn't quite necessary if you just want to see the high-level performance of your grid. By switching save_predictions to False , you can skip the prediction saving. You'll still get your evaluation metrics, so you can look at performance. Don't worry, you can still get your predictions back later by rerunning the Experiment later at default settings, which will find your already-trained models, generate predictions, and save them. CLI: triage experiment myexperiment.yaml --no-save-predictions Python: SingleThreadedExperiment(..., save_predictions=False) Running parts of an Experiment # If you would like incrementally build, or just incrementally run parts of the Experiment look at their outputs, you can do so. Running a full experiment requires the experiment config to be filled out, but when you're getting started using Triage it can be easier to build the experiment piece by piece and see the results as they come in. Make sure logging is set to INFO level before running this to ensure you get all the log messages. Additionally, because the default behavior of triage is to run config file validation (which expects a complete experiment configuration) and fill in missing values in some sections with defaults, you will need to pass partial_run=True when constructing your experiment object for a partial experiment (this will also avoid cleaning up intermediate tables from the run, equivalent to cleanup=False ). Running parts of an experiment is only supported through the Python interface. Python # experiment.run() will run until it no longer has enough configuration to proceed. You will see information in the logs telling you about the steps it was able to perform. You can additionally view the intermediate tables that are built in the database, which are modified with the experiment hash that the experiment calculates, but this will be printed out in the log messages. labels_*<experiment_hash>* for the labels generated per entity and as of date. tmp_sparse_states_*<experiment_hash>* for the membership in each cohort per entity and as_of_date To reproduce the entire Experiment piece by piece, you can run the following. Each one of these methods requires some portion of experiment config to be passed: experiment.split_definitions will parse temporal config and create time splits. It only requires temporal_config . experiment.generate_cohort() will use the cohort config and as of dates from the temporal config to generate an internal table keeping track of what entities are in the cohort on different dates. It requires temporal_config and cohort_config . experiment.generate_labels() will use the label config and as of dates from the temporal config to generate an internal labels table. It requires temporal_config and label_config . experiment.generate_preimputation_features() will use the feature aggregation config and as of dates from the temporal config to generate internal features tables. It requires temporal_config and feature_aggregations . experiment.generate_imputed_features() will use the imputation sections of the feature aggregation config and the results from the preimputed features to create internal imputed features tables. It requires temporal_config and feature_aggregations . experiment.build_matrices() will use all of the internal tables generated before this point, along with feature grouping config, to generate all needed matrices. It requires temporal_config , cohort_config , label_config , and feature_aggregations , though it will also use feature_group_definitions , feature_group_strategies , and user_metadata if present. experiment.train_and_test_models() will use the generated matrices, grid config and evaluation metric config to train and test all needed models. It requires all configuration keys. Evaluating results of an Experiment # After the experiment run, a variety of schemas and tables will be created and populated in the configured database: model_metadata.experiments - The experiment configuration, a hash, and some run-invariant details about the configuration model_metadata.experiment_runs - Information about the experiment run that may change from run to run, pertaining to the run environment, status, and results model_metadata.matrices - Each train or test matrix that is built has a row here, with some basic metadata model_metadata.experiment_matrices - A many-to-many table between experiments and matrices. This will have a row if the experiment used the matrix, regardless of whether or not it had to build it model_metadata.models - A model describes a trained classifier; you'll have one row for each trained file that gets saved. model_metadata.experiment_models - A many-to-many table between experiments and models. This will have a row if the experiment used the model, regardless of whether or not it had to build it model_metadata.model_groups - A model groups refers to all models that share parameters like classifier type, hyperparameters, etc, but have different training windows . Look at these to see how classifiers perform over different training windows. model_metadata.matrices - Each matrix that was used for training and testing has metadata written about it such as the matrix hash, length, and time configuration. model_metadata.subsets - Each evaluation subset that was used for model scoring has its configuation and a hash written here train_results.feature_importances - The sklearn feature importances results for each trained model train_results.predictions - Prediction probabilities for train matrix entities generated against trained models train_results.prediction_metadata - Metadata about the prediction stage for a model and train matrix, such as tiebreaking configuration train_results.evaluations - Metric scores of trained models on the training data. test_results.predictions - Prediction probabilities for test matrix entities generated against trained models test_results.prediction_metadata - Metadata about the prediction stage for a model and test matrix, such as tiebreaking configuration test_results.evaluations - Metric scores of trained models over given testing windows and subsets test_results.individual_importances - Individual feature importance scores for test matrix entities. Here's an example query, which returns the top 10 model groups by precision at the top 100 entities: select model_groups.model_group_id, model_groups.model_type, model_groups.hyperparameters, max(test_evaluations.value) as max_precision from model_metadata.model_groups join model_metadata.models using (model_group_id) join test_results.evaluations using (model_id) where metric = 'precision@' and parameter = '100_abs' group by 1,2,3 order by 4 desc limit 10 Inspecting an Experiment before running # Before you run an experiment, you can inspect properties of the Experiment object to ensure that it is configured in the way you want. Some examples: experiment.all_as_of_times for debugging temporal config. This will show all dates that features and labels will be calculated at. experiment.feature_dicts will output a list of feature dictionaries, representing the feature tables and columns configured in this experiment experiment.matrix_build_tasks will output a list representing each matrix that will be built. Optimizing Experiment Performance # Profiling an Experiment # Experiment running slowly? Try the profile keyword argument, or --profile in the command line. This will output a cProfile file to the project path's profiling_stats directory. This is a binary format but can be read with a variety of visualization programs. snakeviz - A browser based graphical viewer. tuna - Another browser based graphical viewer gprof2dot - A command-line tool to convert files to graphviz format pyprof2calltree - A command-line tool to convert files to Valgrind log format, for viewing in established viewers like KCacheGrind Looking at the profile through a visualization program, you can see which portions of the experiment are taking up the most time. Based on this, you may be able to prioritize changes. For instance, if cohort/label/feature table generation are taking up the bulk of the time, you may add indexes to source tables, or increase the number of database processes. On the other hand, if model training is the culprit, you may temporarily try a smaller grid to get results more quickly. materialize_subquery_fromobjs # By default, experiments will inspect the from_obj of every feature aggregation to see if it looks like a subquery, create a table out of it if so, index it on the knowledge_date_column and entity_id , and use that for running feature queries. This can make feature generation go a lot faster if the from_obj takes a decent amount of time to run and/or there are a lot of as-of-dates in the experiment. It won't do this for from_objs that are just tables, or simple joins (e.g. entities join events using (entity_id) ) as the existing indexes you have on those tables should work just fine. You can turn this off if you'd like, which you may want to do if the from_obj subqueries return a lot of data and you want to save as much disk space as possible. The option is turned off by passing materialize_subquery_fromobjs=False to the Experiment. Build Features Independently of Cohort # By default the feature queries generated by your feature configuration on any given date are joined with the cohort table on that date, which means that no features for entities not in the cohort are saved. This is to save time and database disk space when your cohort on any given date is not very large and allow you to iterate on feature building quickly by default. However, this means that anytime you change your cohort, you have to rebuild all of your features. Depending on your experiment setup (for instance, multiple large cohorts that you experiment with), this may be time-consuming. Change this by passing features_ignore_cohort=True to the Experiment constructor, or --save-all-features to the command-line. Experiment Classes # SingleThreadedExperiment : An experiment that performs all tasks serially in a single thread. Good for simple use on small datasets, or for understanding the general flow of data through a pipeline. MultiCoreExperiment : An experiment that makes use of the pebble library to parallelize various time-consuming steps. Takes an n_processes keyword argument to control how many workers to use. RQExperiment : An experiment that makes use of the python-rq library to enqueue individual tasks onto the default queue, and wait for the jobs to be finished before moving on. python-rq requires Redis and any number of worker processes running the Triage codebase. Triage does not set up any of this needed infrastructure for you. Available through the RQ extra ( pip install triage[rq] )","title":"Running an Experiment"},{"location":"experiments/running/#running-an-experiment","text":"","title":"Running an Experiment"},{"location":"experiments/running/#prerequisites","text":"To use a Triage experiment, you first need: Python 3.5 A PostgreSQL database with your source data (events, geographical data, etc) loaded. Ample space on an available disk (or S3) to store the needed matrices and models for your experiment An experiment definition (see Defining an Experiment ) You may run a Triage experiment two ways: through the Triage command line tool, or through instantiating an Experiment object in your own Python code and running it. The rest of this document will introduce experiment runs of increasing complexity, through both the CLI and Python interfaces.","title":"Prerequisites"},{"location":"experiments/running/#simple-example","text":"To run an experiment, you need to tell triage at a minimum where to find the experiment file (in YAML format), and how to connect to the database, In this simple example, we're assuming that the experiment will be run with only one process, and that the matrices and models should be stored on the local filesystem.","title":"Simple Example"},{"location":"experiments/running/#cli","text":"The Triage CLI defaults database connection information to a file stored in 'database.yaml', so with this you can omit any mention of the database. In addition, if you leave out the project path. In addition, the 'project path' (where matrices and models are stored) defaults to the current working directory. So this is the simplest possible invocation: triage experiment example/config/experiment.yaml If you have the database information stored somewhere else, you may pass it to the top-level 'triage' command: triage -d mydbconfig.yaml experiment example/config/experiment.yaml Assuming you want the matrices and models stored somewhere else, pass it as the --project-path : triage -d mydbconfig.yaml experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data'","title":"CLI"},{"location":"experiments/running/#python","text":"When running an experiment in Python, the database information is passed in the form of a SQLAlchemy database engine, and the experiment information is passed as a dictionary rather as YAML specifically. from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment ( config = experiment_config , # a dictionary db_engine = create_engine ( ... ), # http://docs.sqlalchemy.org/en/latest/core/engines.html project_path = '/path/to/directory/to/save/data' ) experiment . run () Either way you run it, you are likely to see a bunch of log output. Once the feature/cohor/label/matrix building is done and the experiment has moved onto modeling, check out the model_metadata.models and test_results.evaluations tables as data starts to come in. You'll see the simple models (Decision Trees, Scaled Logistic Regression, baselines) populate first, followed by your big models, followed by the rest. You can start to look at the simple model results first to get a handle on what basic classifiers can do for your feature space while you wait for the Random Forests to run.","title":"Python"},{"location":"experiments/running/#multicore-example","text":"Triage also offers the ability to locally parallelize both CPU-heavy and database-heavy tasks. Triage uses the pebble library to perform both of these, but they are separately configurable as the database tasks will more likely be bounded by the number of connections/cores available on the database server instead of the number of cores available on the experiment running machine.","title":"Multicore example"},{"location":"experiments/running/#cli_1","text":"The Triage CLI allows parallelization to be specified through the --n-processes and --n-db-processes parameters. triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --n-db-processes 4 --n-processes 8","title":"CLI"},{"location":"experiments/running/#python_1","text":"In Python, you can use the MultiCoreExperiment instead of the SingleThreadedExperiment , and similarly pass the n_processes and n_db_processes parameters. We also recommend using triage.create_engine . It will create a serializable version of the engine that will be fully reconstructed in multiprocess contexts. If you pass a regular SQLAlchemy engine, in these contexts the engine will be reconstructed with the database URL only , which may cancel other settings you have used to configure your engine. from triage.experiments import MultiCoreExperiment from triage import create_engine experiment = MultiCoreExperiment ( config = experiment_config , # a dictionary db_engine = create_engine ( ... ), project_path = '/path/to/directory/to/save/data' , n_db_processes = 4 , n_processes = 8 , ) experiment . run () The pebble library offers an interface around Python3's concurrent.futures module that adds in a very helpful tool: watching for killed subprocesses . Model training (and sometimes, matrix building) can be a memory-hungry task, and Triage can not guarantee that the operating system you're running on won't kill the worker processes in a way that prevents them from reporting back to the parent Experiment process. With Pebble, this occurrence is caught like a regular Exception, which allows the Process pool to recover and include the information in the Experiment's log.","title":"Python"},{"location":"experiments/running/#using-s3-to-store-matrices-and-models","text":"Triage can operate on different storage engines for matrices and models, and besides the standard filesystem engine comes with S3 support out of the box. To use this, just use the s3:// scheme for your project_path (this is similar for both Python and the CLI).","title":"Using S3 to store matrices and models"},{"location":"experiments/running/#cli_2","text":"triage experiment example/config/experiment.yaml --project-path 's3://bucket/directory/to/save/data'","title":"CLI"},{"location":"experiments/running/#python_2","text":"from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment ( config = experiment_config , # a dictionary db_engine = create_engine ( ... ), project_path = 's3://bucket/directory/to/save/data' ) experiment . run ()","title":"Python"},{"location":"experiments/running/#validating-an-experiment","text":"Configuring an experiment is complex, and running an experiment can take a long time as data scales up. If there are any misconfigured values, it's going to help out a lot to figure out what they are before we run the Experiment. So when you have completed your experiment config and want to test it out, it's best to validate the Experiment first. If any problems are detectable in your Experiment, either in configuration or the database tables referenced by it, this method will throw an exception. For instance, if I refer to the cat_complaints table in a feature aggregation but it doesn't exist, I'll see something like this: *** ValueError: from_obj query does not run. from_obj: \"cat_complaints\" Full error: (psycopg2.ProgrammingError) relation \"cat_complaints\" does not exist LINE 1: explain select * from cat_complaints ^ [SQL: 'explain select * from cat_complaints']","title":"Validating an Experiment"},{"location":"experiments/running/#cli_3","text":"The CLI, by default, validates before running. You can tweak this behavior, and make it not validate, or make it only validate. triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --no-validate triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --validate-only","title":"CLI"},{"location":"experiments/running/#restarting-an-experiment","text":"If an experiment fails for any reason, you can restart it. By default, all work will be recreated. This includes label queries, feature queries, matrix building, model training, etc. However, if you pass the replace=False keyword argument, the Experiment will reuse what work it can. Cohort Table: The Experiment refers to a cohort table namespaced by the cohort name and a hash of the cohort query, and in that way allows you to reuse cohorts between different experiments if their label names and queries are identical. When referring to this table, it will check on an as-of-date level whether or not there are any existing rows for that date, and skip the cohort query for that date if so. For this reason, it is not aware of specific entities or source events so if the source data has changed, ensure that replace is set to True. Labels Table: The Experiment refers to a labels table namespaced by the label name and a hash of the label query, and in that way allows you to reuse labels between different experiments if their label names and queries are identical. When referring to this table, it will check on a per- as_of_date / label timespan level whether or not there are any existing rows, and skip the label query if so. For this reason, it is not aware of specific entities or source events so if the label query has changed or the source data has changed, ensure that replace is set to True. Features Tables: The Experiment will check on a per-table basis whether or not it exists and contains rows for the entire cohort, and skip the feature generation if so. It does not look at the column list for the feature table or inspect the feature data itself. So, if you have modified any source data that affects a feature aggregation, or added any columns to that aggregation, you won't want to set replace to False. However, it is cohort-and-date aware so you can change around your cohort and temporal configuration safely. Matrix Building: Each matrix's metadata is hashed to create a unique id. If a file exists in storage with that hash, it will be reused. Model Training: Each model's metadata (which includes its train matrix's hash) is hashed to create a unique id. If a file exists in storage with that hash, it will be reused.","title":"Restarting an Experiment"},{"location":"experiments/running/#cli_4","text":"triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --replace","title":"CLI"},{"location":"experiments/running/#python_3","text":"from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment ( config = experiment_config , # a dictionary db_engine = create_engine ( ... ), project_path = 's3://bucket/directory/to/save/data' , replace = True ) experiment . run ()","title":"Python"},{"location":"experiments/running/#optimizing-an-experiment","text":"","title":"Optimizing an Experiment"},{"location":"experiments/running/#skipping-prediction-syncing","text":"By default, the Experiment will save predictions to the database. This can take a long time if your test matrices have a lot of rows, and isn't quite necessary if you just want to see the high-level performance of your grid. By switching save_predictions to False , you can skip the prediction saving. You'll still get your evaluation metrics, so you can look at performance. Don't worry, you can still get your predictions back later by rerunning the Experiment later at default settings, which will find your already-trained models, generate predictions, and save them. CLI: triage experiment myexperiment.yaml --no-save-predictions Python: SingleThreadedExperiment(..., save_predictions=False)","title":"Skipping Prediction Syncing"},{"location":"experiments/running/#running-parts-of-an-experiment","text":"If you would like incrementally build, or just incrementally run parts of the Experiment look at their outputs, you can do so. Running a full experiment requires the experiment config to be filled out, but when you're getting started using Triage it can be easier to build the experiment piece by piece and see the results as they come in. Make sure logging is set to INFO level before running this to ensure you get all the log messages. Additionally, because the default behavior of triage is to run config file validation (which expects a complete experiment configuration) and fill in missing values in some sections with defaults, you will need to pass partial_run=True when constructing your experiment object for a partial experiment (this will also avoid cleaning up intermediate tables from the run, equivalent to cleanup=False ). Running parts of an experiment is only supported through the Python interface.","title":"Running parts of an Experiment"},{"location":"experiments/running/#python_4","text":"experiment.run() will run until it no longer has enough configuration to proceed. You will see information in the logs telling you about the steps it was able to perform. You can additionally view the intermediate tables that are built in the database, which are modified with the experiment hash that the experiment calculates, but this will be printed out in the log messages. labels_*<experiment_hash>* for the labels generated per entity and as of date. tmp_sparse_states_*<experiment_hash>* for the membership in each cohort per entity and as_of_date To reproduce the entire Experiment piece by piece, you can run the following. Each one of these methods requires some portion of experiment config to be passed: experiment.split_definitions will parse temporal config and create time splits. It only requires temporal_config . experiment.generate_cohort() will use the cohort config and as of dates from the temporal config to generate an internal table keeping track of what entities are in the cohort on different dates. It requires temporal_config and cohort_config . experiment.generate_labels() will use the label config and as of dates from the temporal config to generate an internal labels table. It requires temporal_config and label_config . experiment.generate_preimputation_features() will use the feature aggregation config and as of dates from the temporal config to generate internal features tables. It requires temporal_config and feature_aggregations . experiment.generate_imputed_features() will use the imputation sections of the feature aggregation config and the results from the preimputed features to create internal imputed features tables. It requires temporal_config and feature_aggregations . experiment.build_matrices() will use all of the internal tables generated before this point, along with feature grouping config, to generate all needed matrices. It requires temporal_config , cohort_config , label_config , and feature_aggregations , though it will also use feature_group_definitions , feature_group_strategies , and user_metadata if present. experiment.train_and_test_models() will use the generated matrices, grid config and evaluation metric config to train and test all needed models. It requires all configuration keys.","title":"Python"},{"location":"experiments/running/#evaluating-results-of-an-experiment","text":"After the experiment run, a variety of schemas and tables will be created and populated in the configured database: model_metadata.experiments - The experiment configuration, a hash, and some run-invariant details about the configuration model_metadata.experiment_runs - Information about the experiment run that may change from run to run, pertaining to the run environment, status, and results model_metadata.matrices - Each train or test matrix that is built has a row here, with some basic metadata model_metadata.experiment_matrices - A many-to-many table between experiments and matrices. This will have a row if the experiment used the matrix, regardless of whether or not it had to build it model_metadata.models - A model describes a trained classifier; you'll have one row for each trained file that gets saved. model_metadata.experiment_models - A many-to-many table between experiments and models. This will have a row if the experiment used the model, regardless of whether or not it had to build it model_metadata.model_groups - A model groups refers to all models that share parameters like classifier type, hyperparameters, etc, but have different training windows . Look at these to see how classifiers perform over different training windows. model_metadata.matrices - Each matrix that was used for training and testing has metadata written about it such as the matrix hash, length, and time configuration. model_metadata.subsets - Each evaluation subset that was used for model scoring has its configuation and a hash written here train_results.feature_importances - The sklearn feature importances results for each trained model train_results.predictions - Prediction probabilities for train matrix entities generated against trained models train_results.prediction_metadata - Metadata about the prediction stage for a model and train matrix, such as tiebreaking configuration train_results.evaluations - Metric scores of trained models on the training data. test_results.predictions - Prediction probabilities for test matrix entities generated against trained models test_results.prediction_metadata - Metadata about the prediction stage for a model and test matrix, such as tiebreaking configuration test_results.evaluations - Metric scores of trained models over given testing windows and subsets test_results.individual_importances - Individual feature importance scores for test matrix entities. Here's an example query, which returns the top 10 model groups by precision at the top 100 entities: select model_groups.model_group_id, model_groups.model_type, model_groups.hyperparameters, max(test_evaluations.value) as max_precision from model_metadata.model_groups join model_metadata.models using (model_group_id) join test_results.evaluations using (model_id) where metric = 'precision@' and parameter = '100_abs' group by 1,2,3 order by 4 desc limit 10","title":"Evaluating results of an Experiment"},{"location":"experiments/running/#inspecting-an-experiment-before-running","text":"Before you run an experiment, you can inspect properties of the Experiment object to ensure that it is configured in the way you want. Some examples: experiment.all_as_of_times for debugging temporal config. This will show all dates that features and labels will be calculated at. experiment.feature_dicts will output a list of feature dictionaries, representing the feature tables and columns configured in this experiment experiment.matrix_build_tasks will output a list representing each matrix that will be built.","title":"Inspecting an Experiment before running"},{"location":"experiments/running/#optimizing-experiment-performance","text":"","title":"Optimizing Experiment Performance"},{"location":"experiments/running/#profiling-an-experiment","text":"Experiment running slowly? Try the profile keyword argument, or --profile in the command line. This will output a cProfile file to the project path's profiling_stats directory. This is a binary format but can be read with a variety of visualization programs. snakeviz - A browser based graphical viewer. tuna - Another browser based graphical viewer gprof2dot - A command-line tool to convert files to graphviz format pyprof2calltree - A command-line tool to convert files to Valgrind log format, for viewing in established viewers like KCacheGrind Looking at the profile through a visualization program, you can see which portions of the experiment are taking up the most time. Based on this, you may be able to prioritize changes. For instance, if cohort/label/feature table generation are taking up the bulk of the time, you may add indexes to source tables, or increase the number of database processes. On the other hand, if model training is the culprit, you may temporarily try a smaller grid to get results more quickly.","title":"Profiling an Experiment"},{"location":"experiments/running/#materialize_subquery_fromobjs","text":"By default, experiments will inspect the from_obj of every feature aggregation to see if it looks like a subquery, create a table out of it if so, index it on the knowledge_date_column and entity_id , and use that for running feature queries. This can make feature generation go a lot faster if the from_obj takes a decent amount of time to run and/or there are a lot of as-of-dates in the experiment. It won't do this for from_objs that are just tables, or simple joins (e.g. entities join events using (entity_id) ) as the existing indexes you have on those tables should work just fine. You can turn this off if you'd like, which you may want to do if the from_obj subqueries return a lot of data and you want to save as much disk space as possible. The option is turned off by passing materialize_subquery_fromobjs=False to the Experiment.","title":"materialize_subquery_fromobjs"},{"location":"experiments/running/#build-features-independently-of-cohort","text":"By default the feature queries generated by your feature configuration on any given date are joined with the cohort table on that date, which means that no features for entities not in the cohort are saved. This is to save time and database disk space when your cohort on any given date is not very large and allow you to iterate on feature building quickly by default. However, this means that anytime you change your cohort, you have to rebuild all of your features. Depending on your experiment setup (for instance, multiple large cohorts that you experiment with), this may be time-consuming. Change this by passing features_ignore_cohort=True to the Experiment constructor, or --save-all-features to the command-line.","title":"Build Features Independently of Cohort"},{"location":"experiments/running/#experiment-classes","text":"SingleThreadedExperiment : An experiment that performs all tasks serially in a single thread. Good for simple use on small datasets, or for understanding the general flow of data through a pipeline. MultiCoreExperiment : An experiment that makes use of the pebble library to parallelize various time-consuming steps. Takes an n_processes keyword argument to control how many workers to use. RQExperiment : An experiment that makes use of the python-rq library to enqueue individual tasks onto the default queue, and wait for the jobs to be finished before moving on. python-rq requires Redis and any number of worker processes running the Triage codebase. Triage does not set up any of this needed infrastructure for you. Available through the RQ extra ( pip install triage[rq] )","title":"Experiment Classes"},{"location":"experiments/temporal-validation/","text":"Temporal Validation Deep Dive # A temporal validation deep dive is currently available in the Dirty Duck tutorial. Dirty Duck - Temporal Cross-validation You can produce the time graphs detailed in the Dirty Duck deep dive using the Triage CLI or through calling Python code directly. The graphs use matplotlib, so you'll need a matplotlib backend to use. Refer to the matplotlib docs for more details. Python Code # Plotting is supported through the visualize_chops function, which takes a fully configured Timechop object. You may store the configuration for this object in a YAML file if you wish and load from a file, but in this example we directly set the parameters as arguments to the Timechop object. This would enable faster iteration of time config in a notebook setting. from triage.component.timechop.plotting import visualize_chops from triage.component.timechop import Timechop chopper = Timechop( feature_start_time='2010-01-01' feature_end_time='2015-01-01' # latest date included in features label_start_time='2012-01-01' # earliest date for which labels are avialable label_end_time='2015-01-01' # day AFTER last label date (all dates in any model are < this date) model_update_frequency='6month' # how frequently to retrain models training_as_of_date_frequencies='1day' # time between as of dates for same entity in train matrix test_as_of_date_frequencies='3month' # time between as of dates for same entity in test matrix max_training_histories=['6month', '3month'] # length of time included in a train matrix test_durations=['0day', '1month', '2month'] # length of time included in a test matrix (0 days will give a single prediction immediately after training end) training_label_timespans=['1month'] # time period across which outcomes are labeled in train matrices test_label_timespans=['7day'] # time period across which outcomes are labeled in test matrices ) visualize_chops(chopper) Triage CLI # The Triage CLI exposes the showtimechops command which just takes a YAML file as input. This YAML file is expected to have a temporal_config section with Timechop parameters. You can use a full experiment config, or just create a YAML file with only temporal config parameters; the temporal config just has to be present. Here, we use the example_experiment_config.yaml from the Triage repository root as an example. triage showtimechops example_experiment_config.yaml Result # Using either method, you should see output similar to this:","title":"Temporal Validation Deep Dive"},{"location":"experiments/temporal-validation/#temporal-validation-deep-dive","text":"A temporal validation deep dive is currently available in the Dirty Duck tutorial. Dirty Duck - Temporal Cross-validation You can produce the time graphs detailed in the Dirty Duck deep dive using the Triage CLI or through calling Python code directly. The graphs use matplotlib, so you'll need a matplotlib backend to use. Refer to the matplotlib docs for more details.","title":"Temporal Validation Deep Dive"},{"location":"experiments/temporal-validation/#python-code","text":"Plotting is supported through the visualize_chops function, which takes a fully configured Timechop object. You may store the configuration for this object in a YAML file if you wish and load from a file, but in this example we directly set the parameters as arguments to the Timechop object. This would enable faster iteration of time config in a notebook setting. from triage.component.timechop.plotting import visualize_chops from triage.component.timechop import Timechop chopper = Timechop( feature_start_time='2010-01-01' feature_end_time='2015-01-01' # latest date included in features label_start_time='2012-01-01' # earliest date for which labels are avialable label_end_time='2015-01-01' # day AFTER last label date (all dates in any model are < this date) model_update_frequency='6month' # how frequently to retrain models training_as_of_date_frequencies='1day' # time between as of dates for same entity in train matrix test_as_of_date_frequencies='3month' # time between as of dates for same entity in test matrix max_training_histories=['6month', '3month'] # length of time included in a train matrix test_durations=['0day', '1month', '2month'] # length of time included in a test matrix (0 days will give a single prediction immediately after training end) training_label_timespans=['1month'] # time period across which outcomes are labeled in train matrices test_label_timespans=['7day'] # time period across which outcomes are labeled in test matrices ) visualize_chops(chopper)","title":"Python Code"},{"location":"experiments/temporal-validation/#triage-cli","text":"The Triage CLI exposes the showtimechops command which just takes a YAML file as input. This YAML file is expected to have a temporal_config section with Timechop parameters. You can use a full experiment config, or just create a YAML file with only temporal config parameters; the temporal config just has to be present. Here, we use the example_experiment_config.yaml from the Triage repository root as an example. triage showtimechops example_experiment_config.yaml","title":"Triage CLI"},{"location":"experiments/temporal-validation/#result","text":"Using either method, you should see output similar to this:","title":"Result"},{"location":"experiments/upgrade-to-v5/","text":"Upgrading your experiment configuration to v5 # This document details the steps needed to update a triage v3 or v4 configuration to v5, mimicing the old behavior (as opposed to taking advantage of new options) as much as possible. In the experiment configuration v5, several things were changed: state_config becomes cohort_config , and receives new options label_config is changed to take a parameterized query model_group_keys is changed to have more robust defaults, and values specified in the config file act as overrides for the defaults instead of additions to them. state_config -> cohort_config # Upgrading the state config is fairly straightforward, as no functionality was removed. The key at which the state table-based configuration can be passed has changed. Before it resided at the top-level state_config key, whereas now it is in the optional dense_states key within the top-level cohort_config key. Old: state_config: table_name: 'states' state_filters: - 'state_one AND state_two' - '(state_one OR state_two) AND state_three' New: cohort_config: dense_states: table_name: 'states' state_filters: - 'state_one AND state_two' - '(state_one OR state_two) AND state_three' label_config # The label config has had functionality changed, so there is more conversion that needs to happen. Instead of taking in an 'events' table and making assumptions suitable for inspections tasks based on that table, for transparency and flexibility this now takes a parameterized query, as well as an optional include_missing_labels_in_train_as boolean. Leaving out this boolean value reproduces the inspections behavior (missing labels are treated as null), so to upgrade old configurations it is not needed. Old: events_table: 'events' New: label_config: query: | select events.entity_id, bool_or(outcome::bool)::integer as outcome from events where '{as_of_date}' <= outcome_date and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id model_group_keys # The model group configuration was changed quite a bit. Before, the Experiment defined a few default grouping keys and would treat anything included in the config as additional. In practice, there were many keys that were almost always included as additional model group keys, and these are now default. There are also other keys that generally make sense if certain things are iterated on (e.g. feature groups). The goal is for most projects to simply leave out this configuration value entirely. If possible, this is the recommended route to go. But for the purposes of this guide, this change should duplicate the old behavior exactly. Old (empty, using defaults): New: model_group_keys: ['class_path', 'parameters', 'feature_names'] Old (more standard in practice, adding some temporal parameters): model_group_keys: ['label_timespan', 'as_of_date_frequency', 'max_training_history'] New: model_group_keys: ['class_path', 'parameters', 'feature_names', 'label_timespan', 'as_of_date_frequency', 'max_training_history'] Upgrading the experiment config version # At this point, you should be able to bump the top-level experiment config version to v5: Old: config_version: 'v4' New: config_version: 'v5'","title":"v3/v4 -> v5"},{"location":"experiments/upgrade-to-v5/#upgrading-your-experiment-configuration-to-v5","text":"This document details the steps needed to update a triage v3 or v4 configuration to v5, mimicing the old behavior (as opposed to taking advantage of new options) as much as possible. In the experiment configuration v5, several things were changed: state_config becomes cohort_config , and receives new options label_config is changed to take a parameterized query model_group_keys is changed to have more robust defaults, and values specified in the config file act as overrides for the defaults instead of additions to them.","title":"Upgrading your experiment configuration to v5"},{"location":"experiments/upgrade-to-v5/#state_config-cohort_config","text":"Upgrading the state config is fairly straightforward, as no functionality was removed. The key at which the state table-based configuration can be passed has changed. Before it resided at the top-level state_config key, whereas now it is in the optional dense_states key within the top-level cohort_config key. Old: state_config: table_name: 'states' state_filters: - 'state_one AND state_two' - '(state_one OR state_two) AND state_three' New: cohort_config: dense_states: table_name: 'states' state_filters: - 'state_one AND state_two' - '(state_one OR state_two) AND state_three'","title":"state_config -&gt; cohort_config"},{"location":"experiments/upgrade-to-v5/#label_config","text":"The label config has had functionality changed, so there is more conversion that needs to happen. Instead of taking in an 'events' table and making assumptions suitable for inspections tasks based on that table, for transparency and flexibility this now takes a parameterized query, as well as an optional include_missing_labels_in_train_as boolean. Leaving out this boolean value reproduces the inspections behavior (missing labels are treated as null), so to upgrade old configurations it is not needed. Old: events_table: 'events' New: label_config: query: | select events.entity_id, bool_or(outcome::bool)::integer as outcome from events where '{as_of_date}' <= outcome_date and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id","title":"label_config"},{"location":"experiments/upgrade-to-v5/#model_group_keys","text":"The model group configuration was changed quite a bit. Before, the Experiment defined a few default grouping keys and would treat anything included in the config as additional. In practice, there were many keys that were almost always included as additional model group keys, and these are now default. There are also other keys that generally make sense if certain things are iterated on (e.g. feature groups). The goal is for most projects to simply leave out this configuration value entirely. If possible, this is the recommended route to go. But for the purposes of this guide, this change should duplicate the old behavior exactly. Old (empty, using defaults): New: model_group_keys: ['class_path', 'parameters', 'feature_names'] Old (more standard in practice, adding some temporal parameters): model_group_keys: ['label_timespan', 'as_of_date_frequency', 'max_training_history'] New: model_group_keys: ['class_path', 'parameters', 'feature_names', 'label_timespan', 'as_of_date_frequency', 'max_training_history']","title":"model_group_keys"},{"location":"experiments/upgrade-to-v5/#upgrading-the-experiment-config-version","text":"At this point, you should be able to bump the top-level experiment config version to v5: Old: config_version: 'v4' New: config_version: 'v5'","title":"Upgrading the experiment config version"},{"location":"experiments/upgrade-to-v6/","text":"Upgrading your experiment configuration to v6 # This document details the steps needed to update a triage v5 configuration to v6, mimicking the old behavior. Experiment configuration v6 includes only one change from v5: When specifying the cohort_config , if a query is given , the {af_of_date} is no longer quoted or casted by Triage. Instead, the user must perform the quoting and casting, as is done already for the label_config . Old: cohort_config: query: | SELECT DISTINCT entity_id FROM semantic.events WHERE event = 'booking' AND startdt <@ daterange(({as_of_date} - '3 years'::interval)::date, {as_of_date}) AND enddt < {as_of_date} LIMIT 100 name: 'booking_last_3_years_limit_100' New: cohort_config: query: | SELECT DISTINCT entity_id FROM semantic.events WHERE event = 'booking' AND startdt <@ daterange(('{as_of_date}'::date - '3 years'::interval)::date, '{as_of_date}'::date) AND enddt < '{as_of_date}' LIMIT 100 name: 'booking_last_3_years_limit_100' Upgrading the experiment config version # At this point, you should be able to bump the top-level experiment config version to v6: Old: config_version: 'v5' New: config_version: 'v6'","title":"v5 -> v6"},{"location":"experiments/upgrade-to-v6/#upgrading-your-experiment-configuration-to-v6","text":"This document details the steps needed to update a triage v5 configuration to v6, mimicking the old behavior. Experiment configuration v6 includes only one change from v5: When specifying the cohort_config , if a query is given , the {af_of_date} is no longer quoted or casted by Triage. Instead, the user must perform the quoting and casting, as is done already for the label_config . Old: cohort_config: query: | SELECT DISTINCT entity_id FROM semantic.events WHERE event = 'booking' AND startdt <@ daterange(({as_of_date} - '3 years'::interval)::date, {as_of_date}) AND enddt < {as_of_date} LIMIT 100 name: 'booking_last_3_years_limit_100' New: cohort_config: query: | SELECT DISTINCT entity_id FROM semantic.events WHERE event = 'booking' AND startdt <@ daterange(('{as_of_date}'::date - '3 years'::interval)::date, '{as_of_date}'::date) AND enddt < '{as_of_date}' LIMIT 100 name: 'booking_last_3_years_limit_100'","title":"Upgrading your experiment configuration to v6"},{"location":"experiments/upgrade-to-v6/#upgrading-the-experiment-config-version","text":"At this point, you should be able to bump the top-level experiment config version to v6: Old: config_version: 'v5' New: config_version: 'v6'","title":"Upgrading the experiment config version"},{"location":"experiments/upgrade-to-v7/","text":"Upgrading your experiment configuration to v7 # This document details the steps needed to update a triage v6 configuration to v7, mimicking the old behavior. Experiment configuration v7 includes only one change from v6: the addition of a mandatory random_seed, that is set at the beginning of the experiment and affects all subsequent random numbers. It is expected to be an integer. Old: config_version : 'v6' # EXPERIMENT METADATA New: config_version : 'v7' # EXPERIMENT METADATA # random_seed will be set in Python at the beginning of the experiment and # affect the generation of all model seeds random_seed : 23895478","title":"v6 -> v7"},{"location":"experiments/upgrade-to-v7/#upgrading-your-experiment-configuration-to-v7","text":"This document details the steps needed to update a triage v6 configuration to v7, mimicking the old behavior. Experiment configuration v7 includes only one change from v6: the addition of a mandatory random_seed, that is set at the beginning of the experiment and affects all subsequent random numbers. It is expected to be an integer. Old: config_version : 'v6' # EXPERIMENT METADATA New: config_version : 'v7' # EXPERIMENT METADATA # random_seed will be set in Python at the beginning of the experiment and # affect the generation of all model seeds random_seed : 23895478","title":"Upgrading your experiment configuration to v7"},{"location":"postmodeling/","text":"Postmodeling # The bulk of postmodeling is documented on its README. Read it here Crosstabs # One of the features in postmodeling, detailed in the README above, is the ability to view crosstabs. Using the .crosstabs property on the ModelEvaluator requires the test_results.crosstabs table to be created first. You can do this either with the CLI or in a Python console: The model crosstabs populates a table in a postgres database containing model_id , as_of_date , threshold_unit , threshold_value , and feature_column , such as the mean and standard deviation of the given feature for the predicted-high-risk and predicted-low-risk group. In other words, this table provides simple descriptives of the feature distributions for the high-risk and low-risk entities, given a specific model and decision threshold. The crosstabs config consist of the following parameters: Output schema and table - the name of the schema and table in the postgresdb where the results should be pushed to. Lists of thresholds (abs and/or pct) to serve as cutoff for high risk (positive) and low risk(negative) predictions. (optional) a list of entity_ids to subset for the crosstabs the analysis Models list query must return a column model_id . You can pass an explicit array of model ids using unnest(ARRAY[1,2,3]):: int or you can query by all model ids from a given model group or by dates, it's up to you (as long as it returns a column model_id ) A list of dates query. Very similar to the previous point, you can either unnest a pre-defined list of dates or execute a more complex query that returns a column as_of_date . The models_dates_join_query is supposed to be fixed. Just change this if you are really sure. This query is necessary because different model_ids can be used for predicting at multiple as_of_dates we need to make sure that model_id, as_of_date pairs really exist in a table containing predictions. The features query is used to specify a feature table or (joins of multiple feature tables) that should be joined with the models_dates_join_query results. Finally, the predictions query should return a model_id, as_of_date, entity_id, score, label_value, rank_abs and rank_pct columns. CLI # triage crosstabs example/config/postmodeling_crosstabs.yaml will run crosstabs for the given config YAML. The config YAML is highly dependent on what model ids and as-of-dates and feature tables are in the database. That example file needs to be modified to work with your experiment and postmodeling interests. Consult the instructions above for help in modifying the file. Python # This can be run using the triage.component.postmodeling.crosstabs.run_crosstabs function, which takes in a database engine and a loaded CrosstabsConfigLoader object. Example: from triage.component.postmodeling.crosstabs import CrosstabsConfigLoader , run_crosstabs from sqlalchemy import create_engine db_engine = create_engine ( < mydburl > ) config = CrosstabsConfigLoader ( config_file = 'example/config/postmodeling_crosstabs.yaml' ) run_crosstabs ( db_engine , config )","title":"Postmodeling"},{"location":"postmodeling/#postmodeling","text":"The bulk of postmodeling is documented on its README. Read it here","title":"Postmodeling"},{"location":"postmodeling/#crosstabs","text":"One of the features in postmodeling, detailed in the README above, is the ability to view crosstabs. Using the .crosstabs property on the ModelEvaluator requires the test_results.crosstabs table to be created first. You can do this either with the CLI or in a Python console: The model crosstabs populates a table in a postgres database containing model_id , as_of_date , threshold_unit , threshold_value , and feature_column , such as the mean and standard deviation of the given feature for the predicted-high-risk and predicted-low-risk group. In other words, this table provides simple descriptives of the feature distributions for the high-risk and low-risk entities, given a specific model and decision threshold. The crosstabs config consist of the following parameters: Output schema and table - the name of the schema and table in the postgresdb where the results should be pushed to. Lists of thresholds (abs and/or pct) to serve as cutoff for high risk (positive) and low risk(negative) predictions. (optional) a list of entity_ids to subset for the crosstabs the analysis Models list query must return a column model_id . You can pass an explicit array of model ids using unnest(ARRAY[1,2,3]):: int or you can query by all model ids from a given model group or by dates, it's up to you (as long as it returns a column model_id ) A list of dates query. Very similar to the previous point, you can either unnest a pre-defined list of dates or execute a more complex query that returns a column as_of_date . The models_dates_join_query is supposed to be fixed. Just change this if you are really sure. This query is necessary because different model_ids can be used for predicting at multiple as_of_dates we need to make sure that model_id, as_of_date pairs really exist in a table containing predictions. The features query is used to specify a feature table or (joins of multiple feature tables) that should be joined with the models_dates_join_query results. Finally, the predictions query should return a model_id, as_of_date, entity_id, score, label_value, rank_abs and rank_pct columns.","title":"Crosstabs"},{"location":"postmodeling/#cli","text":"triage crosstabs example/config/postmodeling_crosstabs.yaml will run crosstabs for the given config YAML. The config YAML is highly dependent on what model ids and as-of-dates and feature tables are in the database. That example file needs to be modified to work with your experiment and postmodeling interests. Consult the instructions above for help in modifying the file.","title":"CLI"},{"location":"postmodeling/#python","text":"This can be run using the triage.component.postmodeling.crosstabs.run_crosstabs function, which takes in a database engine and a loaded CrosstabsConfigLoader object. Example: from triage.component.postmodeling.crosstabs import CrosstabsConfigLoader , run_crosstabs from sqlalchemy import create_engine db_engine = create_engine ( < mydburl > ) config = CrosstabsConfigLoader ( config_file = 'example/config/postmodeling_crosstabs.yaml' ) run_crosstabs ( db_engine , config )","title":"Python"}]}