{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Triage \u00b6 Risk modeling and prediction Predictive analytics projects require the coordination of many different tasks, such as feature generation, classifier training, evaluation, and list generation. These tasks are complicated in their own right, but in addition have to be combined in different ways throughout the course of the project. Triage aims to provide interfaces to these different phases of a project, such as an Experiment . Each phase is defined by configuration specific to the needs of the project, and an arrangement of core data science components that work together to produce the output of that phase. mermaid.initialize({startOnLoad:true}); The phases currently implemented in Triage are: graph LR Experiment[\"Experiment (create features and models)\"] Audition[\"Audition (pick the best models)\"] Postmodeling[\"Postmodeling (dive into best models)\"] Experiment --> Audition Audition --> Postmodeling Experiment \u00b6 I have a bunch of data. How do I create some models? An experiment represents the initial research work of creating design matrices from source data, and training/testing/evaluating a model grid on those matrices. At the end of the experiment, a relational database with results metadata is populated, allowing for evaluation by the researcher. If you're new to Triage Experiments, check out the Dirty Duck tutorial . It's a guided tour through Triage functionality using a real-world problem. If you're familiar with creating an Experiment but want to see more reference documentation and some deep dives, check out the links on the side. Audition \u00b6 I just trained a bunch of models. How do I pick the best ones? Audition is a tool for picking the best trained classifiers from a predictive analytics experiment. Often, production-scale experiments will come up with thousands of trained models, and sifting through all of those results can be time-consuming even after calculating the usual basic metrics like precision and recall. Which metrics matter most? Should you prioritize the best metric value over time or treat recent data as most important? Is low metric variance important? The answers to questions like these may not be obvious up front. Audition introduces a structured, semi-automated way of filtering models based on what you consider important, with an interface that is easy to interact with from a Jupyter notebook (with plots), but is driven by configuration that can easily be scripted. To get started with Audition, check out its README Postmodeling \u00b6 What is the distribution of my scores? What is generating a higher FPR in model x compared to model y? What is the single most important feature in my models?` This questions, and other ones, are the kind of inquiries that the triage user may have in mind when scrolling trough the models selected by the Audition component. Choosing the right model for deployment and exploring its predictions and behavior in time is a pivotal task. postmodeling will help to answer some of this questions by exploring the outcomes of the model, and exploring \"deeply\" into the model behavior across time and features. To get started with Postmodeling, check out its README Background \u00b6 Triage is developed at the University of Chicago's Center For Data Science and Public Policy . We created it in response to commonly occuring challenges we've encountered and patterns we've developed while working on projects for our partners.","title":"Home"},{"location":"#triage","text":"Risk modeling and prediction Predictive analytics projects require the coordination of many different tasks, such as feature generation, classifier training, evaluation, and list generation. These tasks are complicated in their own right, but in addition have to be combined in different ways throughout the course of the project. Triage aims to provide interfaces to these different phases of a project, such as an Experiment . Each phase is defined by configuration specific to the needs of the project, and an arrangement of core data science components that work together to produce the output of that phase. mermaid.initialize({startOnLoad:true}); The phases currently implemented in Triage are: graph LR Experiment[\"Experiment (create features and models)\"] Audition[\"Audition (pick the best models)\"] Postmodeling[\"Postmodeling (dive into best models)\"] Experiment --> Audition Audition --> Postmodeling","title":"Triage"},{"location":"#experiment","text":"I have a bunch of data. How do I create some models? An experiment represents the initial research work of creating design matrices from source data, and training/testing/evaluating a model grid on those matrices. At the end of the experiment, a relational database with results metadata is populated, allowing for evaluation by the researcher. If you're new to Triage Experiments, check out the Dirty Duck tutorial . It's a guided tour through Triage functionality using a real-world problem. If you're familiar with creating an Experiment but want to see more reference documentation and some deep dives, check out the links on the side.","title":"Experiment"},{"location":"#audition","text":"I just trained a bunch of models. How do I pick the best ones? Audition is a tool for picking the best trained classifiers from a predictive analytics experiment. Often, production-scale experiments will come up with thousands of trained models, and sifting through all of those results can be time-consuming even after calculating the usual basic metrics like precision and recall. Which metrics matter most? Should you prioritize the best metric value over time or treat recent data as most important? Is low metric variance important? The answers to questions like these may not be obvious up front. Audition introduces a structured, semi-automated way of filtering models based on what you consider important, with an interface that is easy to interact with from a Jupyter notebook (with plots), but is driven by configuration that can easily be scripted. To get started with Audition, check out its README","title":"Audition"},{"location":"#postmodeling","text":"What is the distribution of my scores? What is generating a higher FPR in model x compared to model y? What is the single most important feature in my models?` This questions, and other ones, are the kind of inquiries that the triage user may have in mind when scrolling trough the models selected by the Audition component. Choosing the right model for deployment and exploring its predictions and behavior in time is a pivotal task. postmodeling will help to answer some of this questions by exploring the outcomes of the model, and exploring \"deeply\" into the model behavior across time and features. To get started with Postmodeling, check out its README","title":"Postmodeling"},{"location":"#background","text":"Triage is developed at the University of Chicago's Center For Data Science and Public Policy . We created it in response to commonly occuring challenges we've encountered and patterns we've developed while working on projects for our partners.","title":"Background"},{"location":"triage.experiments.base/","text":"Source: triage/experiments/base.py#L0 Global Variables \u00b6 CONFIG_VERSION dt_from_str \u00b6 dt_from_str(dt_str) ExperimentBase \u00b6 The Base class for all Experiments. ExperimentBase.all_as_of_times \u00b6 All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes ExperimentBase.all_label_windows \u00b6 All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config ExperimentBase.collate_aggregations \u00b6 collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects ExperimentBase.feature_dicts \u00b6 Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names ExperimentBase.feature_table_tasks \u00b6 All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands ExperimentBase.full_matrix_definitions \u00b6 Full matrix definitions Returns: (list) temporal and feature information for each matrix ExperimentBase.master_feature_dictionary \u00b6 All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names ExperimentBase.matrix_build_tasks \u00b6 Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts ExperimentBase.split_definitions \u00b6 Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] } ExperimentBase. __init__ \u00b6 __init__(self, config, db_engine, model_storage_class=None, project_path=None, replace=True) Initialize self. See help(type(self)) for accurate signature. ExperimentBase.build_matrices \u00b6 build_matrices(self) Generate labels, features, and matrices ExperimentBase.catwalk \u00b6 catwalk(self) Train, test, and evaluate models ExperimentBase.generate_labels \u00b6 generate_labels(self) Generate labels based on experiment configuration Results are stored in the database, not returned ExperimentBase.generate_sparse_states \u00b6 generate_sparse_states(self) ExperimentBase.initialize_components \u00b6 initialize_components(self) ExperimentBase.initialize_factories \u00b6 initialize_factories(self) ExperimentBase.log_split \u00b6 log_split(self, split_num, split) ExperimentBase.matrix_store \u00b6 matrix_store(self, matrix_uuid) Construct a matrix store for a given matrix uuid, using the Experiment's #matrix_store_class Args: matrix_uuid (string) A uuid for a matrix ExperimentBase.run \u00b6 run(self) ExperimentBase.update_split_definitions \u00b6 update_split_definitions(self, new_split_definitions) Update split definitions Args: (dict) split definitions (should have matrix uuids)","title":"Triage.experiments.base"},{"location":"triage.experiments.base/#global-variables","text":"CONFIG_VERSION","title":"Global Variables"},{"location":"triage.experiments.base/#dt_from_str","text":"dt_from_str(dt_str)","title":"dt_from_str"},{"location":"triage.experiments.base/#experimentbase","text":"The Base class for all Experiments.","title":"ExperimentBase"},{"location":"triage.experiments.base/#experimentbaseall_as_of_times","text":"All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes","title":"ExperimentBase.all_as_of_times"},{"location":"triage.experiments.base/#experimentbaseall_label_windows","text":"All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config","title":"ExperimentBase.all_label_windows"},{"location":"triage.experiments.base/#experimentbasecollate_aggregations","text":"collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects","title":"ExperimentBase.collate_aggregations"},{"location":"triage.experiments.base/#experimentbasefeature_dicts","text":"Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names","title":"ExperimentBase.feature_dicts"},{"location":"triage.experiments.base/#experimentbasefeature_table_tasks","text":"All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands","title":"ExperimentBase.feature_table_tasks"},{"location":"triage.experiments.base/#experimentbasefull_matrix_definitions","text":"Full matrix definitions Returns: (list) temporal and feature information for each matrix","title":"ExperimentBase.full_matrix_definitions"},{"location":"triage.experiments.base/#experimentbasemaster_feature_dictionary","text":"All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names","title":"ExperimentBase.master_feature_dictionary"},{"location":"triage.experiments.base/#experimentbasematrix_build_tasks","text":"Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts","title":"ExperimentBase.matrix_build_tasks"},{"location":"triage.experiments.base/#experimentbasesplit_definitions","text":"Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] }","title":"ExperimentBase.split_definitions"},{"location":"triage.experiments.base/#experimentbase__init__","text":"__init__(self, config, db_engine, model_storage_class=None, project_path=None, replace=True) Initialize self. See help(type(self)) for accurate signature.","title":"ExperimentBase.__init__"},{"location":"triage.experiments.base/#experimentbasebuild_matrices","text":"build_matrices(self) Generate labels, features, and matrices","title":"ExperimentBase.build_matrices"},{"location":"triage.experiments.base/#experimentbasecatwalk","text":"catwalk(self) Train, test, and evaluate models","title":"ExperimentBase.catwalk"},{"location":"triage.experiments.base/#experimentbasegenerate_labels","text":"generate_labels(self) Generate labels based on experiment configuration Results are stored in the database, not returned","title":"ExperimentBase.generate_labels"},{"location":"triage.experiments.base/#experimentbasegenerate_sparse_states","text":"generate_sparse_states(self)","title":"ExperimentBase.generate_sparse_states"},{"location":"triage.experiments.base/#experimentbaseinitialize_components","text":"initialize_components(self)","title":"ExperimentBase.initialize_components"},{"location":"triage.experiments.base/#experimentbaseinitialize_factories","text":"initialize_factories(self)","title":"ExperimentBase.initialize_factories"},{"location":"triage.experiments.base/#experimentbaselog_split","text":"log_split(self, split_num, split)","title":"ExperimentBase.log_split"},{"location":"triage.experiments.base/#experimentbasematrix_store","text":"matrix_store(self, matrix_uuid) Construct a matrix store for a given matrix uuid, using the Experiment's #matrix_store_class Args: matrix_uuid (string) A uuid for a matrix","title":"ExperimentBase.matrix_store"},{"location":"triage.experiments.base/#experimentbaserun","text":"run(self)","title":"ExperimentBase.run"},{"location":"triage.experiments.base/#experimentbaseupdate_split_definitions","text":"update_split_definitions(self, new_split_definitions) Update split definitions Args: (dict) split definitions (should have matrix uuids)","title":"ExperimentBase.update_split_definitions"},{"location":"triage.experiments.multicore/","text":"Source: triage/experiments/multicore.py#L0 insert_into_table \u00b6 insert_into_table(insert_statements, feature_generator_factory, db_connection_string) build_matrix \u00b6 build_matrix(build_tasks, planner_factory, db_connection_string) train_model \u00b6 train_model(train_tasks, trainer_factory, db_connection_string) test_and_evaluate \u00b6 test_and_evaluate(model_ids, predictor_factory, evaluator_factory, indiv_importance_factory, \\ test_store, db_connection_string, split_def, train_matrix_columns, config) MultiCoreExperiment \u00b6 The Base class for all Experiments. MultiCoreExperiment.all_as_of_times \u00b6 All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes MultiCoreExperiment.all_label_windows \u00b6 All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config MultiCoreExperiment.collate_aggregations \u00b6 collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects MultiCoreExperiment.feature_dicts \u00b6 Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names MultiCoreExperiment.feature_table_tasks \u00b6 All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands MultiCoreExperiment.full_matrix_definitions \u00b6 Full matrix definitions Returns: (list) temporal and feature information for each matrix MultiCoreExperiment.master_feature_dictionary \u00b6 All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names MultiCoreExperiment.matrix_build_tasks \u00b6 Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts MultiCoreExperiment.split_definitions \u00b6 Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] } MultiCoreExperiment. __init__ \u00b6 __init__(self, n_processes=1, n_db_processes=1, *args, **kwargs) Initialize self. See help(type(self)) for accurate signature. MultiCoreExperiment.build_matrices \u00b6 build_matrices(self) Generate labels, features, and matrices MultiCoreExperiment.catwalk \u00b6 catwalk(self) Train, test, and evaluate models MultiCoreExperiment.parallelize \u00b6 parallelize(self, partially_bound_function, tasks, n_processes, chunksize=1) MultiCoreExperiment.parallelize_with_success_count \u00b6 parallelize_with_success_count(self, partially_bound_function, tasks, n_processes, chunksize=1)","title":"Triage.experiments.multicore"},{"location":"triage.experiments.multicore/#insert_into_table","text":"insert_into_table(insert_statements, feature_generator_factory, db_connection_string)","title":"insert_into_table"},{"location":"triage.experiments.multicore/#build_matrix","text":"build_matrix(build_tasks, planner_factory, db_connection_string)","title":"build_matrix"},{"location":"triage.experiments.multicore/#train_model","text":"train_model(train_tasks, trainer_factory, db_connection_string)","title":"train_model"},{"location":"triage.experiments.multicore/#test_and_evaluate","text":"test_and_evaluate(model_ids, predictor_factory, evaluator_factory, indiv_importance_factory, \\ test_store, db_connection_string, split_def, train_matrix_columns, config)","title":"test_and_evaluate"},{"location":"triage.experiments.multicore/#multicoreexperiment","text":"The Base class for all Experiments.","title":"MultiCoreExperiment"},{"location":"triage.experiments.multicore/#multicoreexperimentall_as_of_times","text":"All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes","title":"MultiCoreExperiment.all_as_of_times"},{"location":"triage.experiments.multicore/#multicoreexperimentall_label_windows","text":"All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config","title":"MultiCoreExperiment.all_label_windows"},{"location":"triage.experiments.multicore/#multicoreexperimentcollate_aggregations","text":"collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects","title":"MultiCoreExperiment.collate_aggregations"},{"location":"triage.experiments.multicore/#multicoreexperimentfeature_dicts","text":"Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names","title":"MultiCoreExperiment.feature_dicts"},{"location":"triage.experiments.multicore/#multicoreexperimentfeature_table_tasks","text":"All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands","title":"MultiCoreExperiment.feature_table_tasks"},{"location":"triage.experiments.multicore/#multicoreexperimentfull_matrix_definitions","text":"Full matrix definitions Returns: (list) temporal and feature information for each matrix","title":"MultiCoreExperiment.full_matrix_definitions"},{"location":"triage.experiments.multicore/#multicoreexperimentmaster_feature_dictionary","text":"All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names","title":"MultiCoreExperiment.master_feature_dictionary"},{"location":"triage.experiments.multicore/#multicoreexperimentmatrix_build_tasks","text":"Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts","title":"MultiCoreExperiment.matrix_build_tasks"},{"location":"triage.experiments.multicore/#multicoreexperimentsplit_definitions","text":"Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] }","title":"MultiCoreExperiment.split_definitions"},{"location":"triage.experiments.multicore/#multicoreexperiment__init__","text":"__init__(self, n_processes=1, n_db_processes=1, *args, **kwargs) Initialize self. See help(type(self)) for accurate signature.","title":"MultiCoreExperiment.__init__"},{"location":"triage.experiments.multicore/#multicoreexperimentbuild_matrices","text":"build_matrices(self) Generate labels, features, and matrices","title":"MultiCoreExperiment.build_matrices"},{"location":"triage.experiments.multicore/#multicoreexperimentcatwalk","text":"catwalk(self) Train, test, and evaluate models","title":"MultiCoreExperiment.catwalk"},{"location":"triage.experiments.multicore/#multicoreexperimentparallelize","text":"parallelize(self, partially_bound_function, tasks, n_processes, chunksize=1)","title":"MultiCoreExperiment.parallelize"},{"location":"triage.experiments.multicore/#multicoreexperimentparallelize_with_success_count","text":"parallelize_with_success_count(self, partially_bound_function, tasks, n_processes, chunksize=1)","title":"MultiCoreExperiment.parallelize_with_success_count"},{"location":"triage.experiments.singlethreaded/","text":"Source: triage/experiments/singlethreaded.py#L0 SingleThreadedExperiment \u00b6 The Base class for all Experiments. SingleThreadedExperiment.all_as_of_times \u00b6 All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes SingleThreadedExperiment.all_label_windows \u00b6 All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config SingleThreadedExperiment.collate_aggregations \u00b6 collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects SingleThreadedExperiment.feature_dicts \u00b6 Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names SingleThreadedExperiment.feature_table_tasks \u00b6 All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands SingleThreadedExperiment.full_matrix_definitions \u00b6 Full matrix definitions Returns: (list) temporal and feature information for each matrix SingleThreadedExperiment.master_feature_dictionary \u00b6 All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names SingleThreadedExperiment.matrix_build_tasks \u00b6 Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts SingleThreadedExperiment.split_definitions \u00b6 Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] } SingleThreadedExperiment. __init__ \u00b6 __init__(self, config, db_engine, model_storage_class=None, project_path=None, replace=True) Initialize self. See help(type(self)) for accurate signature. SingleThreadedExperiment.build_matrices \u00b6 build_matrices(self) Generate labels, features, and matrices SingleThreadedExperiment.catwalk \u00b6 catwalk(self) Train, test, and evaluate models","title":"Triage.experiments.singlethreaded"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperiment","text":"The Base class for all Experiments.","title":"SingleThreadedExperiment"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentall_as_of_times","text":"All 'as of times' in experiment config Used for label and feature generation. Returns: (list) of datetimes","title":"SingleThreadedExperiment.all_as_of_times"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentall_label_windows","text":"All train and test label windows Returns: (list) label windows, in string form as they appeared in the experiment config","title":"SingleThreadedExperiment.all_label_windows"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentcollate_aggregations","text":"collate Aggregation objects used by this experiment. Returns: (list) of collate.Aggregation objects","title":"SingleThreadedExperiment.collate_aggregations"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentfeature_dicts","text":"Feature dictionaries, representing the feature tables and columns configured in this experiment after computing feature groups. Returns: (list) of dicts, keys being feature table names and values being lists of feature names","title":"SingleThreadedExperiment.feature_dicts"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentfeature_table_tasks","text":"All feature table query tasks specified by this Experiment Returns: (dict) keys are group table names, values are themselves dicts, each with keys for different stages of table creation (prepare, inserts, finalize) and with values being lists of SQL commands","title":"SingleThreadedExperiment.feature_table_tasks"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentfull_matrix_definitions","text":"Full matrix definitions Returns: (list) temporal and feature information for each matrix","title":"SingleThreadedExperiment.full_matrix_definitions"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentmaster_feature_dictionary","text":"All possible features found in the database. Not all features will necessarily end up in matrices Returns: (list) of dicts, keys being feature table names and values being lists of feature names","title":"SingleThreadedExperiment.master_feature_dictionary"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentmatrix_build_tasks","text":"Tasks for all matrices that need to be built as a part of this Experiment. Each task contains arguments understood by Architect.build_matrix Returns: (list) of dicts","title":"SingleThreadedExperiment.matrix_build_tasks"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentsplit_definitions","text":"Temporal splits based on the experiment's configuration Returns: (dict) temporal splits Example: { 'beginning_of_time': {datetime}, 'modeling_start_time': {datetime}, 'modeling_end_time': {datetime}, 'train_matrix': { 'matrix_start_time': {datetime}, 'matrix_end_time': {datetime}, 'as_of_times': [list of {datetime}s] }, 'test_matrices': [list of matrix defs similar to train_matrix] }","title":"SingleThreadedExperiment.split_definitions"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperiment__init__","text":"__init__(self, config, db_engine, model_storage_class=None, project_path=None, replace=True) Initialize self. See help(type(self)) for accurate signature.","title":"SingleThreadedExperiment.__init__"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentbuild_matrices","text":"build_matrices(self) Generate labels, features, and matrices","title":"SingleThreadedExperiment.build_matrices"},{"location":"triage.experiments.singlethreaded/#singlethreadedexperimentcatwalk","text":"catwalk(self) Train, test, and evaluate models","title":"SingleThreadedExperiment.catwalk"},{"location":"experiments/algorithm/","text":"Experiment Algorithm Deep Dive \u00b6 This guide's purpose is to provide familiarity of the inner workings of a Triage Experiment to people with some experience in data science and Python. A Triage Experiment is a highly structured way of defining the experimentation phase of a data science project. To those wondering whether this Experiment structure is flexible enough to fit their needs, this should help. 1. Temporal Validation Setup \u00b6 First, the given temporal_config section in the experiment definition is transformed into train and test splits, including as_of_times for each matrix. We create these splits by figuring out the latest reasonable split time from the inputs, and moving backwards in time at the rate of the given model_update_frequency , until we get to the earliest reasonable split time. For each split, we create as_of_times by moving either backwards from the split time towards the max_training_history (for train matrices) or forwards from the split time towards the test_duration (for test matrices) at the provided data_frequency . Many of these configured values may be lists, in which case we generate the cross-product of all the possible values and generate more splits. For a more detailed look at the temporal validation logic, see Temporal Validation Deep Dive . The train and test splits themselves are not used until the Building Matrices section, but a flat list of all computed as_of_times for all matrices needed in the experiment is used in the next section, Transforming Data . 2. Transforming Data \u00b6 With all of the as_of_times for this Experiment now computed, it's now possible to transform the input data into features and labels as of all the required times. Labels \u00b6 The Experiment populates a 'labels' table using the following input: 1. A query, provided by the user in the configuration file, that generates entity_ids and outcomes for a given as_of_date and label_timespan. Each as_of_date and label_timespan defined in temporal config For instance, an inspections-style query (for the given timespan, return the entity and outcome of any matching inspections) would look like: select events.entity_id, bool_or(outcome::bool)::integer as outcome from events where '{as_of_date}' <= outcome_date and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id This binary labels table is scoped to the entire Experiment, so all as_of_time (computed in step 1) and label_timespan (taken straight from temporal_config ) combinations are present. Additionally, the 'label_name' and 'label_type' are also recorded with each row in the table. The name of the labels table is based on both the name of the label and a hash of the label query (e.g labels_failedviolation_a0b1c2d3 ), so any prior experiments that shared both the name and query will be able to reuse the labels table. If the 'replace' flag was sent, for each as_of_time and label_timespan , the labels table is queried to check if any rows exist that match. If any such rows exist, the labels query for that date and timespan is not run. At this point, the 'labels' table may not have entries for all entities and dates that need to be in a given matrix. How these rows have their labels represented is up to the configured include_missing_labels_in_train_as value in the experiment. This value is not processed when we generate the labels table, but later on when the matrix is built (see 'Retrieving Data and Saving Completed Matrix') Cohort Table \u00b6 The Experiment keeps track of the which entities are in the cohort on any given date. Similarly to the labels table, the experiment populates a cohort table using the following input: A query, provided by the user in the configuration file, that generates entity_ids for a given as_of_date. Each as_of_date as defined in temporal config This cohort table is scoped to the entire Experiment, so all as_of_times (computed in step 1) are present. The name of the cohort table is based on both the name of the cohort and a hash of the cohort query (e.g cohort_permitted_a0b1c2d3 ), so any prior experiments that shared both the name and query will be able to reuse the cohort table. If the 'replace' flag was sent, for each as_of_time , the cohort table is queried to check if any rows exist that match. If any such rows exist, the cohort query for that date is not run. Features \u00b6 Each provided feature_aggregation configures the creation and population of several feature tables in the 'features' schema: one for each of the groups specified in the config, one that merges the groups together into one table, and one that fills in null values from the merged table with imputed values based on imputation config. Generating Aggregation SQL \u00b6 To generate the SQL that creates the pre-imputation table, the Experiment assembles building blocks from the feature aggregation config, as well as the experiment's list of as_of_times : from_obj represents, well, the object of the FROM clause in the SQL query. Often this is just a table, but can be configured to be a subquery. This holds all the data that we want to aggregate into features Each as_of_time in the experiment and interval in the feature_aggregation is combined with the knowledge_date_column to create a WHERE clause representing a valid window of events to aggregate in the from_obj : e.g ( where {knowledge_date_column} >= {as_of_time} - interval {interval} ) Each aggregate , categorical , or array_categorical represents a SELECT clause. For aggregates, the quantity is a column or SQL expression representing a numeric quantity present in the from_obj , and the metrics are any number of aggregate functions we want to use. The aggregate function is applied to the quantity. Each group is a column applied to the GROUP BY clause. Generally this is 'entity_id', but higher-level groupings (for instance, 'zip_code') can be used as long as they can be rolled up to 'entity_id'. By default the query is joined with the cohort table to remove unnecessary rows. If features_ignore_cohort is passed to the Experiment this is not done. So a simplified version of a typical query would look like: SELECT {group}, {metric}({quantity}) FROM {from_obj} JOIN {cohort_table} ON ( {cohort_table.entity_id} = {from_obj.entity_id} AND {cohort_table.date} = {as_of_time} ) WHERE {knowledge_date_column} >= {as_of_time} - interval {interval} GROUP BY {group} Writing Group-wide Feature Tables \u00b6 For each as_of_time , the results from the generated query are written to a table whose name is prefixed with the prefix , and suffixed with the group . For instance, if the configuration specifies zipcode-level aggregates and entity-level aggregates, there will be a table for each, keyed on its group plus the as_of_date. Merging into Aggregation-wide Feature Tables \u00b6 Each generated group table is combined into one representing the whole aggregation with a left join. Given that the groups originally came from the same table (the from_obj of the aggregation) and therefore we know the zipcode for each entity, what we do now is create a table that would be keyed on entity and as_of_date, and contain all entity-level and zipcode-level aggregates from both tables. This aggregation-level table represents all of the features in the aggregation, pre-imputation. Its output location is generally {prefix}_aggregation Imputing Values \u00b6 A table that looks similar, but with imputed values is created. The cohort table from above is passed into collate as the comprehensive set of entities and dates for which output should be generated, regardless if they exist in the from_obj . Each feature column has an imputation rule, inherited from some level of the feature definition. The imputation rules that are based on data (e.g. mean ) use the rows from the as_of_time to produce the imputed value. Its output location is generally {prefix}_aggregation_imputed Recap \u00b6 At this point, we have at least three tables that are used to populate matrices: labels_{labelname}_{labelqueryhash} with computed labels for each date cohort_{cohortname}_{cohortqueryhash} with the cohort for each date A features.{prefix}_aggregation_imputed table for each feature aggregation present in the experiment config. 3. Building Matrices \u00b6 At this point, we have to build actual train and test matrices that can be processed by machine learning algorithms, save at the user's specified path, either on the local filesystem or s3 depending on the scheme portion of the path (e.g. s3://bucket-name/project_directory ) First we have to figure out exactly what matrices we have to build. The split definitions from step 1 are a good start -- they are our train and test splits -- but sometimes we also want to test different subsets of the data, like feature groups (e.g. 'how does using group of features A perform against using all features?'). So there's a layer of iteration we introduce for each split, that may produce many more matrices. What do we iterate over? Feature List - All subsets of features that the user wants to cycle through. This is the end result of the feature group generation and mixing process, which is described more below. Cohorts - In theory we can take in different cohorts and iterate in the same experiment. This is not fully implemented, so in reality we just use the one cohort that is passed in the cohort_config Label names - In theory we can take in different labels (e.g. complaints, sustained complaints) in the same experiment. Right now there is no support for multiple label names, but the label name used is configurable through the optional 'label_config'->'name' config value Label types - In theory we can take in different label types (e.g. binary) in the same experiment. Right now this isn't done, there is one label type and it is hardcoded as 'binary'. Feature Lists \u00b6 How do we arrive at the feature lists? There are two pieces of config that are used: feature group_definition and feature_group_strategies . Feature group definitions are just ways to define logical blocks of features, most often features that come from the same source, or describing a particular type of event. These groups within the experiment as a list of feature names, representing some subset of all potential features for the experiment. Feature group strategies are ways to take feature groups and mix them together in various ways. The feature group strategies take these subsets of features and convert them into another list of subsets of features, which is the final list iterated over to create different matrices. Feature Group Definition \u00b6 Feature groups, at present, can be defined as either a prefix (the prefix of the feature name), a table (the feature table that the feature resides in), or all (all features). Each argument is passed as a list, and each entry in the list is interpreted as a group. So, a feature group config of {'table': ['complaints_aggregate_imputed', 'incidents_aggregate_imputed']} would result in two feature groups: one with all the features in complaints_aggregate_imputed , and one with all the features in incidents_aggregate_imputed . Note that this requires a bit of knowledge on the user's part of how the feature table names will be constructed. prefix works on the prefix of the feature name as it exists in the database. So this also requires some knowledge of how these get created. The general format is: {aggregation_prefix}_{group}_{timeperiod}_{quantity} , so with some knowledge the user can create groups with the aggregation's configured prefix (common), or the aggregations configured prefix + group (in case they want to compare, for instance, zip-code level features versus entity level features). all , with a single value of True , will include a feature group with all defined features. If no feature group definition is sent, this is the default. Either way, at the end of this process the experiment will be aware of some list of feature groups, even if the list is just length 1 with all features as one group. Feature Group Mixing \u00b6 A few basic feature group mixing strategies are implemented: leave-one-in , leave-one-out , and all . These are sent in the experiment definition as a list, so different strategies can be tried in the same experiment. Each included strategy will be applied to the list of feature groups from the previous step, to convert them into For instance, 'leave-one-in' will cycle through each feature group, and for each one create a list of features that just represents that feature group, so for some matrices we would only use features from that particular group. leave-one-out does the opposite, for each feature group creating a list of features that includes all other feature groups but that one. all just creates a list of features that represents all feature groups together. Iteration and Matrix Creation \u00b6 At this point, matrices are created by looping through all train/test splits and data subsets (e.g. feature groups, state definitions), grabbing the data corresponding to each from the database, and assembling that data into a design matrix that is saved along with the metadata that defines it. As an example, if the experiment defines 3 train/test splits (one test per train in this example, for simplicity), 3 feature groups that are mixed using the 'leave-one-out' and 'all' strategies, and 1 state definition, we'll expect 18 matrices to be saved: 9 splits after multiplying the time splits by the feature groups, and each one creating a train and test matrix. Associating Matrices with Experiment \u00b6 After all matrices for the Experiment are defined but before any are built, the Experiment is associated with each Matrix in the database through the model_metadata.experiment_matrices table. This means that whether or not the Experiment has to end up building a matrix, after the fact a user can query the database to see if it used said matrix. Retrieving Data and Saving Completed Matrix \u00b6 Each matrix that has to be built (i.e. has not been built by some prior experiment) is built by retrieving its data out of the database. How do we get the data for an individual matrix out of the database? Create an entity-date table for this specific matrix. There is some logic applied to decide what rows show up. There are two possible sets of rows that could show up. all valid entity dates . These dates come from the entity-date-state table for the experiment (populated using the rules defined in the 'cohort_config'), filtered down to the entity-date pairs that match both the state filter and the list of as-of-dates for this matrix . all labeled entity dates . These dates consist of all the valid entity dates from above, that also have an entry in the labels table. If the matrix is a test matrix, all valid entity dates will be present. If the matrix is a train matrix, whether or not valid but unlabeled examples show up is decided by the include_missing_labels_in_train_as configuration value. If it is present in any form, these labels will be in the matrix. Otherwise, they will be filtered out. Write features data from tables to disk in CSV format using a COPY command, table by table. Each table is joined with the matrix-specific entity-date table to only include the desired rows. Write labels data to disk in CSV format using a COPY command. These labels will consist of the rows in the matrix-specific entity-date table left joined to the labels table. Rows not present in the labels table will have their label filled in (either True or False) based on the value of the include_missing_labels_in_train_as configuration key. Merge the features and labels CSV files horizontally, in pandas. They are expected to be of the same shape, which is enforced by the entity-date table. The resulting matrix is indexed on entity_id and as_of_date , and then saved to disk (in CSV format, more formats to come) along with its metadata: time, feature, label, index, and state information. along with any user metadata the experiment config specified. The filename is decided by a hash of this metadata, and the metadata is saved in a YAML file with the same hash and directory. The metadata is additionally added to a database table 'matrices'. Matrix metadata reference: - Train matrix temporal info - Test matrix temporal info - Feature, label, index, cohort, user metadata Recap \u00b6 At this point, all finished matrices and metadata will be saved under the project_path supplied by the user to the Experiment constructor, in the subdirectory matrices . 4. Running Models \u00b6 The last phase of an Experiment run uses the completed design matrices to train, test, and evaluate classifiers. This procedure writes a lot of metadata to the 3 schemas: 'model_metadata', 'train_results', and 'test_results'. Associating Models with Experiment \u00b6 Every combination of training matrix + classifier + hyperparameter is considered a Model. Before any Models are trained, the Experiment is associated with each Model in the database through the model_metadata.experiment_models table. This means that whether or not the Experiment has to end up training a model, after the fact a user can query the database to see if it used said model. Train \u00b6 Each matrix marked for training is sent through the configured grid in the experiment's grid_config . This works much like the scikit-learn ParameterGrid (and in fact uses it on the backend). It cycles through all of the classifiers and hyperparameter combinations contained herein, and calls .fit() with that train matrix. Any classifier that adheres to the scikit-learn .fit/.transform interface and is available in the Python environment will work here, whether it is a standard scikit-learn classifier, a third-party library like XGBoost, or a custom-built one in the calling repository (for instance, one that implements the problem domain's baseline heuristic algorithm for comparison). Metadata about the trained classifier is written to the model_metadata.models Postgres table. The trained model is saved to a filename with the model hash (see Model Hash section below). Model Groups \u00b6 Each model is assigned a 'model group'. A model group represents a number of trained classifiers that we want to treat as equivalent by some criteria. By default, this is aimed at defining models which are equivalent across time splits, to make analyzing model stability easier. This default is accomplished with a set of 'model group keys' that includes data about the classifier (module, hyperparameters), temporal intervals used to create the train matrix (label timespan, training history, as-of-date frequency), and metadata describing the data in the train matrix (features and feature groups, label name, cohort name). The user can override this set of model_group_keys in the experiment definition, with all of the default information plus other matrix metadata at their disposal (See end of 'Retrieving Data and Saving Completed Matrix' section for more about matrix metadata). This data is stored in the model_metadata.model_groups table, along with a model_group_id that is used as a foreign key in the model_metadata.models table. Model Hash \u00b6 Each trained model is assigned a hash, for the purpose of uniquely defining and caching the model. This hash is based on the training matrix metadata, classifier path, hyperparameters (except those which concern execution and do not affect results of the classifier, such as n_jobs ), and the given project path for the Experiment. This hash can be found in each row of the model_metadata.models table. It is enforced as a unique key in the table. Global Feature Importance \u00b6 The training phase also writes global feature importances to the database, in the train_results.feature_importances table. A few methods are queried to attempt to compute feature importances: The bulk of these are computed using the trained model's .feature_importances_ attribute, if it exists. For sklearn's SVC models with a linear kernel, the model's .coef_.squeeze() is used. For sklearn's LogisticRegression models, np.exp(model.coef_).squeeze() is used. Otherwise, no feature importances are written. Test Matrix \u00b6 For each test matrix, predictions, individual importances, and the user-specified testing evaluation metrics are written to the 'test_results' schema. For each train matrix, predictions and the user-specified training evaluation metrics are written to the 'train_results' schema. Predictions \u00b6 The trained model's prediction probabilities ( predict_proba() ) are computed both for the matrix it was trained on and any testing matrices. The predictions for the training matrix are saved in train_results.predictions and those for the testing matrices are saved in the test_results.predictions . More specifically, predict_proba returns the probabilities for each label (false and true), but in this case only the probabilities for the true label are saved in the {train or test}_predictions table. The entity_id and as_of_date are retrieved from the matrix's index, and stored in the database table along with the probability score, label value (if it has one), as well as other metadata. Individual Feature Importance \u00b6 Feature importances (of a configurable number of top features, defaulting to 5) for each prediction are computed and written to the test_results.individual_importances table. Right now, there are no sophisticated calculation methods integrated into the experiment; simply the top 5 global feature importances for the model are copied to the individual_importances table. Metrics \u00b6 Triage allows for the computation of both testing set and training set evaluation metrics. Evaluation metrics, such as precision and recall at various thresholds, are written to either the train_results.evaluations table or the test_results.evaluations . Triage defines a number of Evaluation Metrics metrics that can be addressed by name in the experiment definition, along with a list of thresholds and/or other parameters (such as the 'beta' value for fbeta) to iterate through. Thresholding is done either via absolute value (top k) or percentile by sorting the predictions and labels by the row's predicted probability score, with ties broken at random (the random seed can be passed in the config file to make this deterministic), and assigning the predicted value as True for those above the threshold. Note that the percentile thresholds are in terms of the population percentage, not a cutoff threshold for the predicted probability. Sometimes test matrices may not have labels for every row, so it's worth mentioning here how that is handled and interacts with thresholding. Rows with missing labels are not considered in the metric calculations, and if some of these rows are in the top k of the test matrix, no more rows are taken from the rest of the list for consideration. So if the experiment is calculating precision at the top 100 rows, and 40 of the top 100 rows are missing a label, the precision will actually be calculated on the 60 of the top 100 rows that do have a label. To make the results of this more transparent for users, a few extra pieces of metadata are written to the evaluations table for each metric score. num_labeled_examples - The number of rows in the test matrix that have labels num_labeled_above_threshold - The number of rows above the configured threshold for this metric score that have labels num_positive_labels - The number of positive labels in the test matrix Recap \u00b6 At this point, the 'model_metadata', 'train_results', and 'test_results' database schemas are fully populated with data about models, model groups, predictions, feature importances, and evaluation metrics for the researcher to query. In addition, the trained model pickle files are saved in the configured project path. The experiment is considered finished.","title":"Experiment Algorithm"},{"location":"experiments/algorithm/#experiment-algorithm-deep-dive","text":"This guide's purpose is to provide familiarity of the inner workings of a Triage Experiment to people with some experience in data science and Python. A Triage Experiment is a highly structured way of defining the experimentation phase of a data science project. To those wondering whether this Experiment structure is flexible enough to fit their needs, this should help.","title":"Experiment Algorithm Deep Dive"},{"location":"experiments/algorithm/#1-temporal-validation-setup","text":"First, the given temporal_config section in the experiment definition is transformed into train and test splits, including as_of_times for each matrix. We create these splits by figuring out the latest reasonable split time from the inputs, and moving backwards in time at the rate of the given model_update_frequency , until we get to the earliest reasonable split time. For each split, we create as_of_times by moving either backwards from the split time towards the max_training_history (for train matrices) or forwards from the split time towards the test_duration (for test matrices) at the provided data_frequency . Many of these configured values may be lists, in which case we generate the cross-product of all the possible values and generate more splits. For a more detailed look at the temporal validation logic, see Temporal Validation Deep Dive . The train and test splits themselves are not used until the Building Matrices section, but a flat list of all computed as_of_times for all matrices needed in the experiment is used in the next section, Transforming Data .","title":"1. Temporal Validation Setup"},{"location":"experiments/algorithm/#2-transforming-data","text":"With all of the as_of_times for this Experiment now computed, it's now possible to transform the input data into features and labels as of all the required times.","title":"2. Transforming Data"},{"location":"experiments/algorithm/#labels","text":"The Experiment populates a 'labels' table using the following input: 1. A query, provided by the user in the configuration file, that generates entity_ids and outcomes for a given as_of_date and label_timespan. Each as_of_date and label_timespan defined in temporal config For instance, an inspections-style query (for the given timespan, return the entity and outcome of any matching inspections) would look like: select events.entity_id, bool_or(outcome::bool)::integer as outcome from events where '{as_of_date}' <= outcome_date and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id This binary labels table is scoped to the entire Experiment, so all as_of_time (computed in step 1) and label_timespan (taken straight from temporal_config ) combinations are present. Additionally, the 'label_name' and 'label_type' are also recorded with each row in the table. The name of the labels table is based on both the name of the label and a hash of the label query (e.g labels_failedviolation_a0b1c2d3 ), so any prior experiments that shared both the name and query will be able to reuse the labels table. If the 'replace' flag was sent, for each as_of_time and label_timespan , the labels table is queried to check if any rows exist that match. If any such rows exist, the labels query for that date and timespan is not run. At this point, the 'labels' table may not have entries for all entities and dates that need to be in a given matrix. How these rows have their labels represented is up to the configured include_missing_labels_in_train_as value in the experiment. This value is not processed when we generate the labels table, but later on when the matrix is built (see 'Retrieving Data and Saving Completed Matrix')","title":"Labels"},{"location":"experiments/algorithm/#cohort-table","text":"The Experiment keeps track of the which entities are in the cohort on any given date. Similarly to the labels table, the experiment populates a cohort table using the following input: A query, provided by the user in the configuration file, that generates entity_ids for a given as_of_date. Each as_of_date as defined in temporal config This cohort table is scoped to the entire Experiment, so all as_of_times (computed in step 1) are present. The name of the cohort table is based on both the name of the cohort and a hash of the cohort query (e.g cohort_permitted_a0b1c2d3 ), so any prior experiments that shared both the name and query will be able to reuse the cohort table. If the 'replace' flag was sent, for each as_of_time , the cohort table is queried to check if any rows exist that match. If any such rows exist, the cohort query for that date is not run.","title":"Cohort Table"},{"location":"experiments/algorithm/#features","text":"Each provided feature_aggregation configures the creation and population of several feature tables in the 'features' schema: one for each of the groups specified in the config, one that merges the groups together into one table, and one that fills in null values from the merged table with imputed values based on imputation config.","title":"Features"},{"location":"experiments/algorithm/#generating-aggregation-sql","text":"To generate the SQL that creates the pre-imputation table, the Experiment assembles building blocks from the feature aggregation config, as well as the experiment's list of as_of_times : from_obj represents, well, the object of the FROM clause in the SQL query. Often this is just a table, but can be configured to be a subquery. This holds all the data that we want to aggregate into features Each as_of_time in the experiment and interval in the feature_aggregation is combined with the knowledge_date_column to create a WHERE clause representing a valid window of events to aggregate in the from_obj : e.g ( where {knowledge_date_column} >= {as_of_time} - interval {interval} ) Each aggregate , categorical , or array_categorical represents a SELECT clause. For aggregates, the quantity is a column or SQL expression representing a numeric quantity present in the from_obj , and the metrics are any number of aggregate functions we want to use. The aggregate function is applied to the quantity. Each group is a column applied to the GROUP BY clause. Generally this is 'entity_id', but higher-level groupings (for instance, 'zip_code') can be used as long as they can be rolled up to 'entity_id'. By default the query is joined with the cohort table to remove unnecessary rows. If features_ignore_cohort is passed to the Experiment this is not done. So a simplified version of a typical query would look like: SELECT {group}, {metric}({quantity}) FROM {from_obj} JOIN {cohort_table} ON ( {cohort_table.entity_id} = {from_obj.entity_id} AND {cohort_table.date} = {as_of_time} ) WHERE {knowledge_date_column} >= {as_of_time} - interval {interval} GROUP BY {group}","title":"Generating Aggregation SQL"},{"location":"experiments/algorithm/#writing-group-wide-feature-tables","text":"For each as_of_time , the results from the generated query are written to a table whose name is prefixed with the prefix , and suffixed with the group . For instance, if the configuration specifies zipcode-level aggregates and entity-level aggregates, there will be a table for each, keyed on its group plus the as_of_date.","title":"Writing Group-wide Feature Tables"},{"location":"experiments/algorithm/#merging-into-aggregation-wide-feature-tables","text":"Each generated group table is combined into one representing the whole aggregation with a left join. Given that the groups originally came from the same table (the from_obj of the aggregation) and therefore we know the zipcode for each entity, what we do now is create a table that would be keyed on entity and as_of_date, and contain all entity-level and zipcode-level aggregates from both tables. This aggregation-level table represents all of the features in the aggregation, pre-imputation. Its output location is generally {prefix}_aggregation","title":"Merging into Aggregation-wide Feature Tables"},{"location":"experiments/algorithm/#imputing-values","text":"A table that looks similar, but with imputed values is created. The cohort table from above is passed into collate as the comprehensive set of entities and dates for which output should be generated, regardless if they exist in the from_obj . Each feature column has an imputation rule, inherited from some level of the feature definition. The imputation rules that are based on data (e.g. mean ) use the rows from the as_of_time to produce the imputed value. Its output location is generally {prefix}_aggregation_imputed","title":"Imputing Values"},{"location":"experiments/algorithm/#recap","text":"At this point, we have at least three tables that are used to populate matrices: labels_{labelname}_{labelqueryhash} with computed labels for each date cohort_{cohortname}_{cohortqueryhash} with the cohort for each date A features.{prefix}_aggregation_imputed table for each feature aggregation present in the experiment config.","title":"Recap"},{"location":"experiments/algorithm/#3-building-matrices","text":"At this point, we have to build actual train and test matrices that can be processed by machine learning algorithms, save at the user's specified path, either on the local filesystem or s3 depending on the scheme portion of the path (e.g. s3://bucket-name/project_directory ) First we have to figure out exactly what matrices we have to build. The split definitions from step 1 are a good start -- they are our train and test splits -- but sometimes we also want to test different subsets of the data, like feature groups (e.g. 'how does using group of features A perform against using all features?'). So there's a layer of iteration we introduce for each split, that may produce many more matrices. What do we iterate over? Feature List - All subsets of features that the user wants to cycle through. This is the end result of the feature group generation and mixing process, which is described more below. Cohorts - In theory we can take in different cohorts and iterate in the same experiment. This is not fully implemented, so in reality we just use the one cohort that is passed in the cohort_config Label names - In theory we can take in different labels (e.g. complaints, sustained complaints) in the same experiment. Right now there is no support for multiple label names, but the label name used is configurable through the optional 'label_config'->'name' config value Label types - In theory we can take in different label types (e.g. binary) in the same experiment. Right now this isn't done, there is one label type and it is hardcoded as 'binary'.","title":"3. Building Matrices"},{"location":"experiments/algorithm/#feature-lists","text":"How do we arrive at the feature lists? There are two pieces of config that are used: feature group_definition and feature_group_strategies . Feature group definitions are just ways to define logical blocks of features, most often features that come from the same source, or describing a particular type of event. These groups within the experiment as a list of feature names, representing some subset of all potential features for the experiment. Feature group strategies are ways to take feature groups and mix them together in various ways. The feature group strategies take these subsets of features and convert them into another list of subsets of features, which is the final list iterated over to create different matrices.","title":"Feature Lists"},{"location":"experiments/algorithm/#feature-group-definition","text":"Feature groups, at present, can be defined as either a prefix (the prefix of the feature name), a table (the feature table that the feature resides in), or all (all features). Each argument is passed as a list, and each entry in the list is interpreted as a group. So, a feature group config of {'table': ['complaints_aggregate_imputed', 'incidents_aggregate_imputed']} would result in two feature groups: one with all the features in complaints_aggregate_imputed , and one with all the features in incidents_aggregate_imputed . Note that this requires a bit of knowledge on the user's part of how the feature table names will be constructed. prefix works on the prefix of the feature name as it exists in the database. So this also requires some knowledge of how these get created. The general format is: {aggregation_prefix}_{group}_{timeperiod}_{quantity} , so with some knowledge the user can create groups with the aggregation's configured prefix (common), or the aggregations configured prefix + group (in case they want to compare, for instance, zip-code level features versus entity level features). all , with a single value of True , will include a feature group with all defined features. If no feature group definition is sent, this is the default. Either way, at the end of this process the experiment will be aware of some list of feature groups, even if the list is just length 1 with all features as one group.","title":"Feature Group Definition"},{"location":"experiments/algorithm/#feature-group-mixing","text":"A few basic feature group mixing strategies are implemented: leave-one-in , leave-one-out , and all . These are sent in the experiment definition as a list, so different strategies can be tried in the same experiment. Each included strategy will be applied to the list of feature groups from the previous step, to convert them into For instance, 'leave-one-in' will cycle through each feature group, and for each one create a list of features that just represents that feature group, so for some matrices we would only use features from that particular group. leave-one-out does the opposite, for each feature group creating a list of features that includes all other feature groups but that one. all just creates a list of features that represents all feature groups together.","title":"Feature Group Mixing"},{"location":"experiments/algorithm/#iteration-and-matrix-creation","text":"At this point, matrices are created by looping through all train/test splits and data subsets (e.g. feature groups, state definitions), grabbing the data corresponding to each from the database, and assembling that data into a design matrix that is saved along with the metadata that defines it. As an example, if the experiment defines 3 train/test splits (one test per train in this example, for simplicity), 3 feature groups that are mixed using the 'leave-one-out' and 'all' strategies, and 1 state definition, we'll expect 18 matrices to be saved: 9 splits after multiplying the time splits by the feature groups, and each one creating a train and test matrix.","title":"Iteration and Matrix Creation"},{"location":"experiments/algorithm/#associating-matrices-with-experiment","text":"After all matrices for the Experiment are defined but before any are built, the Experiment is associated with each Matrix in the database through the model_metadata.experiment_matrices table. This means that whether or not the Experiment has to end up building a matrix, after the fact a user can query the database to see if it used said matrix.","title":"Associating Matrices with Experiment"},{"location":"experiments/algorithm/#retrieving-data-and-saving-completed-matrix","text":"Each matrix that has to be built (i.e. has not been built by some prior experiment) is built by retrieving its data out of the database. How do we get the data for an individual matrix out of the database? Create an entity-date table for this specific matrix. There is some logic applied to decide what rows show up. There are two possible sets of rows that could show up. all valid entity dates . These dates come from the entity-date-state table for the experiment (populated using the rules defined in the 'cohort_config'), filtered down to the entity-date pairs that match both the state filter and the list of as-of-dates for this matrix . all labeled entity dates . These dates consist of all the valid entity dates from above, that also have an entry in the labels table. If the matrix is a test matrix, all valid entity dates will be present. If the matrix is a train matrix, whether or not valid but unlabeled examples show up is decided by the include_missing_labels_in_train_as configuration value. If it is present in any form, these labels will be in the matrix. Otherwise, they will be filtered out. Write features data from tables to disk in CSV format using a COPY command, table by table. Each table is joined with the matrix-specific entity-date table to only include the desired rows. Write labels data to disk in CSV format using a COPY command. These labels will consist of the rows in the matrix-specific entity-date table left joined to the labels table. Rows not present in the labels table will have their label filled in (either True or False) based on the value of the include_missing_labels_in_train_as configuration key. Merge the features and labels CSV files horizontally, in pandas. They are expected to be of the same shape, which is enforced by the entity-date table. The resulting matrix is indexed on entity_id and as_of_date , and then saved to disk (in CSV format, more formats to come) along with its metadata: time, feature, label, index, and state information. along with any user metadata the experiment config specified. The filename is decided by a hash of this metadata, and the metadata is saved in a YAML file with the same hash and directory. The metadata is additionally added to a database table 'matrices'. Matrix metadata reference: - Train matrix temporal info - Test matrix temporal info - Feature, label, index, cohort, user metadata","title":"Retrieving Data and Saving Completed Matrix"},{"location":"experiments/algorithm/#recap_1","text":"At this point, all finished matrices and metadata will be saved under the project_path supplied by the user to the Experiment constructor, in the subdirectory matrices .","title":"Recap"},{"location":"experiments/algorithm/#4-running-models","text":"The last phase of an Experiment run uses the completed design matrices to train, test, and evaluate classifiers. This procedure writes a lot of metadata to the 3 schemas: 'model_metadata', 'train_results', and 'test_results'.","title":"4. Running Models"},{"location":"experiments/algorithm/#associating-models-with-experiment","text":"Every combination of training matrix + classifier + hyperparameter is considered a Model. Before any Models are trained, the Experiment is associated with each Model in the database through the model_metadata.experiment_models table. This means that whether or not the Experiment has to end up training a model, after the fact a user can query the database to see if it used said model.","title":"Associating Models with Experiment"},{"location":"experiments/algorithm/#train","text":"Each matrix marked for training is sent through the configured grid in the experiment's grid_config . This works much like the scikit-learn ParameterGrid (and in fact uses it on the backend). It cycles through all of the classifiers and hyperparameter combinations contained herein, and calls .fit() with that train matrix. Any classifier that adheres to the scikit-learn .fit/.transform interface and is available in the Python environment will work here, whether it is a standard scikit-learn classifier, a third-party library like XGBoost, or a custom-built one in the calling repository (for instance, one that implements the problem domain's baseline heuristic algorithm for comparison). Metadata about the trained classifier is written to the model_metadata.models Postgres table. The trained model is saved to a filename with the model hash (see Model Hash section below).","title":"Train"},{"location":"experiments/algorithm/#model-groups","text":"Each model is assigned a 'model group'. A model group represents a number of trained classifiers that we want to treat as equivalent by some criteria. By default, this is aimed at defining models which are equivalent across time splits, to make analyzing model stability easier. This default is accomplished with a set of 'model group keys' that includes data about the classifier (module, hyperparameters), temporal intervals used to create the train matrix (label timespan, training history, as-of-date frequency), and metadata describing the data in the train matrix (features and feature groups, label name, cohort name). The user can override this set of model_group_keys in the experiment definition, with all of the default information plus other matrix metadata at their disposal (See end of 'Retrieving Data and Saving Completed Matrix' section for more about matrix metadata). This data is stored in the model_metadata.model_groups table, along with a model_group_id that is used as a foreign key in the model_metadata.models table.","title":"Model Groups"},{"location":"experiments/algorithm/#model-hash","text":"Each trained model is assigned a hash, for the purpose of uniquely defining and caching the model. This hash is based on the training matrix metadata, classifier path, hyperparameters (except those which concern execution and do not affect results of the classifier, such as n_jobs ), and the given project path for the Experiment. This hash can be found in each row of the model_metadata.models table. It is enforced as a unique key in the table.","title":"Model Hash"},{"location":"experiments/algorithm/#global-feature-importance","text":"The training phase also writes global feature importances to the database, in the train_results.feature_importances table. A few methods are queried to attempt to compute feature importances: The bulk of these are computed using the trained model's .feature_importances_ attribute, if it exists. For sklearn's SVC models with a linear kernel, the model's .coef_.squeeze() is used. For sklearn's LogisticRegression models, np.exp(model.coef_).squeeze() is used. Otherwise, no feature importances are written.","title":"Global Feature Importance"},{"location":"experiments/algorithm/#test-matrix","text":"For each test matrix, predictions, individual importances, and the user-specified testing evaluation metrics are written to the 'test_results' schema. For each train matrix, predictions and the user-specified training evaluation metrics are written to the 'train_results' schema.","title":"Test Matrix"},{"location":"experiments/algorithm/#predictions","text":"The trained model's prediction probabilities ( predict_proba() ) are computed both for the matrix it was trained on and any testing matrices. The predictions for the training matrix are saved in train_results.predictions and those for the testing matrices are saved in the test_results.predictions . More specifically, predict_proba returns the probabilities for each label (false and true), but in this case only the probabilities for the true label are saved in the {train or test}_predictions table. The entity_id and as_of_date are retrieved from the matrix's index, and stored in the database table along with the probability score, label value (if it has one), as well as other metadata.","title":"Predictions"},{"location":"experiments/algorithm/#individual-feature-importance","text":"Feature importances (of a configurable number of top features, defaulting to 5) for each prediction are computed and written to the test_results.individual_importances table. Right now, there are no sophisticated calculation methods integrated into the experiment; simply the top 5 global feature importances for the model are copied to the individual_importances table.","title":"Individual Feature Importance"},{"location":"experiments/algorithm/#metrics","text":"Triage allows for the computation of both testing set and training set evaluation metrics. Evaluation metrics, such as precision and recall at various thresholds, are written to either the train_results.evaluations table or the test_results.evaluations . Triage defines a number of Evaluation Metrics metrics that can be addressed by name in the experiment definition, along with a list of thresholds and/or other parameters (such as the 'beta' value for fbeta) to iterate through. Thresholding is done either via absolute value (top k) or percentile by sorting the predictions and labels by the row's predicted probability score, with ties broken at random (the random seed can be passed in the config file to make this deterministic), and assigning the predicted value as True for those above the threshold. Note that the percentile thresholds are in terms of the population percentage, not a cutoff threshold for the predicted probability. Sometimes test matrices may not have labels for every row, so it's worth mentioning here how that is handled and interacts with thresholding. Rows with missing labels are not considered in the metric calculations, and if some of these rows are in the top k of the test matrix, no more rows are taken from the rest of the list for consideration. So if the experiment is calculating precision at the top 100 rows, and 40 of the top 100 rows are missing a label, the precision will actually be calculated on the 60 of the top 100 rows that do have a label. To make the results of this more transparent for users, a few extra pieces of metadata are written to the evaluations table for each metric score. num_labeled_examples - The number of rows in the test matrix that have labels num_labeled_above_threshold - The number of rows above the configured threshold for this metric score that have labels num_positive_labels - The number of positive labels in the test matrix","title":"Metrics"},{"location":"experiments/algorithm/#recap_2","text":"At this point, the 'model_metadata', 'train_results', and 'test_results' database schemas are fully populated with data about models, model groups, predictions, feature importances, and evaluation metrics for the researcher to query. In addition, the trained model pickle files are saved in the configured project path. The experiment is considered finished.","title":"Recap"},{"location":"experiments/architecture/","text":"Experiment Architecture \u00b6 mermaid.initialize({startOnLoad:true}); This document is aimed at people wishing to contribute to Triage development. It explains the design and architecture of the Experiment class. Dependency Graphs \u00b6 For a general overview of how the parts of an experiment depend on each other, refer to the graphs below. Experiment (high-level) \u00b6 graph TD TC[Timechop] subgraph Architect LG[Label Generator] CG[Cohort Generator] FG[\"Feature Generator (+ feature groups)\"] MB[Matrix Builder] end subgraph Catwalk, per-model MT[Model Trainer] PR[Predictor] EV[Model Evaluator] end TC --> LG TC --> CG TC --> FG LG --> MB CG --> MB FG --> MB MB --> MT MB --> PR MT --> PR PR --> EV The FeatureGenerator section above hides some details to make the overall graph flow more concise. To support feature grouping, there are more operations that happen between feature table creation and matrix building. The relevant section of the dependency graph is expanded below, along with the output that each pair of components sends between each other within the arrow Feature Dependency Details \u00b6 graph TD TC[Timechop] FG[Feature Generator] FDG[Feature Dictionary Generator] FGC[Feature Group Creator] FGM[Feature Group Mixer] PL[Planner] MB[Matrix Builder] TC -- as-of-dates --> FG FG -- feature tables --> FDG FDG -- master feature dictionary --> FGC FGC -- feature groups --> FGM FGM -- recombined feature groups --> PL TC -- time splits --> PL FG -- feature tables --> MB PL -- matrix build tasks --> MB Component List and Input/Output \u00b6 These are where the interesting data science work is done. Timechop (temporal cross-validation) Architect (design matrix creation) Cohort Table Generator Label Generator Feature Generator Feature Dictionary Creator Feature Group Creator Feature Group Mixer Planner Matrix Builder Catwalk (modeling) Model Train/Tester Model Grouper Model Trainer Predictor Model Evaluator Individual Importance Calculator Timechop \u00b6 Timechop does the necessary temporal math to set up temporal cross-validation. It 'chops' time according to config into train-test split definitions, which other components use. Input temporal_config in experiment config Output Time splits containing temporal cross-validation definition, including each as_of_date to be included in the matrices in each time split Cohort Table Generator \u00b6 The CohortTableGenerator manages a cohort table by running the configured cohort query for a number of different as_of_dates . Input All unique as_of_dates needed by matrices in the experiment, as provided by Timechop query and name from cohort_config in experiment config cohort table name that the caller wants to use Output A cohort table in the database, consisting of entity ids and dates Label Generator \u00b6 The LabelGenerator manages a labels table by running the configured label query for a number of different as_of_dates and label_timespans . Input All unique as_of_dates and label_timespans , needed by matrices in the experiment, as provided by Timechop query and name from label_config in experiment config Output A labels table in the database, consisting of entity ids, dates, and boolean labels Feature Generator \u00b6 The FeatureGenerator manages a number of features tables by converting the configured feature_aggregations into collate.Spacetime objects, and then running the queries generated by collate . For each feature_aggregation , it runs a few passes: Optionally, convert a complex from object (e.g. the FROM part of the configured aggregation query) into an indexed table for speed. Create a number of empty tables at different GROUP BY levels (e.g. entity_id , zip_code ). and run inserts individually for each as_of_date . These inserts are split up into individual tasks and parallelized for speed. Roll up the GROUP BY tables from step 1 to the entity_id level with a single LEFT JOIN query. Use the cohort table to find all members of the cohort not present in the table from step 2 and create a new table with all members of the cohort, null values filled in with values based on the rules in the feature_aggregations config. Input All unique as_of_dates needed by matrices in the experiment, and the start time for features, as provided by Timechop The populated cohort table, as provided by Cohort Table Generator feature_aggregations in experiment config Output Populated feature tables in the database, one for each feature_aggregation Feature Dictionary Creator \u00b6 Summarizes the feature tables created by FeatureGenerator into a dictionary more easily usable for feature grouping and serialization purposes. Does this by querying the database's information_schema . Input Names of feature tables and the index of each table, as provided by Feature Generator Output A master feature dictionary, consisting of each populated feature table and all of its feature column names. Feature Group Creator \u00b6 Creates feature groups by taking the configured feature grouping rules and applying them to the master feature dictionary, to create a collection of smaller feature dictionaries. Input Master feature dictionary, as provided by Feature Dictionary Creator feature_group_definition in experiment config Output List of feature dictionaries, each representing one feature group Feature Group Mixer \u00b6 Combines feature groups into new ones based on the configured rules (e.g. leave-one-out , leave-one-in ). Input List of feature dictionaries, as provided by Feature Group Creator feature_group_strategies in experiment config Output List of feature dictionaries, each representing one or more feature groups. Planner \u00b6 Mixes time split definitions and feature groups to create the master list of matrices that are required for modeling to proceed. Input List of feature dictionaries, as provided by Feature Group Mixer List of matrix split definitions, as provided by Timechop user_metadata , in experiment config feature_start_time from temporal_config in experiment config cohort name from cohort_config in experiment config label name from cohort_config in experiment config Output List of serializable matrix build tasks, consisting of everything needed to build a single matrix: list of as-of-dates a label name a label type a feature dictionary matrix uuid matrix metadata matrix type (train or test) Matrix Builder \u00b6 Takes matrix build tasks from the Planner and builds them if they don't already exist. Input A matrix build task, as provided by Planner include_missing_labels_in_train_as from label_config in experiment config The experiment's MatrixStorageEngine Output The built matrix saved in the MatrixStorageEngine A row describing the matrix saved in the database's model_metadata.matrices table. ModelTrainTester \u00b6 A meta-component of sorts. Encompasses all of the other catwalk components. Input One temporal split, as provided by Timechop grid_config in experiment config Fully configured ModelTrainer , Predictor , ModelEvaluator , Individual Importance Calculator objects Output All of its components are run, resulting in trained models, predictions, evaluation metrics, and individual importances ModelGrouper \u00b6 Assigns a model group to each model based on its metadata. Input model_group_keys in experiment config All the data about a particular model neded to decide a model group for the model: classifier name, hyperparameter list, and matrix metadata, as provided by ModelTrainer Output a model group id corresponding to a row in the model_metadata.model_groups table, either a matching one that already existed in the table or one that it autoprovisioned. ModelTrainer \u00b6 Trains a model, stores it, and saves its metadata (including model group information and feature importances) to the database. Each model to be trained is expressed as a serializable task so that it can be parallelized. Input an instance of the ModelGrouper class. the experiment's ModelStorageEngine a MatrixStore object an importable classifier path and a set of hyperparameters Output - a row in the database's model_metadata.model_groups table, the model_metadata.models table, and rows in train_results.feature_importances for each feature. - the trained model persisted in the ModelStorageEngine Predictor \u00b6 Generates predictions for a given model and matrix, both returning them for immediate use and saving them to the database. Input The experiment's Model Storage Engine A model id corresponding to a row from the database A MatrixStore object Output The predictions as an array Each prediction saved to the database, unless configured not to. The table they are stored in depends on which type of matrix it is (e.g. test_results.predictions or train_results.predictions ) ModelEvaluator \u00b6 Generates evaluation metrics for a given model and matrix. Input scoring in experiment config array of predictions the MatrixStore and model_id that the predictions were generated from Output A row in the database for each evaluation metric. The table they are stored in depends on which type of matrix it is (e.g. test_results.evaluations or train_results.evaluations ). Individual Importance Calculator \u00b6 Generates the top n feature importances for each entity in a given model. Input individual_importance_config in experiment config. model id a MatrixStore object for a test matrix an as-of-date Output rows in the test_results.individual_importances table for the model, date, and matrix based on the configured method and number of top features per entity. General Class Design \u00b6 The Experiment class is designed to have all work done by component objects that reside as attributes on the instance. The purpose of this is to maximize the reuse potential of the components outside of the Experiment, as well as avoid excessive class inheritance within the Experiment. The inheritance tree of the Experiment is reserved for execution concerns , such as switching between singlethreaded, multiprocess, or cluster execution. To enable these different execution contexts without excessive duplicated code, the components that cover computationally or memory-intensive work generally implement methods to generate a collection of serializable tasks to perform later, on either that same object or perhaps another one running in another process or machine. The subclasses of Experiment then differentiate themselves by implementing methods to execute a collection of these tasks using their preferred method of execution, whether it be a simple loop, a process pool, or a cluster. The components are created and experiment configuration is bound to them at Experiment construction time, so that the instance methods can have concise call signatures that only cover the information passed by other components mid-experiment. Data reuse/replacement is handled within components. The Experiment generally just hands the replace flag to each component at object construction, and at runtime each component uses that and determines whether or not the needed work has already been done. I'm trying to find some behavior. Where does it reside? \u00b6 If you're looking to change behavior of the Experiment, When possible, the logic resides in one of the components and hopefully the component list above should be helpful at finding the lines between components. Logic that specifically relates to parallel execution is in one of the experiment subclasses (see parallelization section below). Everything else is in the Experiment base class . This is where the public interface ( .run() ) resides, and follows a template method pattern to define the skeleton of the Experiment: instantating components based on experiment configuration and runtime inputs, and passing output from one component to another. I want to add a new option. Where should I put it? \u00b6 Generally, the experiment configuration is where any new options go that change any data science-related functionality; in other words, if you could conceivably get better precision from the change, it should make it into experiment configuration. This is so the hashed experiment config is meaningful and the experiment can be audited by looking at the experiment configuration rather than requiring the perusal of custom code. The blind spot in this is, of course, the state of the database, which can always change results, but it's useful for database state to continue to be the only exception to this rule. On the other hand, new options that affect only runtime concerns (e.g. performance boosts) should go as arguments to the Experiment. For instance, changing the number of cores to use for matrix building, or telling it to skip predictions won't change the answer you're looking for; options like these just help you potentially get to the answer faster. Once an experiment is completed, runtime flags like these should be totally safe to ignore in analysis. Storage Abstractions \u00b6 Another important part of enabling different execution contexts is being able to pass large, persisted objects (e.g. matrices or models) by reference to another process or cluster. To achieve this, as well as provide the ability to configure different storage mediums (e.g. S3) and formats (e,g, HDF) without changes to the Experiment class, all references to these large objects within any components are handled through an abstraction layer. Matrix Storage \u00b6 All interactions with individual matrices and their bundled metadata are handled through MatrixStore objects. The storage medium is handled through a base Store object that is an attribute of the MatrixStore . The storage format is handled through inheritance on the MatrixStore : Each subclass, such as CSVMatrixStore or HDFMatrixStore , implements the necessary methods ( save , load , head_of_matrix ) to properly persist or load a matrix from its storage. In addition, the MatrixStore provides a variety of methods to retrieve data from either the base matrix itself or its metadata. For instance (this is not meant to be a complete list): matrix - the raw matrix metadata - the raw metadata dictionary exists - whether or not it exists in storage columns - the column list labels - the label column uuid - the matrix's UUID as_of_dates - the matrix's list of as-of-dates One MatrixStorageEngine exists at the Experiment level, and roughly corresponds with a directory wherever matrices are stored. Its only interface is to provide a MatrixStore object given a matrix UUID. Model Storage \u00b6 Model storage is handled similarly to matrix storage, although the interactions with it are far simpler so there is no single-model class akin to the MatrixStore . One ModelStorageEngine exists at the Experiment level, configured with the Experiment's storage medium, and through it trained models can be saved or loaded. The ModelStorageEngine uses joblib to save and load compressed pickles of the model. Miscellaneous Project Storage \u00b6 Both the ModelStorageEngine and MatrixStorageEngine are based on a more general storage abstraction that is suitable for any other auxiliary objects (e.g. graph images) that need to be stored. That is the ProjectStorage object, which roughly corresponds to a directory on some storage medium where we store everything. One of these exists as an Experiment attribute, and its interface .get_store can be used to persist or load whatever is needed. Parallelization/Subclassing Details \u00b6 In the Class Design section above, we introduced tasks for parallelization and subclassing for execution changes. In this section, we expand on these to help provide a new guide to working with these. Currently there are three methods that must be implemented by subclasses of Experiment in order to be fully functional. Abstract Methods \u00b6 process_query_tasks - Run feature generation queries. Receives a list of tasks. each task actually represents a table and is split into three lists of queries to enable the implementation to avoid deadlocks: prepare (table creation), inserts (a collection of INSERT INTO SELECT queries), and finalize (indexing). prepare needs to be run before the inserts and finalize is best run after the inserts, so it is advised that only the inserts are parallelized. The subclass should run each individual batch of queries by calling self.feature_generator.run_commands([list of queries]) , which will run all of the queries serially, so the implementation can send a batch of queries to each worker instead of having each individual query be on a new worker. process_matrix_build_tasks - Run matrix build tasks (that assume all the necessary label/cohort/feature tables have been built). Receives a dictionary of tasks. Each key is a matrix UUID, and each value is a dictionary that has all the necessary keyword arguments to call self.matrix_builder.build_matrix to build one matrix. process_train_test_tasks - Run model train/test tasks (that assume all matrices are built). Receives a list of tasks, each value is a dictionary that has all the necessary keyword arguments to call self.model_train_tester.process_task to train and test one model. Each task covers model training, prediction (on both test and train matrices), model evaluation (on both test and train matrices), and saving of global and individual feature importances. Reference Implementations \u00b6 SingleThreadedExperiment is a barebones implementation that runs everything serially. MultiCoreExperiment utilizes local multiprocessing to run tasks through a worker pool. Reading this is helpful to see the minimal implementation needed for some parallelization. RQExperiment - utilizes an RQ worker cluster to allow the tasks to be parallelized either locally or distributed to other. Does not take care of spawning a cluster or any other infrastructural concerns: it expects that the cluster is running somewhere and is reading from the same Redis instance that is passed to the RQExperiment . The RQExperiment simply enqueues tasks and waits for them to be completed. Reading this is helpful as a simple example of how to enable distributed computing.","title":"Experiment Architecture"},{"location":"experiments/architecture/#experiment-architecture","text":"mermaid.initialize({startOnLoad:true}); This document is aimed at people wishing to contribute to Triage development. It explains the design and architecture of the Experiment class.","title":"Experiment Architecture"},{"location":"experiments/architecture/#dependency-graphs","text":"For a general overview of how the parts of an experiment depend on each other, refer to the graphs below.","title":"Dependency Graphs"},{"location":"experiments/architecture/#experiment-high-level","text":"graph TD TC[Timechop] subgraph Architect LG[Label Generator] CG[Cohort Generator] FG[\"Feature Generator (+ feature groups)\"] MB[Matrix Builder] end subgraph Catwalk, per-model MT[Model Trainer] PR[Predictor] EV[Model Evaluator] end TC --> LG TC --> CG TC --> FG LG --> MB CG --> MB FG --> MB MB --> MT MB --> PR MT --> PR PR --> EV The FeatureGenerator section above hides some details to make the overall graph flow more concise. To support feature grouping, there are more operations that happen between feature table creation and matrix building. The relevant section of the dependency graph is expanded below, along with the output that each pair of components sends between each other within the arrow","title":"Experiment (high-level)"},{"location":"experiments/architecture/#feature-dependency-details","text":"graph TD TC[Timechop] FG[Feature Generator] FDG[Feature Dictionary Generator] FGC[Feature Group Creator] FGM[Feature Group Mixer] PL[Planner] MB[Matrix Builder] TC -- as-of-dates --> FG FG -- feature tables --> FDG FDG -- master feature dictionary --> FGC FGC -- feature groups --> FGM FGM -- recombined feature groups --> PL TC -- time splits --> PL FG -- feature tables --> MB PL -- matrix build tasks --> MB","title":"Feature Dependency Details"},{"location":"experiments/architecture/#component-list-and-inputoutput","text":"These are where the interesting data science work is done. Timechop (temporal cross-validation) Architect (design matrix creation) Cohort Table Generator Label Generator Feature Generator Feature Dictionary Creator Feature Group Creator Feature Group Mixer Planner Matrix Builder Catwalk (modeling) Model Train/Tester Model Grouper Model Trainer Predictor Model Evaluator Individual Importance Calculator","title":"Component List and Input/Output"},{"location":"experiments/architecture/#timechop","text":"Timechop does the necessary temporal math to set up temporal cross-validation. It 'chops' time according to config into train-test split definitions, which other components use. Input temporal_config in experiment config Output Time splits containing temporal cross-validation definition, including each as_of_date to be included in the matrices in each time split","title":"Timechop"},{"location":"experiments/architecture/#cohort-table-generator","text":"The CohortTableGenerator manages a cohort table by running the configured cohort query for a number of different as_of_dates . Input All unique as_of_dates needed by matrices in the experiment, as provided by Timechop query and name from cohort_config in experiment config cohort table name that the caller wants to use Output A cohort table in the database, consisting of entity ids and dates","title":"Cohort Table Generator"},{"location":"experiments/architecture/#label-generator","text":"The LabelGenerator manages a labels table by running the configured label query for a number of different as_of_dates and label_timespans . Input All unique as_of_dates and label_timespans , needed by matrices in the experiment, as provided by Timechop query and name from label_config in experiment config Output A labels table in the database, consisting of entity ids, dates, and boolean labels","title":"Label Generator"},{"location":"experiments/architecture/#feature-generator","text":"The FeatureGenerator manages a number of features tables by converting the configured feature_aggregations into collate.Spacetime objects, and then running the queries generated by collate . For each feature_aggregation , it runs a few passes: Optionally, convert a complex from object (e.g. the FROM part of the configured aggregation query) into an indexed table for speed. Create a number of empty tables at different GROUP BY levels (e.g. entity_id , zip_code ). and run inserts individually for each as_of_date . These inserts are split up into individual tasks and parallelized for speed. Roll up the GROUP BY tables from step 1 to the entity_id level with a single LEFT JOIN query. Use the cohort table to find all members of the cohort not present in the table from step 2 and create a new table with all members of the cohort, null values filled in with values based on the rules in the feature_aggregations config. Input All unique as_of_dates needed by matrices in the experiment, and the start time for features, as provided by Timechop The populated cohort table, as provided by Cohort Table Generator feature_aggregations in experiment config Output Populated feature tables in the database, one for each feature_aggregation","title":"Feature Generator"},{"location":"experiments/architecture/#feature-dictionary-creator","text":"Summarizes the feature tables created by FeatureGenerator into a dictionary more easily usable for feature grouping and serialization purposes. Does this by querying the database's information_schema . Input Names of feature tables and the index of each table, as provided by Feature Generator Output A master feature dictionary, consisting of each populated feature table and all of its feature column names.","title":"Feature Dictionary Creator"},{"location":"experiments/architecture/#feature-group-creator","text":"Creates feature groups by taking the configured feature grouping rules and applying them to the master feature dictionary, to create a collection of smaller feature dictionaries. Input Master feature dictionary, as provided by Feature Dictionary Creator feature_group_definition in experiment config Output List of feature dictionaries, each representing one feature group","title":"Feature Group Creator"},{"location":"experiments/architecture/#feature-group-mixer","text":"Combines feature groups into new ones based on the configured rules (e.g. leave-one-out , leave-one-in ). Input List of feature dictionaries, as provided by Feature Group Creator feature_group_strategies in experiment config Output List of feature dictionaries, each representing one or more feature groups.","title":"Feature Group Mixer"},{"location":"experiments/architecture/#planner","text":"Mixes time split definitions and feature groups to create the master list of matrices that are required for modeling to proceed. Input List of feature dictionaries, as provided by Feature Group Mixer List of matrix split definitions, as provided by Timechop user_metadata , in experiment config feature_start_time from temporal_config in experiment config cohort name from cohort_config in experiment config label name from cohort_config in experiment config Output List of serializable matrix build tasks, consisting of everything needed to build a single matrix: list of as-of-dates a label name a label type a feature dictionary matrix uuid matrix metadata matrix type (train or test)","title":"Planner"},{"location":"experiments/architecture/#matrix-builder","text":"Takes matrix build tasks from the Planner and builds them if they don't already exist. Input A matrix build task, as provided by Planner include_missing_labels_in_train_as from label_config in experiment config The experiment's MatrixStorageEngine Output The built matrix saved in the MatrixStorageEngine A row describing the matrix saved in the database's model_metadata.matrices table.","title":"Matrix Builder"},{"location":"experiments/architecture/#modeltraintester","text":"A meta-component of sorts. Encompasses all of the other catwalk components. Input One temporal split, as provided by Timechop grid_config in experiment config Fully configured ModelTrainer , Predictor , ModelEvaluator , Individual Importance Calculator objects Output All of its components are run, resulting in trained models, predictions, evaluation metrics, and individual importances","title":"ModelTrainTester"},{"location":"experiments/architecture/#modelgrouper","text":"Assigns a model group to each model based on its metadata. Input model_group_keys in experiment config All the data about a particular model neded to decide a model group for the model: classifier name, hyperparameter list, and matrix metadata, as provided by ModelTrainer Output a model group id corresponding to a row in the model_metadata.model_groups table, either a matching one that already existed in the table or one that it autoprovisioned.","title":"ModelGrouper"},{"location":"experiments/architecture/#modeltrainer","text":"Trains a model, stores it, and saves its metadata (including model group information and feature importances) to the database. Each model to be trained is expressed as a serializable task so that it can be parallelized. Input an instance of the ModelGrouper class. the experiment's ModelStorageEngine a MatrixStore object an importable classifier path and a set of hyperparameters Output - a row in the database's model_metadata.model_groups table, the model_metadata.models table, and rows in train_results.feature_importances for each feature. - the trained model persisted in the ModelStorageEngine","title":"ModelTrainer"},{"location":"experiments/architecture/#predictor","text":"Generates predictions for a given model and matrix, both returning them for immediate use and saving them to the database. Input The experiment's Model Storage Engine A model id corresponding to a row from the database A MatrixStore object Output The predictions as an array Each prediction saved to the database, unless configured not to. The table they are stored in depends on which type of matrix it is (e.g. test_results.predictions or train_results.predictions )","title":"Predictor"},{"location":"experiments/architecture/#modelevaluator","text":"Generates evaluation metrics for a given model and matrix. Input scoring in experiment config array of predictions the MatrixStore and model_id that the predictions were generated from Output A row in the database for each evaluation metric. The table they are stored in depends on which type of matrix it is (e.g. test_results.evaluations or train_results.evaluations ).","title":"ModelEvaluator"},{"location":"experiments/architecture/#individual-importance-calculator","text":"Generates the top n feature importances for each entity in a given model. Input individual_importance_config in experiment config. model id a MatrixStore object for a test matrix an as-of-date Output rows in the test_results.individual_importances table for the model, date, and matrix based on the configured method and number of top features per entity.","title":"Individual Importance Calculator"},{"location":"experiments/architecture/#general-class-design","text":"The Experiment class is designed to have all work done by component objects that reside as attributes on the instance. The purpose of this is to maximize the reuse potential of the components outside of the Experiment, as well as avoid excessive class inheritance within the Experiment. The inheritance tree of the Experiment is reserved for execution concerns , such as switching between singlethreaded, multiprocess, or cluster execution. To enable these different execution contexts without excessive duplicated code, the components that cover computationally or memory-intensive work generally implement methods to generate a collection of serializable tasks to perform later, on either that same object or perhaps another one running in another process or machine. The subclasses of Experiment then differentiate themselves by implementing methods to execute a collection of these tasks using their preferred method of execution, whether it be a simple loop, a process pool, or a cluster. The components are created and experiment configuration is bound to them at Experiment construction time, so that the instance methods can have concise call signatures that only cover the information passed by other components mid-experiment. Data reuse/replacement is handled within components. The Experiment generally just hands the replace flag to each component at object construction, and at runtime each component uses that and determines whether or not the needed work has already been done.","title":"General Class Design"},{"location":"experiments/architecture/#im-trying-to-find-some-behavior-where-does-it-reside","text":"If you're looking to change behavior of the Experiment, When possible, the logic resides in one of the components and hopefully the component list above should be helpful at finding the lines between components. Logic that specifically relates to parallel execution is in one of the experiment subclasses (see parallelization section below). Everything else is in the Experiment base class . This is where the public interface ( .run() ) resides, and follows a template method pattern to define the skeleton of the Experiment: instantating components based on experiment configuration and runtime inputs, and passing output from one component to another.","title":"I'm trying to find some behavior. Where does it reside?"},{"location":"experiments/architecture/#i-want-to-add-a-new-option-where-should-i-put-it","text":"Generally, the experiment configuration is where any new options go that change any data science-related functionality; in other words, if you could conceivably get better precision from the change, it should make it into experiment configuration. This is so the hashed experiment config is meaningful and the experiment can be audited by looking at the experiment configuration rather than requiring the perusal of custom code. The blind spot in this is, of course, the state of the database, which can always change results, but it's useful for database state to continue to be the only exception to this rule. On the other hand, new options that affect only runtime concerns (e.g. performance boosts) should go as arguments to the Experiment. For instance, changing the number of cores to use for matrix building, or telling it to skip predictions won't change the answer you're looking for; options like these just help you potentially get to the answer faster. Once an experiment is completed, runtime flags like these should be totally safe to ignore in analysis.","title":"I want to add a new option. Where should I put it?"},{"location":"experiments/architecture/#storage-abstractions","text":"Another important part of enabling different execution contexts is being able to pass large, persisted objects (e.g. matrices or models) by reference to another process or cluster. To achieve this, as well as provide the ability to configure different storage mediums (e.g. S3) and formats (e,g, HDF) without changes to the Experiment class, all references to these large objects within any components are handled through an abstraction layer.","title":"Storage Abstractions"},{"location":"experiments/architecture/#matrix-storage","text":"All interactions with individual matrices and their bundled metadata are handled through MatrixStore objects. The storage medium is handled through a base Store object that is an attribute of the MatrixStore . The storage format is handled through inheritance on the MatrixStore : Each subclass, such as CSVMatrixStore or HDFMatrixStore , implements the necessary methods ( save , load , head_of_matrix ) to properly persist or load a matrix from its storage. In addition, the MatrixStore provides a variety of methods to retrieve data from either the base matrix itself or its metadata. For instance (this is not meant to be a complete list): matrix - the raw matrix metadata - the raw metadata dictionary exists - whether or not it exists in storage columns - the column list labels - the label column uuid - the matrix's UUID as_of_dates - the matrix's list of as-of-dates One MatrixStorageEngine exists at the Experiment level, and roughly corresponds with a directory wherever matrices are stored. Its only interface is to provide a MatrixStore object given a matrix UUID.","title":"Matrix Storage"},{"location":"experiments/architecture/#model-storage","text":"Model storage is handled similarly to matrix storage, although the interactions with it are far simpler so there is no single-model class akin to the MatrixStore . One ModelStorageEngine exists at the Experiment level, configured with the Experiment's storage medium, and through it trained models can be saved or loaded. The ModelStorageEngine uses joblib to save and load compressed pickles of the model.","title":"Model Storage"},{"location":"experiments/architecture/#miscellaneous-project-storage","text":"Both the ModelStorageEngine and MatrixStorageEngine are based on a more general storage abstraction that is suitable for any other auxiliary objects (e.g. graph images) that need to be stored. That is the ProjectStorage object, which roughly corresponds to a directory on some storage medium where we store everything. One of these exists as an Experiment attribute, and its interface .get_store can be used to persist or load whatever is needed.","title":"Miscellaneous Project Storage"},{"location":"experiments/architecture/#parallelizationsubclassing-details","text":"In the Class Design section above, we introduced tasks for parallelization and subclassing for execution changes. In this section, we expand on these to help provide a new guide to working with these. Currently there are three methods that must be implemented by subclasses of Experiment in order to be fully functional.","title":"Parallelization/Subclassing Details"},{"location":"experiments/architecture/#abstract-methods","text":"process_query_tasks - Run feature generation queries. Receives a list of tasks. each task actually represents a table and is split into three lists of queries to enable the implementation to avoid deadlocks: prepare (table creation), inserts (a collection of INSERT INTO SELECT queries), and finalize (indexing). prepare needs to be run before the inserts and finalize is best run after the inserts, so it is advised that only the inserts are parallelized. The subclass should run each individual batch of queries by calling self.feature_generator.run_commands([list of queries]) , which will run all of the queries serially, so the implementation can send a batch of queries to each worker instead of having each individual query be on a new worker. process_matrix_build_tasks - Run matrix build tasks (that assume all the necessary label/cohort/feature tables have been built). Receives a dictionary of tasks. Each key is a matrix UUID, and each value is a dictionary that has all the necessary keyword arguments to call self.matrix_builder.build_matrix to build one matrix. process_train_test_tasks - Run model train/test tasks (that assume all matrices are built). Receives a list of tasks, each value is a dictionary that has all the necessary keyword arguments to call self.model_train_tester.process_task to train and test one model. Each task covers model training, prediction (on both test and train matrices), model evaluation (on both test and train matrices), and saving of global and individual feature importances.","title":"Abstract Methods"},{"location":"experiments/architecture/#reference-implementations","text":"SingleThreadedExperiment is a barebones implementation that runs everything serially. MultiCoreExperiment utilizes local multiprocessing to run tasks through a worker pool. Reading this is helpful to see the minimal implementation needed for some parallelization. RQExperiment - utilizes an RQ worker cluster to allow the tasks to be parallelized either locally or distributed to other. Does not take care of spawning a cluster or any other infrastructural concerns: it expects that the cluster is running somewhere and is reading from the same Redis instance that is passed to the RQExperiment . The RQExperiment simply enqueues tasks and waits for them to be completed. Reading this is helpful as a simple example of how to enable distributed computing.","title":"Reference Implementations"},{"location":"experiments/cohort-labels/","text":"Cohort and Label Deep Dive \u00b6 This document is intended at providing a deep dive into the concepts of cohorts and labels as they apply to Triage. For context, reading the Triage section of the Dirty Duck tutorial may be helpful before reading this document. Temporal Validation Refresher \u00b6 Triage uses temporal validation to select models because the real-world problems that Triage is built for tend to evolve or change over time. Picking a date range to train on and a date range afterwards to test on ensures that we don't leak data from the future into our models that wouldn't be available in a real-world deployment scenario. Because of this, we often talk in Triage about the as-of-date : all models trained by Triage are associated with an as-of-date , which means that all the data that goes into the model is only included if it was known about before that date . The matrix used to train the model may have multiple as-of-dates , and the most recent is referred to as the model's as-of-date so it's easy to see the cutoff date for data included in the model. For more on temporal validation, see the relevant section in Dirty Duck . What are Cohorts and Labels in Triage? \u00b6 This document assumes that the reader is familiar with the concept of a machine learning target variable and will focus on explaining what is unique to Triage. A cohort is the population used used for modeling on a given as-of-date . This is expressed as a list of entities . An entity is simply the object of prediction, such as a facility to inspect or a patient coming in for a visit. Early warning systems tend to include their entire population (or at least a large subset of it) in the cohort at any given date, while appointment-based problems may only include in a date's cohort the people who are scheduled for an appointment on that date. A label is the binary target variable for a member of the cohort at a given as-of-date and a given label timespan. For instance, in an inspection prioritization problem the question being asked may be 'what facilities are at high risk of having a failed inspection in the next 6 months?' For this problem, the label_timespan is 6 months. There may be multiple label timespans tested in the same experiment, in which case there could be multiple labels for an entity and date. In addition, multiple label definitions are often tested against each other, such as \"any inspection failures\" vs \"inspection failures with serious issues\". Both labels and cohorts are defined in Triage's experiment configuration using SQL queries, with the variables ( as_of_date , label_timespan ) given as placeholders. This allows the definitions to be given in a concise manner while allowing the temporal configuration defined elsewhere in the experiment to produce the actual list of dates and timespans that are calculated during the experiment. Cohort Definition and Examples \u00b6 The cohort is configured with a query that returns a unique list of entity_id s given an as_of_date , and it runs the query for each as_of_date that is produced by your temporal config. You tell Triage where to place each as_of_date with a placeholder surrounded by brackets: {as_of_date} . Note 1 \u00b6 The as_of_date is parsed as a timestamp in the database, which Postgres defaults to midnight at the beginning of the date in question . It's important to consider how this is used for feature generation. Features are only included if they are known about before this timestamp . So features will be only included for an as_of_date if they are known about before that as_of_date . If you want to work around this (e.g for visit-level problems in which you want to intake data on the day of the visit and make predictions using that data the same day ), you can move your cohort up a day. The time splitting in Triage is designed for day granularity so approaches to train up to a specific hour and test at another hour of the same day are not supported. Note 2 \u00b6 Triage expects all entity ids to be integers. Note 3 \u00b6 Triage expects the cohort to be a unique list of entity ids. Throughout the cohort example queries you will see distinct(entity_id) used to ensure this. Example: Inspections \u00b6 Let's say I am prioritizing the inspection of food service facilities such as restaurants, caterers or grocery stores. One simple definition of a cohort for facility inspection would be to include any facilities that have active permits in the last year in the cohort. Assume that these permits are contained in a table, named permits , with the facility's id, a start date, and an end date of the permit. Inspections Cohort Source Table \u00b6 entity_id start_date end_date 25 2016-01-01 2016-02-01 44 2016-01-01 2016-02-01 25 2016-02-01 2016-03-01 Triage expects the cohort query passed to it to return a unique list of entity_id s given an as_of_date , and it runs the query for each as_of_date that is produced by your temporal config. You tell Triage where to place each as_of_date with a placeholder surrounded by brackets: {as_of_date} . An example query that implements the 'past year' definition would be: select distinct(entity_id) from permits where tsrange(start_date, end_date, '[]') @> {as_of_date} Running this query using the as_of_date '2017-01-15' would return both entity ids 25 and 44. Running it with '2017-02-15' would return only entity id 25. Running it with '2017-03-15' would return no rows. Inspections Cohort Config \u00b6 The way this looks in an Experiment configuration YAML is as follows: cohort_config: query: | select distinct(entity_id) from permits where tsrange(start_time, end_time, '[]') @> {as_of_date} name: 'permits_in_last_year' The name key is optional. Part of its purpose is to help you organize different cohorts in your configuration, but it is also included in each matrix's metadata file to help you keep them straight afterwards. Example: Early Intervention \u00b6 An example of an early intervention system is identifying people at risk of recidivism so they can receive extra support to encourage positive outcomes. This example defines the cohort as everybody who has been released from jail within the last three years. It does this by querying an events table for events of type 'release'. Early Intervention Cohort Source Table \u00b6 entity_id event_type knowledge_date 25 booking 2016-02-01 44 booking 2016-02-01 25 release 2016-03-01 Early Intervention Cohort Config \u00b6 cohort_config: query: | SELECT distinct(entity_id) FROM events WHERE event_type = 'release' AND knowledge_date <@ daterange(('{as_of_date}'::date - '3 years'::interval)::date, '{as_of_date}'::date) name: 'booking_last_3_years' Example: Visits \u00b6 Another problem type we may want to model is visit/appointment level modeling. An example would be a health clinic that wants to figure out which patients on a given day who are most at risk for developing diabetes within some time period but don't currently have it. Visits Cohort Source Tables \u00b6 Here we actually define two tables: an appointments table that contains the appointment schedule, and a diabetes diagnoses table that contains positive diabetes diagnoses. appointments entity_id appointment_date 25 2016-02-01 44 2016-02-01 25 2016-03-01 diabetes_diagnoses entity_id diagnosis_date 44 2015-02-01 86 2012-06-01 Visits Cohort Config \u00b6 The cohort config here queries the visits table for the next day, and excludes those who have a diabetes diagnosis at some point in the past. There's a twist: a day is subtracted from the as-of-date. Why? We may be collecting useful data during the appointment about whether or not they will develop diabetes, and we may want to use this data as features. Because the as-of-date refers to the timestamp at the beginning of the day ( see note 1 ), if the as-of-date and appointment date match up exactly we won't be able to use those features. So, appointments show up in the next day's as-of-date. Whether or not this is correct depends on the feasability of generating a prediction during the visit to use this data, which depends on the deployment plans for the system. If data entry and prediction can only happen nightly, you can't expect to use data from the visit in features and would change the as-of-date to match the appointment_date. cohort_config: query: | select distinct(entity_id) from appointments where appointment_date = ('{as_of_date}'::date - interval '1 days')::date and not exists( select entity_id from diabetes_diagnoses where entity_id = appointments.entity_id and as_of_date < '{as_of_date}' group by entity_id) group by entity_id name: 'visit_day_no_previous_diabetes' Testing Cohort Configuration \u00b6 If you want to test out a cohort query without running an entire experiment, there are a few ways, and the easiest way depends on how much of the rest of the experiment you have configured. Option 1: You have not started writing an experiment config file yet . If you just want to test your query with a hardcoded list of dates as Triage does it (including as-of-date interpolation), you can instantiate the CohortTableGenerator with the query and run it for those dates. This skips any temporal config, so you don't have to worry about temporal config: from triage.component.architect.cohort_table_generators import CohortTableGenerator from triage import create_engine from datetime import datetime CohortTableGenerator( query=\"select entity_id from permits where tsrange(start_time, end_time, '[]') @> {as_of_date}\", db_engine=create_engine(...), cohort_table_name=\"my_test_cohort_table\" ).generate_cohort_table([datetime(2016, 1, 1), datetime(2016, 2, 1), datetime(2016, 3, 1)]) Running this will generate a table with the name you gave it ( my_test_cohort_table ), populated with the cohort for that list of dates. You can inspect this table in your SQL browser of choice. Option 2: You have an experiment config file that includes temporal config, and want to look at the cohort in isolation in the database . If you want to actually create the cohort for each date that results from your temporal config, you can go as far as instantiating an Experiment and telling it to generate the cohort. from triage.experiments import SingleThreadedExperiment from triage import create_engine import yaml with open('<your_experiment_config.yaml>') as fd: experiment_config = yaml.load(fd) experiment = SingleThreadedExperiment( experiment_config=experiment_config, db_engine=create_engine(...), project_path='./' ) experiment.generate_cohort() print(experiment.cohort_table_name) This will generate the entire cohort needed for your experiment. The table name is autogenerated by the Experiment, and you can retrieve it using the cohort_table_name attribute of the Experiment. Here, as in option 1, you can look at the data in your SQL browser of choice. These options should be enough to test your cohort in isolation. How the cohort shows up in matrices is also dependent on its interaction with the labels, and later we'll show how to test that. Label Definition and Examples \u00b6 The labels table works similarly to the cohort table: you give it a query with a placeholder for an as-of-date. However, the label query has one more dependency: a label timespan For instance, if you are inspecting buildings for violations, a label timespan of 6 months translates into a label of 'will this building have a violation in the next 6 months?'. These label timespans are generated by your temporal configuration as well and you may have multiple in a single experiment, so what you send Triage in your label query is also a placeholder. Note: The label query is expected by Triage to return only one row per entity id for a given as-of-date/label timespan combination. Missing Labels \u00b6 Since the cohort has its own definition query, separate from the label query, we have to consider the possibility that not every entity in the cohort is present in the label query, and how to deal with these missing labels. The label value in the train matrix in these cases is controlled by a flag in the label config: include_missing_labels_in_train_as . If you omit the flag, they show up as missing. This is common for inspections problems, wherein you really don't know a suitable label. The facility wasn't inspected, so you really don't know what the label is. This makes evaluation a bit more complicated, as some of the facilities with high risk scores may have no labels. But this is a common tradeoff in inspections problems. If you set it to True, that means that all of the rows have positive label. What does this mean? It depends on what exactly your label query is, but a common use would be to model early warning problems of dropouts, in which the absence of an event (e.g. a school enrollment event) is the positive label. If you set it to False, that means that all of these rows have a negative label. A common use for this would be in early warning problems of adverse events, in which the presence of an event (e.g. excessive use of force by a police officer) is the positive label. Example: Inspections \u00b6 Inspections Label Source Table \u00b6 To translate this into our restaurant example above, consider a source table named 'inspections' that contains information about inspections. A simplified version of this table may look like: entity_id date result 25 2016-01-05 pass 44 2016-01-04 fail 25 2016-02-04 fail The entity id is the same as the cohort above: it identifies the restaurant. The date is just the date that the inspection happened, and the result is a string 'pass'/'fail' stating whether or not the restaurant passed the inspection. Inspections Label Config \u00b6 In constructing the label query, we have to consider the note above that we want to return only one row for a given entity id. The easiest way to do this, given that this query is run per as-of-date, is to group by the entity id and aggregate all the matched events somehow. In this case, a sensible definition is that we want any failed inspections to trigger a positive label. So if there is one pass and one fail that falls under the label timespan , the label should be True. bool_or is a handy Postgres aggregation function that does this. A query to find any failed inspections would be written in an experiment YAML config as follows: label_config: query: | select entity_id, bool_or(result = 'fail')::integer as outcome from inspections where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id name: 'failed_inspection' Example: Early Intervention \u00b6 Early Intervention Label Source Table \u00b6 We reuse the generic events table used in the early intervention cohort section. Early Intervention Label Config \u00b6 We would like to assign a True label to everybody who is booked into jail within the label timespan. Note the include_missing_labels_in_train_as value: False . Anybody who does not show up in this query can be assumed to not have been booked into jail, so they can be assigned a False label. label_config: query: | SELECT entity_id, bool_or(CASE WHEN event_type = 'booking' THEN TRUE END)::integer AS outcome FROM events WHERE knowledge_date <@ daterange('{as_of_date}'::date, ('{as_of_date}'::date + interval '{label_timespan}')::date) GROUP BY entity_id include_missing_labels_in_train_as: False name: 'booking' Example: Visits \u00b6 Visits Label Source Table \u00b6 We reuse the diabetes_diagnoses table from the cohort section. Visits Label Config \u00b6 We would like to identify people who are diagnosed with diabetes within a certain label_timespan after the given as-of-date . Note that include_missing_labels_in_train_as is False here as well. Any diagnoses would show up here, so the lack of any results from this query would remove all ambiguity. label_config: query: | select entity_id, 1 as outcome from diabetes_diagnoses where as_of_date <@ daterange('{as_of_date}' :: date, ('{as_of_date}' :: date + interval '{label_timespan}') :: date) group by entity_id include_missing_labels_in_train_as: False name: 'diabetes' Note: If you broadened the scope of this diabetes problem to concern not just diabetes diagnoses but having diabetes in general, and you had access to both positive and negative diabetes tests, you might avoid setting include_missing_labels_in_train_as , similar to the inspections problem, to more completely take into account the possibility that a person may or may not have diabetes. Testing Label Configuration \u00b6 If you want to test out a label query without running a whole experiment, you can test it out similarly to the cohort section above. Option 1: You have not started writing an experiment config file yet . If you just want to test your label query with a hardcoded list of dates as Triage does it (including as-of-date interpolation), you can instantiate the LabelGenerator with the query and run it for those dates. This skips any temporal config, so you don't have to worry about temporal config: from triage.component.architect.label_generators import LabelGenerator from triage import create_engine from datetime import datetime LabelGenerator( query=\"select entity_id, bool_or(result='fail')::integer as outcome from inspections where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id\" db_engine=create_engine(...), ).generate_all_labels( labels_table='test_labels', as_of_dates=[datetime(2016, 1, 1), datetime(2016, 2, 1), datetime(2016, 3, 1)], label_timespans=['3 month'], ) Running this will generate a table with the name you gave it ( test_labels ), populated with the labels for that list of dates. You can inspect this table in your SQL browser of choice. Option 2: You have an experiment config file that includes temporal config, and want to look at the labels in isolation in the database . If you want to actually create the labels for each date that results from your temporal config, you can go as far as instantiating an Experiment and telling it to generate the labels. from triage.experiments import SingleThreadedExperiment from triage import create_engine import yaml with open('<your_experiment_config.yaml>') as fd: experiment_config = yaml.load(fd) experiment = SingleThreadedExperiment( experiment_config=experiment_config, db_engine=create_engine(...), project_path='./' ) experiment.generate_labels() print(experiment.labels_table_name) This will generate the labels for each as-of-date in your experiment. The table name is autogenerated by the Experiment, and you can retrieve it using the labels_table_name attribute of the Experiment. Here, as in option 1, you can look at the data in your SQL browser of choice. These options should be enough to test your labels in isolation. How the labels shows up in matrices is also dependent on its interaction with the cohort, and later we'll show how to test that. Combining Cohorts and Labels to make Matrices \u00b6 Looking at the cohort and labels tables in isolation doesn't quite get you the whole picture. They are combined with features to make matrices, and some of the functionality (e.g. include_missing_labels_in_train_as ) isn't applied until the matrices are made for performance/database disk space purposes. How does this work? Let's look at some example cohort and label tables. Cohort \u00b6 entity_id as_of_date 25 2016-01-01 44 2016-01-01 25 2016-02-01 44 2016-02-01 25 2016-03-01 44 2016-03-01 60 2016-03-01 Label \u00b6 entity_id as_of_date label 25 2016-01-01 True 25 2016-02-01 False 44 2016-02-01 True 25 2016-03-01 False 44 2016-03-01 True 60 2016-03-01 True Above we observe three total cohorts, on 2016-01-01 , 2016-02-01 , and 2016-03-01 . The first two cohorts have two entities each and the last one has a new third entity. For the first cohort, only one of the entities has an explicitly defined label (meaning the label query didn't return anything for them on that date). For simplicity's sake, we are going to assume only one matrix is created that includes all of these cohorts. Depending on the experiment's temporal configuration, there may be one, many, or all dates in a matrix, but the details here are outside of the scope of this document. In general, the index of the matrix is created using a left join in SQL: The cohort table is the left table, and the labels table is the right table, and they are joined on entity id/as of date. So all of the rows that are in the cohort but not the labels table (in this case, just entity 44/date 2016-01-01) will initially have a null label. The final contents of the matrix, however, depend on the include_missing_labels_in_train_as setting. Inspections-Style (preserve missing labels as null) \u00b6 If include_missing_labels_in_train_as is not set, Triage treats it as a truly missing label. The final matrix will look like: entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... null 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True Early Warning Style (missing means False) \u00b6 If include_missing_labels_in_train_as is set to False, Triage treats the absence of a label row as a False label. The final matrix will look like: entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... False 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True Dropout Style (missing means True) \u00b6 If include_missing_labels_in_train_as is set to True, Triage treats the absence of a label row as a True label. The final matrix will look like: entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... True 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True If you would like to test how your cohort and label combine to make matrices, you can tell Triage to generate matrices and then inspect the matrices. To do this, we assume that you have your cohort and label defined in an experiment config file, as well as temporal config. The last piece needed to make matrices is some kind of features. Of course, the features aren't our main focus here, so let's use a placeholder feature that should create very quickly. config_version: 'v6' temporal_config: feature_start_time: '2010-01-04' feature_end_time: '2018-03-01' label_start_time: '2015-02-01' label_end_time: '2018-03-01' model_update_frequency: '1y' training_label_timespans: ['1month'] training_as_of_date_frequencies: '1month' test_durations: '1month' test_label_timespans: ['1month'] test_as_of_date_frequencies: '1month' max_training_histories: '5y' cohort_config: query: | select distinct(entity_id) from permits where tsrange(start_time, end_time, '[]') @> {as_of_date} name: 'permits_in_last_year' label_config: query: | select entity_id, bool_or(result = 'fail')::integer as outcome from inspections where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id name: 'failed_inspection' feature_aggregations: - prefix: 'test' from_obj: 'permits' knowledge_date_column: 'date' aggregates_imputation: all: type: 'zero_noflag' aggregates: [{quantity: '1', metrics: ['sum']}] intervals: ['3month'] groups: ['entity_id'] The above feature aggregation should just create a feature with the value 1 for each entity, but what's important here is that it's a valid feature config that allows us to make complete matrices. To make matrices using all of this configuration, you can run: from triage.experiments import SingleThreadedExperiment from triage import create_engine import yaml with open('<your_experiment_config.yaml>') as fd: experiment_config = yaml.load(fd) experiment = SingleThreadedExperiment( experiment_config=experiment_config, db_engine=create_engine(...), project_path='./' ) experiment.generate_matrices() The matrix generation process will run all of the cohort/label/feature generation above, and then save matrices to your project_path's matrices directory. By default, these are CSVs and should have a few columns: 'entity_id', 'date', 'test_1_sum', and 'failed_inspection'. The 'entity_id' and 'date' columns represent the index of this matrix, and 'failed_inspection' is the label. Each of these CSV files has a YAML file starting with the same hash representing metadata about that matrix. If you want to look for just the train matrices to inspect the results of the include_missing_labels_in_train_as flag, try this command (assuming you can use bash): $ grep \"matrix_type: train\" *.yaml 3343ebf255af6dbb5204a60a4390c7e1.yaml:matrix_type: train 6ee3cd406f00f0f47999513ef5d49e3f.yaml:matrix_type: train 74e2a246e9f6360124b96bea3115e01f.yaml:matrix_type: train a29c9579aa67e5a75b2f814d906e5867.yaml:matrix_type: train a558fae39238d101a66f9d2602a409e6.yaml:matrix_type: train f5bb7bd8f251a2978944ba2b82866153.yaml:matrix_type: train You can then open up those files and ensure that the labels for each entity_id/date pair match what you expect. Wrapup \u00b6 Cohorts and Labels require a lot of care to define correctly as they constitute a large part of the problem framing. Even if you leave all of your feature generation the same, you can completely change the problem you're modeling by changing the label and cohort. Testing your cohort and label config can give you confidence that you're framing the problem the way you expect.","title":"Cohort and Label Deep Dive"},{"location":"experiments/cohort-labels/#cohort-and-label-deep-dive","text":"This document is intended at providing a deep dive into the concepts of cohorts and labels as they apply to Triage. For context, reading the Triage section of the Dirty Duck tutorial may be helpful before reading this document.","title":"Cohort and Label Deep Dive"},{"location":"experiments/cohort-labels/#temporal-validation-refresher","text":"Triage uses temporal validation to select models because the real-world problems that Triage is built for tend to evolve or change over time. Picking a date range to train on and a date range afterwards to test on ensures that we don't leak data from the future into our models that wouldn't be available in a real-world deployment scenario. Because of this, we often talk in Triage about the as-of-date : all models trained by Triage are associated with an as-of-date , which means that all the data that goes into the model is only included if it was known about before that date . The matrix used to train the model may have multiple as-of-dates , and the most recent is referred to as the model's as-of-date so it's easy to see the cutoff date for data included in the model. For more on temporal validation, see the relevant section in Dirty Duck .","title":"Temporal Validation Refresher"},{"location":"experiments/cohort-labels/#what-are-cohorts-and-labels-in-triage","text":"This document assumes that the reader is familiar with the concept of a machine learning target variable and will focus on explaining what is unique to Triage. A cohort is the population used used for modeling on a given as-of-date . This is expressed as a list of entities . An entity is simply the object of prediction, such as a facility to inspect or a patient coming in for a visit. Early warning systems tend to include their entire population (or at least a large subset of it) in the cohort at any given date, while appointment-based problems may only include in a date's cohort the people who are scheduled for an appointment on that date. A label is the binary target variable for a member of the cohort at a given as-of-date and a given label timespan. For instance, in an inspection prioritization problem the question being asked may be 'what facilities are at high risk of having a failed inspection in the next 6 months?' For this problem, the label_timespan is 6 months. There may be multiple label timespans tested in the same experiment, in which case there could be multiple labels for an entity and date. In addition, multiple label definitions are often tested against each other, such as \"any inspection failures\" vs \"inspection failures with serious issues\". Both labels and cohorts are defined in Triage's experiment configuration using SQL queries, with the variables ( as_of_date , label_timespan ) given as placeholders. This allows the definitions to be given in a concise manner while allowing the temporal configuration defined elsewhere in the experiment to produce the actual list of dates and timespans that are calculated during the experiment.","title":"What are Cohorts and Labels in Triage?"},{"location":"experiments/cohort-labels/#cohort-definition-and-examples","text":"The cohort is configured with a query that returns a unique list of entity_id s given an as_of_date , and it runs the query for each as_of_date that is produced by your temporal config. You tell Triage where to place each as_of_date with a placeholder surrounded by brackets: {as_of_date} .","title":"Cohort Definition and Examples"},{"location":"experiments/cohort-labels/#note-1","text":"The as_of_date is parsed as a timestamp in the database, which Postgres defaults to midnight at the beginning of the date in question . It's important to consider how this is used for feature generation. Features are only included if they are known about before this timestamp . So features will be only included for an as_of_date if they are known about before that as_of_date . If you want to work around this (e.g for visit-level problems in which you want to intake data on the day of the visit and make predictions using that data the same day ), you can move your cohort up a day. The time splitting in Triage is designed for day granularity so approaches to train up to a specific hour and test at another hour of the same day are not supported.","title":"Note 1"},{"location":"experiments/cohort-labels/#note-2","text":"Triage expects all entity ids to be integers.","title":"Note 2"},{"location":"experiments/cohort-labels/#note-3","text":"Triage expects the cohort to be a unique list of entity ids. Throughout the cohort example queries you will see distinct(entity_id) used to ensure this.","title":"Note 3"},{"location":"experiments/cohort-labels/#example-inspections","text":"Let's say I am prioritizing the inspection of food service facilities such as restaurants, caterers or grocery stores. One simple definition of a cohort for facility inspection would be to include any facilities that have active permits in the last year in the cohort. Assume that these permits are contained in a table, named permits , with the facility's id, a start date, and an end date of the permit.","title":"Example: Inspections"},{"location":"experiments/cohort-labels/#inspections-cohort-source-table","text":"entity_id start_date end_date 25 2016-01-01 2016-02-01 44 2016-01-01 2016-02-01 25 2016-02-01 2016-03-01 Triage expects the cohort query passed to it to return a unique list of entity_id s given an as_of_date , and it runs the query for each as_of_date that is produced by your temporal config. You tell Triage where to place each as_of_date with a placeholder surrounded by brackets: {as_of_date} . An example query that implements the 'past year' definition would be: select distinct(entity_id) from permits where tsrange(start_date, end_date, '[]') @> {as_of_date} Running this query using the as_of_date '2017-01-15' would return both entity ids 25 and 44. Running it with '2017-02-15' would return only entity id 25. Running it with '2017-03-15' would return no rows.","title":"Inspections Cohort Source Table"},{"location":"experiments/cohort-labels/#inspections-cohort-config","text":"The way this looks in an Experiment configuration YAML is as follows: cohort_config: query: | select distinct(entity_id) from permits where tsrange(start_time, end_time, '[]') @> {as_of_date} name: 'permits_in_last_year' The name key is optional. Part of its purpose is to help you organize different cohorts in your configuration, but it is also included in each matrix's metadata file to help you keep them straight afterwards.","title":"Inspections Cohort Config"},{"location":"experiments/cohort-labels/#example-early-intervention","text":"An example of an early intervention system is identifying people at risk of recidivism so they can receive extra support to encourage positive outcomes. This example defines the cohort as everybody who has been released from jail within the last three years. It does this by querying an events table for events of type 'release'.","title":"Example: Early Intervention"},{"location":"experiments/cohort-labels/#early-intervention-cohort-source-table","text":"entity_id event_type knowledge_date 25 booking 2016-02-01 44 booking 2016-02-01 25 release 2016-03-01","title":"Early Intervention Cohort Source Table"},{"location":"experiments/cohort-labels/#early-intervention-cohort-config","text":"cohort_config: query: | SELECT distinct(entity_id) FROM events WHERE event_type = 'release' AND knowledge_date <@ daterange(('{as_of_date}'::date - '3 years'::interval)::date, '{as_of_date}'::date) name: 'booking_last_3_years'","title":"Early Intervention Cohort Config"},{"location":"experiments/cohort-labels/#example-visits","text":"Another problem type we may want to model is visit/appointment level modeling. An example would be a health clinic that wants to figure out which patients on a given day who are most at risk for developing diabetes within some time period but don't currently have it.","title":"Example: Visits"},{"location":"experiments/cohort-labels/#visits-cohort-source-tables","text":"Here we actually define two tables: an appointments table that contains the appointment schedule, and a diabetes diagnoses table that contains positive diabetes diagnoses. appointments entity_id appointment_date 25 2016-02-01 44 2016-02-01 25 2016-03-01 diabetes_diagnoses entity_id diagnosis_date 44 2015-02-01 86 2012-06-01","title":"Visits Cohort Source Tables"},{"location":"experiments/cohort-labels/#visits-cohort-config","text":"The cohort config here queries the visits table for the next day, and excludes those who have a diabetes diagnosis at some point in the past. There's a twist: a day is subtracted from the as-of-date. Why? We may be collecting useful data during the appointment about whether or not they will develop diabetes, and we may want to use this data as features. Because the as-of-date refers to the timestamp at the beginning of the day ( see note 1 ), if the as-of-date and appointment date match up exactly we won't be able to use those features. So, appointments show up in the next day's as-of-date. Whether or not this is correct depends on the feasability of generating a prediction during the visit to use this data, which depends on the deployment plans for the system. If data entry and prediction can only happen nightly, you can't expect to use data from the visit in features and would change the as-of-date to match the appointment_date. cohort_config: query: | select distinct(entity_id) from appointments where appointment_date = ('{as_of_date}'::date - interval '1 days')::date and not exists( select entity_id from diabetes_diagnoses where entity_id = appointments.entity_id and as_of_date < '{as_of_date}' group by entity_id) group by entity_id name: 'visit_day_no_previous_diabetes'","title":"Visits Cohort Config"},{"location":"experiments/cohort-labels/#testing-cohort-configuration","text":"If you want to test out a cohort query without running an entire experiment, there are a few ways, and the easiest way depends on how much of the rest of the experiment you have configured. Option 1: You have not started writing an experiment config file yet . If you just want to test your query with a hardcoded list of dates as Triage does it (including as-of-date interpolation), you can instantiate the CohortTableGenerator with the query and run it for those dates. This skips any temporal config, so you don't have to worry about temporal config: from triage.component.architect.cohort_table_generators import CohortTableGenerator from triage import create_engine from datetime import datetime CohortTableGenerator( query=\"select entity_id from permits where tsrange(start_time, end_time, '[]') @> {as_of_date}\", db_engine=create_engine(...), cohort_table_name=\"my_test_cohort_table\" ).generate_cohort_table([datetime(2016, 1, 1), datetime(2016, 2, 1), datetime(2016, 3, 1)]) Running this will generate a table with the name you gave it ( my_test_cohort_table ), populated with the cohort for that list of dates. You can inspect this table in your SQL browser of choice. Option 2: You have an experiment config file that includes temporal config, and want to look at the cohort in isolation in the database . If you want to actually create the cohort for each date that results from your temporal config, you can go as far as instantiating an Experiment and telling it to generate the cohort. from triage.experiments import SingleThreadedExperiment from triage import create_engine import yaml with open('<your_experiment_config.yaml>') as fd: experiment_config = yaml.load(fd) experiment = SingleThreadedExperiment( experiment_config=experiment_config, db_engine=create_engine(...), project_path='./' ) experiment.generate_cohort() print(experiment.cohort_table_name) This will generate the entire cohort needed for your experiment. The table name is autogenerated by the Experiment, and you can retrieve it using the cohort_table_name attribute of the Experiment. Here, as in option 1, you can look at the data in your SQL browser of choice. These options should be enough to test your cohort in isolation. How the cohort shows up in matrices is also dependent on its interaction with the labels, and later we'll show how to test that.","title":"Testing Cohort Configuration"},{"location":"experiments/cohort-labels/#label-definition-and-examples","text":"The labels table works similarly to the cohort table: you give it a query with a placeholder for an as-of-date. However, the label query has one more dependency: a label timespan For instance, if you are inspecting buildings for violations, a label timespan of 6 months translates into a label of 'will this building have a violation in the next 6 months?'. These label timespans are generated by your temporal configuration as well and you may have multiple in a single experiment, so what you send Triage in your label query is also a placeholder. Note: The label query is expected by Triage to return only one row per entity id for a given as-of-date/label timespan combination.","title":"Label Definition and Examples"},{"location":"experiments/cohort-labels/#missing-labels","text":"Since the cohort has its own definition query, separate from the label query, we have to consider the possibility that not every entity in the cohort is present in the label query, and how to deal with these missing labels. The label value in the train matrix in these cases is controlled by a flag in the label config: include_missing_labels_in_train_as . If you omit the flag, they show up as missing. This is common for inspections problems, wherein you really don't know a suitable label. The facility wasn't inspected, so you really don't know what the label is. This makes evaluation a bit more complicated, as some of the facilities with high risk scores may have no labels. But this is a common tradeoff in inspections problems. If you set it to True, that means that all of the rows have positive label. What does this mean? It depends on what exactly your label query is, but a common use would be to model early warning problems of dropouts, in which the absence of an event (e.g. a school enrollment event) is the positive label. If you set it to False, that means that all of these rows have a negative label. A common use for this would be in early warning problems of adverse events, in which the presence of an event (e.g. excessive use of force by a police officer) is the positive label.","title":"Missing Labels"},{"location":"experiments/cohort-labels/#example-inspections_1","text":"","title":"Example: Inspections"},{"location":"experiments/cohort-labels/#inspections-label-source-table","text":"To translate this into our restaurant example above, consider a source table named 'inspections' that contains information about inspections. A simplified version of this table may look like: entity_id date result 25 2016-01-05 pass 44 2016-01-04 fail 25 2016-02-04 fail The entity id is the same as the cohort above: it identifies the restaurant. The date is just the date that the inspection happened, and the result is a string 'pass'/'fail' stating whether or not the restaurant passed the inspection.","title":"Inspections Label Source Table"},{"location":"experiments/cohort-labels/#inspections-label-config","text":"In constructing the label query, we have to consider the note above that we want to return only one row for a given entity id. The easiest way to do this, given that this query is run per as-of-date, is to group by the entity id and aggregate all the matched events somehow. In this case, a sensible definition is that we want any failed inspections to trigger a positive label. So if there is one pass and one fail that falls under the label timespan , the label should be True. bool_or is a handy Postgres aggregation function that does this. A query to find any failed inspections would be written in an experiment YAML config as follows: label_config: query: | select entity_id, bool_or(result = 'fail')::integer as outcome from inspections where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id name: 'failed_inspection'","title":"Inspections Label Config"},{"location":"experiments/cohort-labels/#example-early-intervention_1","text":"","title":"Example: Early Intervention"},{"location":"experiments/cohort-labels/#early-intervention-label-source-table","text":"We reuse the generic events table used in the early intervention cohort section.","title":"Early Intervention Label Source Table"},{"location":"experiments/cohort-labels/#early-intervention-label-config","text":"We would like to assign a True label to everybody who is booked into jail within the label timespan. Note the include_missing_labels_in_train_as value: False . Anybody who does not show up in this query can be assumed to not have been booked into jail, so they can be assigned a False label. label_config: query: | SELECT entity_id, bool_or(CASE WHEN event_type = 'booking' THEN TRUE END)::integer AS outcome FROM events WHERE knowledge_date <@ daterange('{as_of_date}'::date, ('{as_of_date}'::date + interval '{label_timespan}')::date) GROUP BY entity_id include_missing_labels_in_train_as: False name: 'booking'","title":"Early Intervention Label Config"},{"location":"experiments/cohort-labels/#example-visits_1","text":"","title":"Example: Visits"},{"location":"experiments/cohort-labels/#visits-label-source-table","text":"We reuse the diabetes_diagnoses table from the cohort section.","title":"Visits Label Source Table"},{"location":"experiments/cohort-labels/#visits-label-config","text":"We would like to identify people who are diagnosed with diabetes within a certain label_timespan after the given as-of-date . Note that include_missing_labels_in_train_as is False here as well. Any diagnoses would show up here, so the lack of any results from this query would remove all ambiguity. label_config: query: | select entity_id, 1 as outcome from diabetes_diagnoses where as_of_date <@ daterange('{as_of_date}' :: date, ('{as_of_date}' :: date + interval '{label_timespan}') :: date) group by entity_id include_missing_labels_in_train_as: False name: 'diabetes' Note: If you broadened the scope of this diabetes problem to concern not just diabetes diagnoses but having diabetes in general, and you had access to both positive and negative diabetes tests, you might avoid setting include_missing_labels_in_train_as , similar to the inspections problem, to more completely take into account the possibility that a person may or may not have diabetes.","title":"Visits Label Config"},{"location":"experiments/cohort-labels/#testing-label-configuration","text":"If you want to test out a label query without running a whole experiment, you can test it out similarly to the cohort section above. Option 1: You have not started writing an experiment config file yet . If you just want to test your label query with a hardcoded list of dates as Triage does it (including as-of-date interpolation), you can instantiate the LabelGenerator with the query and run it for those dates. This skips any temporal config, so you don't have to worry about temporal config: from triage.component.architect.label_generators import LabelGenerator from triage import create_engine from datetime import datetime LabelGenerator( query=\"select entity_id, bool_or(result='fail')::integer as outcome from inspections where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id\" db_engine=create_engine(...), ).generate_all_labels( labels_table='test_labels', as_of_dates=[datetime(2016, 1, 1), datetime(2016, 2, 1), datetime(2016, 3, 1)], label_timespans=['3 month'], ) Running this will generate a table with the name you gave it ( test_labels ), populated with the labels for that list of dates. You can inspect this table in your SQL browser of choice. Option 2: You have an experiment config file that includes temporal config, and want to look at the labels in isolation in the database . If you want to actually create the labels for each date that results from your temporal config, you can go as far as instantiating an Experiment and telling it to generate the labels. from triage.experiments import SingleThreadedExperiment from triage import create_engine import yaml with open('<your_experiment_config.yaml>') as fd: experiment_config = yaml.load(fd) experiment = SingleThreadedExperiment( experiment_config=experiment_config, db_engine=create_engine(...), project_path='./' ) experiment.generate_labels() print(experiment.labels_table_name) This will generate the labels for each as-of-date in your experiment. The table name is autogenerated by the Experiment, and you can retrieve it using the labels_table_name attribute of the Experiment. Here, as in option 1, you can look at the data in your SQL browser of choice. These options should be enough to test your labels in isolation. How the labels shows up in matrices is also dependent on its interaction with the cohort, and later we'll show how to test that.","title":"Testing Label Configuration"},{"location":"experiments/cohort-labels/#combining-cohorts-and-labels-to-make-matrices","text":"Looking at the cohort and labels tables in isolation doesn't quite get you the whole picture. They are combined with features to make matrices, and some of the functionality (e.g. include_missing_labels_in_train_as ) isn't applied until the matrices are made for performance/database disk space purposes. How does this work? Let's look at some example cohort and label tables.","title":"Combining Cohorts and Labels to make Matrices"},{"location":"experiments/cohort-labels/#cohort","text":"entity_id as_of_date 25 2016-01-01 44 2016-01-01 25 2016-02-01 44 2016-02-01 25 2016-03-01 44 2016-03-01 60 2016-03-01","title":"Cohort"},{"location":"experiments/cohort-labels/#label","text":"entity_id as_of_date label 25 2016-01-01 True 25 2016-02-01 False 44 2016-02-01 True 25 2016-03-01 False 44 2016-03-01 True 60 2016-03-01 True Above we observe three total cohorts, on 2016-01-01 , 2016-02-01 , and 2016-03-01 . The first two cohorts have two entities each and the last one has a new third entity. For the first cohort, only one of the entities has an explicitly defined label (meaning the label query didn't return anything for them on that date). For simplicity's sake, we are going to assume only one matrix is created that includes all of these cohorts. Depending on the experiment's temporal configuration, there may be one, many, or all dates in a matrix, but the details here are outside of the scope of this document. In general, the index of the matrix is created using a left join in SQL: The cohort table is the left table, and the labels table is the right table, and they are joined on entity id/as of date. So all of the rows that are in the cohort but not the labels table (in this case, just entity 44/date 2016-01-01) will initially have a null label. The final contents of the matrix, however, depend on the include_missing_labels_in_train_as setting.","title":"Label"},{"location":"experiments/cohort-labels/#inspections-style-preserve-missing-labels-as-null","text":"If include_missing_labels_in_train_as is not set, Triage treats it as a truly missing label. The final matrix will look like: entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... null 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True","title":"Inspections-Style (preserve missing labels as null)"},{"location":"experiments/cohort-labels/#early-warning-style-missing-means-false","text":"If include_missing_labels_in_train_as is set to False, Triage treats the absence of a label row as a False label. The final matrix will look like: entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... False 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True","title":"Early Warning Style (missing means False)"},{"location":"experiments/cohort-labels/#dropout-style-missing-means-true","text":"If include_missing_labels_in_train_as is set to True, Triage treats the absence of a label row as a True label. The final matrix will look like: entity_id as_of_date ...features... label 25 2016-01-01 ... True 44 2016-01-01 ... True 25 2016-02-01 ... False 44 2016-02-01 ... True 25 2016-03-01 ... False 44 2016-03-01 ... True 60 2016-03-01 ... True If you would like to test how your cohort and label combine to make matrices, you can tell Triage to generate matrices and then inspect the matrices. To do this, we assume that you have your cohort and label defined in an experiment config file, as well as temporal config. The last piece needed to make matrices is some kind of features. Of course, the features aren't our main focus here, so let's use a placeholder feature that should create very quickly. config_version: 'v6' temporal_config: feature_start_time: '2010-01-04' feature_end_time: '2018-03-01' label_start_time: '2015-02-01' label_end_time: '2018-03-01' model_update_frequency: '1y' training_label_timespans: ['1month'] training_as_of_date_frequencies: '1month' test_durations: '1month' test_label_timespans: ['1month'] test_as_of_date_frequencies: '1month' max_training_histories: '5y' cohort_config: query: | select distinct(entity_id) from permits where tsrange(start_time, end_time, '[]') @> {as_of_date} name: 'permits_in_last_year' label_config: query: | select entity_id, bool_or(result = 'fail')::integer as outcome from inspections where '{as_of_date}'::timestamp <= date and date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id name: 'failed_inspection' feature_aggregations: - prefix: 'test' from_obj: 'permits' knowledge_date_column: 'date' aggregates_imputation: all: type: 'zero_noflag' aggregates: [{quantity: '1', metrics: ['sum']}] intervals: ['3month'] groups: ['entity_id'] The above feature aggregation should just create a feature with the value 1 for each entity, but what's important here is that it's a valid feature config that allows us to make complete matrices. To make matrices using all of this configuration, you can run: from triage.experiments import SingleThreadedExperiment from triage import create_engine import yaml with open('<your_experiment_config.yaml>') as fd: experiment_config = yaml.load(fd) experiment = SingleThreadedExperiment( experiment_config=experiment_config, db_engine=create_engine(...), project_path='./' ) experiment.generate_matrices() The matrix generation process will run all of the cohort/label/feature generation above, and then save matrices to your project_path's matrices directory. By default, these are CSVs and should have a few columns: 'entity_id', 'date', 'test_1_sum', and 'failed_inspection'. The 'entity_id' and 'date' columns represent the index of this matrix, and 'failed_inspection' is the label. Each of these CSV files has a YAML file starting with the same hash representing metadata about that matrix. If you want to look for just the train matrices to inspect the results of the include_missing_labels_in_train_as flag, try this command (assuming you can use bash): $ grep \"matrix_type: train\" *.yaml 3343ebf255af6dbb5204a60a4390c7e1.yaml:matrix_type: train 6ee3cd406f00f0f47999513ef5d49e3f.yaml:matrix_type: train 74e2a246e9f6360124b96bea3115e01f.yaml:matrix_type: train a29c9579aa67e5a75b2f814d906e5867.yaml:matrix_type: train a558fae39238d101a66f9d2602a409e6.yaml:matrix_type: train f5bb7bd8f251a2978944ba2b82866153.yaml:matrix_type: train You can then open up those files and ensure that the labels for each entity_id/date pair match what you expect.","title":"Dropout Style (missing means True)"},{"location":"experiments/cohort-labels/#wrapup","text":"Cohorts and Labels require a lot of care to define correctly as they constitute a large part of the problem framing. Even if you leave all of your feature generation the same, you can completely change the problem you're modeling by changing the label and cohort. Testing your cohort and label config can give you confidence that you're framing the problem the way you expect.","title":"Wrapup"},{"location":"experiments/defining/","text":"Defining an Experiment \u00b6 This doc is coming soon. In the meantime, check out: Example Experiment Definition for an overview of the different sections of an experiment definition. Dirty Duck for a beginning-to-end walkthrough to Triage, include deep dives on creating experiment configuration.","title":"Defining an Experiment"},{"location":"experiments/defining/#defining-an-experiment","text":"This doc is coming soon. In the meantime, check out: Example Experiment Definition for an overview of the different sections of an experiment definition. Dirty Duck for a beginning-to-end walkthrough to Triage, include deep dives on creating experiment configuration.","title":"Defining an Experiment"},{"location":"experiments/extending-features/","text":"Extending Feature Generation \u00b6 This document describes how to extend Triage's feature generation capabilities by writing new FeatureBlock classes and incorporating them into Experiments. What is a FeatureBlock? \u00b6 A FeatureBlock represents a single feature table in the database and how to generate it. If you're familiar with collate parlance, a SpacetimeAggregation is similar in scope to a FeatureBlock. A FeatureBlock class can be instantiated with whatever arguments it needs,and from there can provide queries to produce its output feature table. Full-size Triage experiments tend to contain multiple feature blocks. These all live in a collection as the experiment.feature_blocks property in the Experiment. What existing FeatureBlock classes can I use? \u00b6 Class name Experiment config key Use triage.component.collate.SpacetimeAggregation spacetime_aggregations Temporal aggregations of event-based data Writing a new FeatureBlock class \u00b6 The FeatureBlock base class defines a set of abstract methods that any child class must implement, as well as a number of initialization arguments that it must take and implement in order to fulfill expectations Triage users have on feature generators. Triage expects these classes to define the queries they need to run, as opposed to generating the tables themselves, so that Triage can implement scaling by parallelization. Abstract methods \u00b6 Any method here without parentheses afterwards is expected to be a property. Method Task Return Type final_feature_table_name The name of the final table with all features filled in (no missing values) string feature_columns The list of feature columns in the final, postimputation table. Should exclude any index columns (e.g. entity id, date) list preinsert_queries Return all queries that should be run before inserting any data. The creation of your feature table should happen here, and is expected to have entity_id(integer) and as_of_date(timestamp) columns. list insert_queries Return all inserts to populate this data. Each query in this list should be parallelizable, and should be valid after all preinsert_queries are run. list postinsert_queries Return all queries that should be run after inserting all data list imputation_queries Return all queries that should be run to fill in missing data with imputed values. list verify_no_nulls() Verify that there are no nulls remaining in the imputed table. Should raise an error if there are any. void Any of the query list properties can be empty: for instance, if your implementation doesn't have inserts separate from table creation and is just one big query (e.g. a CREATE TABLE AS ), you could just define preinsert_queries so be that one mega-query and leave the other properties as empty lists. Properties Provided by Base Class \u00b6 Name Type Purpose as_of_dates list Features are created \"as of\" specific dates, and expects that each of these dates will be populated with a row for each member of the cohort on that date. cohort_table string The final shape of the feature table should at least include every entity id/date pair in this cohort table. db_engine sqlalchemy.engine The engine to use to access the database. Although these instances are mostly returning queries, the engine may be useful for implementing imputation. features_schema_name string The database schema where all feature tables should reside. feature_start_time string/datetime A time before which no data should be considered for features features_ignore_cohort bool If True (the default), features are only computed for members of the cohort. If False, the shape of the final feature table could include more. FeatureBlock child classes can, and in almost all cases will, include more configuration at initialization time that are specific to them. They probably also define many more methods to use internally. But as long as they adhere to this interface, they'll work with Triage. Making the new FeatureBlock available to experiments \u00b6 Triage Experiments run on serializable configuration, and although it's possible to take fully generated FeatureBlock instances and bypass this (e.g. experiment.feature_blocks = <my_collection_of_feature_blocks> ), it's not recommended. The last step is to pick a config key for use within the features key of experiment configs, within triage.component.architect.feature_block_generators.FEATURE_BLOCK_GENERATOR_LOOKUP and point it to a function that instantiates a bunch of your objects based on config.","title":"Extending Feature Generation"},{"location":"experiments/extending-features/#extending-feature-generation","text":"This document describes how to extend Triage's feature generation capabilities by writing new FeatureBlock classes and incorporating them into Experiments.","title":"Extending Feature Generation"},{"location":"experiments/extending-features/#what-is-a-featureblock","text":"A FeatureBlock represents a single feature table in the database and how to generate it. If you're familiar with collate parlance, a SpacetimeAggregation is similar in scope to a FeatureBlock. A FeatureBlock class can be instantiated with whatever arguments it needs,and from there can provide queries to produce its output feature table. Full-size Triage experiments tend to contain multiple feature blocks. These all live in a collection as the experiment.feature_blocks property in the Experiment.","title":"What is a FeatureBlock?"},{"location":"experiments/extending-features/#what-existing-featureblock-classes-can-i-use","text":"Class name Experiment config key Use triage.component.collate.SpacetimeAggregation spacetime_aggregations Temporal aggregations of event-based data","title":"What existing FeatureBlock classes can I use?"},{"location":"experiments/extending-features/#writing-a-new-featureblock-class","text":"The FeatureBlock base class defines a set of abstract methods that any child class must implement, as well as a number of initialization arguments that it must take and implement in order to fulfill expectations Triage users have on feature generators. Triage expects these classes to define the queries they need to run, as opposed to generating the tables themselves, so that Triage can implement scaling by parallelization.","title":"Writing a new FeatureBlock class"},{"location":"experiments/extending-features/#abstract-methods","text":"Any method here without parentheses afterwards is expected to be a property. Method Task Return Type final_feature_table_name The name of the final table with all features filled in (no missing values) string feature_columns The list of feature columns in the final, postimputation table. Should exclude any index columns (e.g. entity id, date) list preinsert_queries Return all queries that should be run before inserting any data. The creation of your feature table should happen here, and is expected to have entity_id(integer) and as_of_date(timestamp) columns. list insert_queries Return all inserts to populate this data. Each query in this list should be parallelizable, and should be valid after all preinsert_queries are run. list postinsert_queries Return all queries that should be run after inserting all data list imputation_queries Return all queries that should be run to fill in missing data with imputed values. list verify_no_nulls() Verify that there are no nulls remaining in the imputed table. Should raise an error if there are any. void Any of the query list properties can be empty: for instance, if your implementation doesn't have inserts separate from table creation and is just one big query (e.g. a CREATE TABLE AS ), you could just define preinsert_queries so be that one mega-query and leave the other properties as empty lists.","title":"Abstract methods"},{"location":"experiments/extending-features/#properties-provided-by-base-class","text":"Name Type Purpose as_of_dates list Features are created \"as of\" specific dates, and expects that each of these dates will be populated with a row for each member of the cohort on that date. cohort_table string The final shape of the feature table should at least include every entity id/date pair in this cohort table. db_engine sqlalchemy.engine The engine to use to access the database. Although these instances are mostly returning queries, the engine may be useful for implementing imputation. features_schema_name string The database schema where all feature tables should reside. feature_start_time string/datetime A time before which no data should be considered for features features_ignore_cohort bool If True (the default), features are only computed for members of the cohort. If False, the shape of the final feature table could include more. FeatureBlock child classes can, and in almost all cases will, include more configuration at initialization time that are specific to them. They probably also define many more methods to use internally. But as long as they adhere to this interface, they'll work with Triage.","title":"Properties Provided by Base Class"},{"location":"experiments/extending-features/#making-the-new-featureblock-available-to-experiments","text":"Triage Experiments run on serializable configuration, and although it's possible to take fully generated FeatureBlock instances and bypass this (e.g. experiment.feature_blocks = <my_collection_of_feature_blocks> ), it's not recommended. The last step is to pick a config key for use within the features key of experiment configs, within triage.component.architect.feature_block_generators.FEATURE_BLOCK_GENERATOR_LOOKUP and point it to a function that instantiates a bunch of your objects based on config.","title":"Making the new FeatureBlock available to experiments"},{"location":"experiments/feature-testing/","text":"Testing a Feature Aggregation \u00b6 Developing features for Triage experiments can be a daunting task. There are a lot of things to configure, a small amount of configuration can result in a ton of SQL, and it can take a long time to validate your feature configuration in the context of an Experiment being run on real data. To speed up the process of iterating on features, you can run a list of feature aggregations, without imputation, on just one as-of-date. This functionality can be accessed through the triage command line tool or called directly from code (say, in a Jupyter notebook) using the FeatureGenerator component. Using Triage CLI \u00b6 The command-line interface for testing features takes in two arguments: - A feature config file. Refer to example_feature_config.yaml . Essentially this is the content of the example_experiment_config.yaml 's feature_aggregations section. It consists of a YAML list, with one or more feature_aggregation rows present. - An as-of-date. This should be in the format 2016-01-01 . Example: triage experiment featuretest example/config/feature.yaml 2016-01-01 All given feature aggregations will be processed for the given date. You will see a bunch of queries pass by in your terminal, populating tables in the features_test schema which you can inspect afterwards. Using Python Code \u00b6 If you'd like to call this from a notebook or from any other Python code, the arguments look similar but are a bit different. You have to supply your own sqlalchemy database engine to create a 'FeatureGenerator' object, and then call the create_features_before_imputation method with your feature config as a list of dictionaries, along with an as-of-date as a string. Make sure your logging level is set to INFO if you want to see all of the queries. from triage.component.architect.feature_generators import FeatureGenerator from triage.util.db import create_engine import logging import yaml logging.basicConfig(level=logging.INFO) # create a db_engine db_url = 'your db url here' db_engine = create_engine(db_url) feature_config = [{ 'prefix': 'aprefix', 'aggregates': [ { 'quantity': 'quantity_one', 'metrics': ['sum', 'count'], ], 'categoricals': [ { 'column': 'cat_one', 'choices': ['good', 'bad'], 'metrics': ['sum'] }, ], 'groups': ['entity_id', 'zip_code'], 'intervals': ['all'], 'knowledge_date_column': 'knowledge_date', 'from_obj': 'data' }] FeatureGenerator(db_engine, 'features_test').create_features_before_imputation( feature_aggregation_config=feature_config, feature_dates=['2016-01-01'] )","title":"Testing Feature Configuration"},{"location":"experiments/feature-testing/#testing-a-feature-aggregation","text":"Developing features for Triage experiments can be a daunting task. There are a lot of things to configure, a small amount of configuration can result in a ton of SQL, and it can take a long time to validate your feature configuration in the context of an Experiment being run on real data. To speed up the process of iterating on features, you can run a list of feature aggregations, without imputation, on just one as-of-date. This functionality can be accessed through the triage command line tool or called directly from code (say, in a Jupyter notebook) using the FeatureGenerator component.","title":"Testing a Feature Aggregation"},{"location":"experiments/feature-testing/#using-triage-cli","text":"The command-line interface for testing features takes in two arguments: - A feature config file. Refer to example_feature_config.yaml . Essentially this is the content of the example_experiment_config.yaml 's feature_aggregations section. It consists of a YAML list, with one or more feature_aggregation rows present. - An as-of-date. This should be in the format 2016-01-01 . Example: triage experiment featuretest example/config/feature.yaml 2016-01-01 All given feature aggregations will be processed for the given date. You will see a bunch of queries pass by in your terminal, populating tables in the features_test schema which you can inspect afterwards.","title":"Using Triage CLI"},{"location":"experiments/feature-testing/#using-python-code","text":"If you'd like to call this from a notebook or from any other Python code, the arguments look similar but are a bit different. You have to supply your own sqlalchemy database engine to create a 'FeatureGenerator' object, and then call the create_features_before_imputation method with your feature config as a list of dictionaries, along with an as-of-date as a string. Make sure your logging level is set to INFO if you want to see all of the queries. from triage.component.architect.feature_generators import FeatureGenerator from triage.util.db import create_engine import logging import yaml logging.basicConfig(level=logging.INFO) # create a db_engine db_url = 'your db url here' db_engine = create_engine(db_url) feature_config = [{ 'prefix': 'aprefix', 'aggregates': [ { 'quantity': 'quantity_one', 'metrics': ['sum', 'count'], ], 'categoricals': [ { 'column': 'cat_one', 'choices': ['good', 'bad'], 'metrics': ['sum'] }, ], 'groups': ['entity_id', 'zip_code'], 'intervals': ['all'], 'knowledge_date_column': 'knowledge_date', 'from_obj': 'data' }] FeatureGenerator(db_engine, 'features_test').create_features_before_imputation( feature_aggregation_config=feature_config, feature_dates=['2016-01-01'] )","title":"Using Python Code"},{"location":"experiments/features/","text":"Feature Generation Recipe Book \u00b6 This document is a collection of 'collate' aggregate features that we have found useful to create in Triage that may not be apparent at first. For an introduction to feature generation in Triage, refer to Dirty Duck Feature Generation Age \u00b6 You can calculate age from a date of birth column using the collate_date special variable. This variable is marked as a placeholder in the feature quantity input, but is replaced with each as-of-date when features are being calculated. Combined with the Postgres age function, this calculates a person's age at each as-of-date as a feature. For this example, let's assume you have a column called 'dob' that is a timestamp (or anything that can be cast to a date) in your source table. The feature_aggregation 's quantity would be: EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE)) If Triage is calculating this for the as-of-date '2016-01-01', it will internally expand the collate_date out to: EXTRACT(YEAR FROM AGE('2016-01-01'::DATE, dob::DATE)) In context, a feature aggregate that uses age may look more like: aggregates: - # age in years quantity: age: \"EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE))\" metrics: ['max'] Here, we call the feature 'age' and since everything in collate is defined as an aggregate, we pick 'max'; Any records for the same person and as-of-date should have the same 'dob', so there are many aggregates you can use that will arrive at the same answer (e.g. 'min', 'avg'). In these cases 'max' is the standard aggregate metric of choice in Triage.","title":"Feature Generation Recipe Book"},{"location":"experiments/features/#feature-generation-recipe-book","text":"This document is a collection of 'collate' aggregate features that we have found useful to create in Triage that may not be apparent at first. For an introduction to feature generation in Triage, refer to Dirty Duck Feature Generation","title":"Feature Generation Recipe Book"},{"location":"experiments/features/#age","text":"You can calculate age from a date of birth column using the collate_date special variable. This variable is marked as a placeholder in the feature quantity input, but is replaced with each as-of-date when features are being calculated. Combined with the Postgres age function, this calculates a person's age at each as-of-date as a feature. For this example, let's assume you have a column called 'dob' that is a timestamp (or anything that can be cast to a date) in your source table. The feature_aggregation 's quantity would be: EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE)) If Triage is calculating this for the as-of-date '2016-01-01', it will internally expand the collate_date out to: EXTRACT(YEAR FROM AGE('2016-01-01'::DATE, dob::DATE)) In context, a feature aggregate that uses age may look more like: aggregates: - # age in years quantity: age: \"EXTRACT(YEAR FROM AGE('{collate_date}'::DATE, dob::DATE))\" metrics: ['max'] Here, we call the feature 'age' and since everything in collate is defined as an aggregate, we pick 'max'; Any records for the same person and as-of-date should have the same 'dob', so there are many aggregates you can use that will arrive at the same answer (e.g. 'min', 'avg'). In these cases 'max' is the standard aggregate metric of choice in Triage.","title":"Age"},{"location":"experiments/running/","text":"Running an Experiment \u00b6 Prerequisites \u00b6 To use a Triage experiment, you first need: Python 3.5 A PostgreSQL database with your source data (events, geographical data, etc) loaded. Ample space on an available disk (or S3) to store the needed matrices and models for your experiment An experiment definition (see Defining an Experiment ) You may run a Triage experiment two ways: through the Triage command line tool, or through instantiating an Experiment object in your own Python code and running it. The rest of this document will introduce experiment runs of increasing complexity, through both the CLI and Python interfaces. Simple Example \u00b6 To run an experiment, you need to tell triage at a minimum where to find the experiment file (in YAML format), and how to connect to the database, In this simple example, we're assuming that the experiment will be run with only one process, and that the matrices and models should be stored on the local filesystem. CLI \u00b6 The Triage CLI defaults database connection information to a file stored in 'database.yaml', so with this you can omit any mention of the database. In addition, if you leave out the project path. In addition, the 'project path' (where matrices and models are stored) defaults to the current working directory. So this is the simplest possible invocation: triage experiment example/config/experiment.yaml If you have the database information stored somewhere else, you may pass it to the top-level 'triage' command: triage -d mydbconfig.yaml experiment example/config/experiment.yaml Assuming you want the matrices and models stored somewhere else, pass it as the --project-path : triage -d mydbconfig.yaml experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' Python \u00b6 When running an experiment in Python, the database information is passed in the form of a SQLAlchemy database engine, and the experiment information is passed as a dictionary rather as YAML specifically. from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), # http://docs.sqlalchemy.org/en/latest/core/engines.html project_path='/path/to/directory/to/save/data' ) experiment.run() Multicore example \u00b6 Triage also offers the ability to locally parallelize both CPU-heavy and database-heavy tasks. Triage uses the pebble library to perform both of these, but they are separately configurable as the database tasks will more likely be bounded by the number of connections/cores available on the database server instead of the number of cores available on the experiment running machine. CLI \u00b6 The Triage CLI allows parallelization to be specified through the --n-processes and --n-db-processes parameters. triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --n-db-processes 4 --n-processes 8 Python \u00b6 In Python, you can use the MultiCoreExperiment instead of the SingleThreadedExperiment , and similarly pass the n_processes and n_db_processes parameters. We also recommend using triage.create_engine . It will create a serializable version of the engine that will be fully reconstructed in multiprocess contexts. If you pass a regular SQLAlchemy engine, in these contexts the engine will be reconstructed with the database URL only , which may cancel other settings you have used to configure your engine. from triage.experiments import MultiCoreExperiment from triage import create_engine experiment = MultiCoreExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), project_path='/path/to/directory/to/save/data', n_db_processes=4, n_processes=8, ) experiment.run() The pebble library offers an interface around Python3's concurrent.futures module that adds in a very helpful tool: watching for killed subprocesses . Model training (and sometimes, matrix building) can be a memory-hungry task, and Triage can not guarantee that the operating system you're running on won't kill the worker processes in a way that prevents them from reporting back to the parent Experiment process. With Pebble, this occurrence is caught like a regular Exception, which allows the Process pool to recover and include the information in the Experiment's log. Using S3 to store matrices and models \u00b6 Triage can operate on different storage engines for matrices and models, and besides the standard filesystem engine comes with S3 support out of the box. To use this, just use the s3:// scheme for your project_path (this is similar for both Python and the CLI). CLI \u00b6 triage experiment example/config/experiment.yaml --project-path 's3://bucket/directory/to/save/data' Python \u00b6 from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), project_path='s3://bucket/directory/to/save/data' ) experiment.run() Using HDF5 as a matrix storage format \u00b6 Triage by default uses CSV format to store matrices, but this can take up a lot of space. However, this is configurable. Triage ships with an HDF5 storage module that you can use. CLI \u00b6 On the command-line, this is configurable using the --matrix-format option, and supports csv and hdf . triage experiment example/config/experiment.yaml --matrix-format hdf Python \u00b6 In Python, this is configurable using the matrix_storage_class keyword argument. To allow users to write their own storage modules, this is passed in the form of a class. The shipped modules are in triage.component.catwalk.storage . If you'd like to write your own storage module, you can use the existing modules as a guide. from triage.experiments import SingleThreadedExperiment from triage.component.catwalk.storage import HDFMatrixStore experiment = SingleThreadedExperiment( config=experiment_config db_engine=create_engine(...), matrix_storage_class=HDFMatrixStore, project_path='/path/to/directory/to/save/data', ) experiment.run() Note: The HDF storage option is not compatible with S3. Validating an Experiment \u00b6 Configuring an experiment is complex, and running an experiment can take a long time as data scales up. If there are any misconfigured values, it's going to help out a lot to figure out what they are before we run the Experiment. So when you have completed your experiment config and want to test it out, it's best to validate the Experiment first. If any problems are detectable in your Experiment, either in configuration or the database tables referenced by it, this method will throw an exception. For instance, if I refer to the cat_complaints table in a feature aggregation but it doesn't exist, I'll see something like this: *** ValueError: from_obj query does not run. from_obj: \"cat_complaints\" Full error: (psycopg2.ProgrammingError) relation \"cat_complaints\" does not exist LINE 1: explain select * from cat_complaints ^ [SQL: 'explain select * from cat_complaints'] CLI \u00b6 The CLI, by default, validates before running. You can tweak this behavior, and make it not validate, or make it only validate. triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --no-validate triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --validate-only Python \u00b6 Experiments expose a validate method that can be run as needed. Experiment instantiation doesn't change from the run examples at all. experiment.validate() By default, the validate method will stop as soon as it encounters an error ('strict' mode). If you would like it to validate each section without stopping (i.e. if you have only written part of the experiment configuration), call validate(strict=False) and all of the errors will be changed to warnings. We'd like to add more validations for common misconfiguration problems over time. If you got an unexpected error that turned out to be related to a confusing configuration value, help us out by adding to the validation module and submitting a pull request! Restarting an Experiment \u00b6 If an experiment fails for any reason, you can restart it. By default, all work will be recreated. This includes label queries, feature queries, matrix building, model training, etc. However, if you pass the replace=False keyword argument, the Experiment will reuse what work it can. Cohort Table: The Experiment refers to a cohort table namespaced by the cohort name and a hash of the cohort query, and in that way allows you to reuse cohorts between different experiments if their label names and queries are identical. When referring to this table, it will check on an as-of-date level whether or not there are any existing rows for that date, and skip the cohort query for that date if so. For this reason, it is not aware of specific entities or source events so if the source data has changed, ensure that replace is set to True. Labels Table: The Experiment refers to a labels table namespaced by the label name and a hash of the label query, and in that way allows you to reuse labels between different experiments if their label names and queries are identical. When referring to this table, it will check on a per- as_of_date / label timespan level whether or not there are any existing rows, and skip the label query if so. For this reason, it is not aware of specific entities or source events so if the label query has changed or the source data has changed, ensure that replace is set to True. Features Tables: The Experiment will check on a per-table basis whether or not it exists and contains rows for the entire cohort, and skip the feature generation if so. It does not look at the column list for the feature table or inspect the feature data itself. So, if you have modified any source data that affects a feature aggregation, or added any columns to that aggregation, you won't want to set replace to False. However, it is cohort-and-date aware so you can change around your cohort and temporal configuration safely. Matrix Building: Each matrix's metadata is hashed to create a unique id. If a file exists in storage with that hash, it will be reused. Model Training: Each model's metadata (which includes its train matrix's hash) is hashed to create a unique id. If a file exists in storage with that hash, it will be reused. CLI \u00b6 triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --replace Python \u00b6 from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), project_path='s3://bucket/directory/to/save/data', replace=True ) experiment.run() Optimizing an Experiment \u00b6 Skipping Prediction Syncing \u00b6 By default, the Experiment will save predictions to the database. This can take a long time if your test matrices have a lot of rows, and isn't quite necessary if you just want to see the high-level performance of your grid. By switching save_predictions to False , you can skip the prediction saving. You'll still get your evaluation metrics, so you can look at performance. Don't worry, you can still get your predictions back later by rerunning the Experiment later at default settings, which will find your already-trained models, generate predictions, and save them. CLI: triage experiment myexperiment.yaml --no-save-predictions Python: SingleThreadedExperiment(..., save_predictions=False) Running parts of an Experiment \u00b6 If you would like incrementally build, or just incrementally run parts of the Experiment look at their outputs, you can do so. Running a full experiment requires the experiment config to be filled out, but when you're getting started using Triage it can be easier to build the experiment piece by piece and see the results as they come in. Make sure logging is set to INFO level before running this to ensure you get all the log messages. Running parts of an experiment is only supported through the Python interface. Python \u00b6 experiment.run() will run until it no longer has enough configuration to proceed. You will see information in the logs telling you about the steps it was able to perform. If you initialize the Experiment with cleanup=False , you can view the intermediate tables that are built. They are modified with the experiment hash that the experiment calculates, but this will be printed out in the log messages. labels_*<experiment_hash>* for the labels generated per entity and as of date. tmp_sparse_states_*<experiment_hash>* for the membership in each cohort per entity and as_of_date To reproduce the entire Experiment piece by piece, you can run the following. Each one of these methods requires some portion of experiment config to be passed: experiment.split_definitions will parse temporal config and create time splits. It only requires temporal_config . experiment.generate_cohort() will use the cohort config and as of dates from the temporal config to generate an internal table keeping track of what entities are in the cohort on different dates. It requires temporal_config and cohort_config . experiment.generate_labels() will use the label config and as of dates from the temporal config to generate an internal labels table. It requires temporal_config and label_config . experiment.generate_preimputation_features() will use the feature aggregation config and as of dates from the temporal config to generate internal features tables. It requires temporal_config and feature_aggregations . experiment.generate_imputed_features() will use the imputation sections of the feature aggregation config and the results from the preimputed features to create internal imputed features tables. It requires temporal_config and feature_aggregations . experiment.build_matrices() will use all of the internal tables generated before this point, along with feature grouping config, to generate all needed matrices. It requires temporal_config , cohort_config , label_config , and feature_aggregations , though it will also use feature_group_definitions , feature_group_strategies , and user_metadata if present. experiment.train_and_test_models() will use the generated matrices, grid config and evaluation metric config to train and test all needed models. It requires all configuration keys. Evaluating results of an Experiment \u00b6 After the experiment run, a variety of schemas and tables will be created and populated in the configured database: model_metadata.experiments - The experiment configuration and a hash model_metadata.matrices - Each train or test matrix that is built has a row here, with some basic metadata model_metadata.experiment_matrices - A many-to-many table between experiments and matrices. This will have a row if the experiment used the matrix, regardless of whether or not it had to build it model_metadata.models - A model describes a trained classifier; you'll have one row for each trained file that gets saved. model_metadata.experiment_models - A many-to-many table between experiments and models. This will have a row if the experiment used the model, regardless of whether or not it had to build it model_metadata.model_groups - A model groups refers to all models that share parameters like classifier type, hyperparameters, etc, but have different training windows . Look at these to see how classifiers perform over different training windows. model_metadata.matrices - Each matrix that was used for training and testing has metadata written about it such as the matrix hash, length, and time configuration. train_results.feature_importances - The sklearn feature importances results for each trained model train_results.predictions - Prediction probabilities for train matrix entities generated against trained models train_results.evaluations - Metric scores of trained models on the training data. test_results.predictions - Prediction probabilities for test matrix entities generated against trained models test_results.evaluations - Metric scores of trained models over given testing windows test_results.individual_importances - Individual feature importance scores for test matrix entities. Here's an example query, which returns the top 10 model groups by precision at the top 100 entities: select model_groups.model_group_id, model_groups.model_type, model_groups.hyperparameters, max(test_evaluations.value) as max_precision from model_metadata.model_groups join model_metadata.models using (model_group_id) join test_results.evaluations using (model_id) where metric = 'precision@' and parameter = '100_abs' group by 1,2,3 order by 4 desc limit 10 Inspecting an Experiment before running \u00b6 Before you run an experiment, you can inspect properties of the Experiment object to ensure that it is configured in the way you want. Some examples: experiment.all_as_of_times for debugging temporal config. This will show all dates that features and labels will be calculated at. experiment.feature_dicts will output a list of feature dictionaries, representing the feature tables and columns configured in this experiment experiment.matrix_build_tasks will output a list representing each matrix that will be built. Optimizing Experiment Performance \u00b6 Profiling an Experiment \u00b6 Experiment running slowly? Try the profile keyword argument, or --profile in the command line. This will output a cProfile file to the project path's profiling_stats directory. This is a binary format but can be read with a variety of visualization programs. snakeviz - A browser based graphical viewer. tuna - Another browser based graphical viewer gprof2dot - A command-line tool to convert files to graphviz format pyprof2calltree - A command-line tool to convert files to Valgrind log format, for viewing in established viewers like KCacheGrind Looking at the profile through a visualization program, you can see which portions of the experiment are taking up the most time. Based on this, you may be able to prioritize changes. For instance, if cohort/label/feature table generation are taking up the bulk of the time, you may add indexes to source tables, or increase the number of database processes. On the other hand, if model training is the culprit, you may temporarily try a smaller grid to get results more quickly. materialize_subquery_fromobjs \u00b6 By default, experiments will inspect the from_obj of every feature aggregation to see if it looks like a subquery, create a table out of it if so, index it on the knowledge_date_column and entity_id , and use that for running feature queries. This can make feature generation go a lot faster if the from_obj takes a decent amount of time to run and/or there are a lot of as-of-dates in the experiment. It won't do this for from_objs that are just tables, or simple joins (e.g. entities join events using (entity_id) ) as the existing indexes you have on those tables should work just fine. You can turn this off if you'd like, which you may want to do if the from_obj subqueries return a lot of data and you want to save as much disk space as possible. The option is turned off by passing materialize_subquery_fromobjs=False to the Experiment. Build Features Independently of Cohort \u00b6 By default the feature queries generated by your feature configuration on any given date are joined with the cohort table on that date, which means that no features for entities not in the cohort are saved. This is to save time and database disk space when your cohort on any given date is not very large and allow you to iterate on feature building quickly by default. However, this means that anytime you change your cohort, you have to rebuild all of your features. Depending on your experiment setup (for instance, multiple large cohorts that you experiment with), this may be time-consuming. Change this by passing features_ignore_cohort=True to the Experiment constructor, or --save-all-features to the command-line. Experiment Classes \u00b6 SingleThreadedExperiment : An experiment that performs all tasks serially in a single thread. Good for simple use on small datasets, or for understanding the general flow of data through a pipeline. MultiCoreExperiment : An experiment that makes use of the pebble library to parallelize various time-consuming steps. Takes an n_processes keyword argument to control how many workers to use. RQExperiment : An experiment that makes use of the python-rq library to enqueue individual tasks onto the default queue, and wait for the jobs to be finished before moving on. python-rq requires Redis and any number of worker processes running the Triage codebase. Triage does not set up any of this needed infrastructure for you. Available through the RQ extra ( pip install triage[rq] )","title":"Running an Experiment"},{"location":"experiments/running/#running-an-experiment","text":"","title":"Running an Experiment"},{"location":"experiments/running/#prerequisites","text":"To use a Triage experiment, you first need: Python 3.5 A PostgreSQL database with your source data (events, geographical data, etc) loaded. Ample space on an available disk (or S3) to store the needed matrices and models for your experiment An experiment definition (see Defining an Experiment ) You may run a Triage experiment two ways: through the Triage command line tool, or through instantiating an Experiment object in your own Python code and running it. The rest of this document will introduce experiment runs of increasing complexity, through both the CLI and Python interfaces.","title":"Prerequisites"},{"location":"experiments/running/#simple-example","text":"To run an experiment, you need to tell triage at a minimum where to find the experiment file (in YAML format), and how to connect to the database, In this simple example, we're assuming that the experiment will be run with only one process, and that the matrices and models should be stored on the local filesystem.","title":"Simple Example"},{"location":"experiments/running/#cli","text":"The Triage CLI defaults database connection information to a file stored in 'database.yaml', so with this you can omit any mention of the database. In addition, if you leave out the project path. In addition, the 'project path' (where matrices and models are stored) defaults to the current working directory. So this is the simplest possible invocation: triage experiment example/config/experiment.yaml If you have the database information stored somewhere else, you may pass it to the top-level 'triage' command: triage -d mydbconfig.yaml experiment example/config/experiment.yaml Assuming you want the matrices and models stored somewhere else, pass it as the --project-path : triage -d mydbconfig.yaml experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data'","title":"CLI"},{"location":"experiments/running/#python","text":"When running an experiment in Python, the database information is passed in the form of a SQLAlchemy database engine, and the experiment information is passed as a dictionary rather as YAML specifically. from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), # http://docs.sqlalchemy.org/en/latest/core/engines.html project_path='/path/to/directory/to/save/data' ) experiment.run()","title":"Python"},{"location":"experiments/running/#multicore-example","text":"Triage also offers the ability to locally parallelize both CPU-heavy and database-heavy tasks. Triage uses the pebble library to perform both of these, but they are separately configurable as the database tasks will more likely be bounded by the number of connections/cores available on the database server instead of the number of cores available on the experiment running machine.","title":"Multicore example"},{"location":"experiments/running/#cli_1","text":"The Triage CLI allows parallelization to be specified through the --n-processes and --n-db-processes parameters. triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --n-db-processes 4 --n-processes 8","title":"CLI"},{"location":"experiments/running/#python_1","text":"In Python, you can use the MultiCoreExperiment instead of the SingleThreadedExperiment , and similarly pass the n_processes and n_db_processes parameters. We also recommend using triage.create_engine . It will create a serializable version of the engine that will be fully reconstructed in multiprocess contexts. If you pass a regular SQLAlchemy engine, in these contexts the engine will be reconstructed with the database URL only , which may cancel other settings you have used to configure your engine. from triage.experiments import MultiCoreExperiment from triage import create_engine experiment = MultiCoreExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), project_path='/path/to/directory/to/save/data', n_db_processes=4, n_processes=8, ) experiment.run() The pebble library offers an interface around Python3's concurrent.futures module that adds in a very helpful tool: watching for killed subprocesses . Model training (and sometimes, matrix building) can be a memory-hungry task, and Triage can not guarantee that the operating system you're running on won't kill the worker processes in a way that prevents them from reporting back to the parent Experiment process. With Pebble, this occurrence is caught like a regular Exception, which allows the Process pool to recover and include the information in the Experiment's log.","title":"Python"},{"location":"experiments/running/#using-s3-to-store-matrices-and-models","text":"Triage can operate on different storage engines for matrices and models, and besides the standard filesystem engine comes with S3 support out of the box. To use this, just use the s3:// scheme for your project_path (this is similar for both Python and the CLI).","title":"Using S3 to store matrices and models"},{"location":"experiments/running/#cli_2","text":"triage experiment example/config/experiment.yaml --project-path 's3://bucket/directory/to/save/data'","title":"CLI"},{"location":"experiments/running/#python_2","text":"from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), project_path='s3://bucket/directory/to/save/data' ) experiment.run()","title":"Python"},{"location":"experiments/running/#using-hdf5-as-a-matrix-storage-format","text":"Triage by default uses CSV format to store matrices, but this can take up a lot of space. However, this is configurable. Triage ships with an HDF5 storage module that you can use.","title":"Using HDF5 as a matrix storage format"},{"location":"experiments/running/#cli_3","text":"On the command-line, this is configurable using the --matrix-format option, and supports csv and hdf . triage experiment example/config/experiment.yaml --matrix-format hdf","title":"CLI"},{"location":"experiments/running/#python_3","text":"In Python, this is configurable using the matrix_storage_class keyword argument. To allow users to write their own storage modules, this is passed in the form of a class. The shipped modules are in triage.component.catwalk.storage . If you'd like to write your own storage module, you can use the existing modules as a guide. from triage.experiments import SingleThreadedExperiment from triage.component.catwalk.storage import HDFMatrixStore experiment = SingleThreadedExperiment( config=experiment_config db_engine=create_engine(...), matrix_storage_class=HDFMatrixStore, project_path='/path/to/directory/to/save/data', ) experiment.run() Note: The HDF storage option is not compatible with S3.","title":"Python"},{"location":"experiments/running/#validating-an-experiment","text":"Configuring an experiment is complex, and running an experiment can take a long time as data scales up. If there are any misconfigured values, it's going to help out a lot to figure out what they are before we run the Experiment. So when you have completed your experiment config and want to test it out, it's best to validate the Experiment first. If any problems are detectable in your Experiment, either in configuration or the database tables referenced by it, this method will throw an exception. For instance, if I refer to the cat_complaints table in a feature aggregation but it doesn't exist, I'll see something like this: *** ValueError: from_obj query does not run. from_obj: \"cat_complaints\" Full error: (psycopg2.ProgrammingError) relation \"cat_complaints\" does not exist LINE 1: explain select * from cat_complaints ^ [SQL: 'explain select * from cat_complaints']","title":"Validating an Experiment"},{"location":"experiments/running/#cli_4","text":"The CLI, by default, validates before running. You can tweak this behavior, and make it not validate, or make it only validate. triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --no-validate triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --validate-only","title":"CLI"},{"location":"experiments/running/#python_4","text":"Experiments expose a validate method that can be run as needed. Experiment instantiation doesn't change from the run examples at all. experiment.validate() By default, the validate method will stop as soon as it encounters an error ('strict' mode). If you would like it to validate each section without stopping (i.e. if you have only written part of the experiment configuration), call validate(strict=False) and all of the errors will be changed to warnings. We'd like to add more validations for common misconfiguration problems over time. If you got an unexpected error that turned out to be related to a confusing configuration value, help us out by adding to the validation module and submitting a pull request!","title":"Python"},{"location":"experiments/running/#restarting-an-experiment","text":"If an experiment fails for any reason, you can restart it. By default, all work will be recreated. This includes label queries, feature queries, matrix building, model training, etc. However, if you pass the replace=False keyword argument, the Experiment will reuse what work it can. Cohort Table: The Experiment refers to a cohort table namespaced by the cohort name and a hash of the cohort query, and in that way allows you to reuse cohorts between different experiments if their label names and queries are identical. When referring to this table, it will check on an as-of-date level whether or not there are any existing rows for that date, and skip the cohort query for that date if so. For this reason, it is not aware of specific entities or source events so if the source data has changed, ensure that replace is set to True. Labels Table: The Experiment refers to a labels table namespaced by the label name and a hash of the label query, and in that way allows you to reuse labels between different experiments if their label names and queries are identical. When referring to this table, it will check on a per- as_of_date / label timespan level whether or not there are any existing rows, and skip the label query if so. For this reason, it is not aware of specific entities or source events so if the label query has changed or the source data has changed, ensure that replace is set to True. Features Tables: The Experiment will check on a per-table basis whether or not it exists and contains rows for the entire cohort, and skip the feature generation if so. It does not look at the column list for the feature table or inspect the feature data itself. So, if you have modified any source data that affects a feature aggregation, or added any columns to that aggregation, you won't want to set replace to False. However, it is cohort-and-date aware so you can change around your cohort and temporal configuration safely. Matrix Building: Each matrix's metadata is hashed to create a unique id. If a file exists in storage with that hash, it will be reused. Model Training: Each model's metadata (which includes its train matrix's hash) is hashed to create a unique id. If a file exists in storage with that hash, it will be reused.","title":"Restarting an Experiment"},{"location":"experiments/running/#cli_5","text":"triage experiment example/config/experiment.yaml --project-path '/path/to/directory/to/save/data' --replace","title":"CLI"},{"location":"experiments/running/#python_5","text":"from triage.experiments import SingleThreadedExperiment experiment = SingleThreadedExperiment( config=experiment_config, # a dictionary db_engine=create_engine(...), project_path='s3://bucket/directory/to/save/data', replace=True ) experiment.run()","title":"Python"},{"location":"experiments/running/#optimizing-an-experiment","text":"","title":"Optimizing an Experiment"},{"location":"experiments/running/#skipping-prediction-syncing","text":"By default, the Experiment will save predictions to the database. This can take a long time if your test matrices have a lot of rows, and isn't quite necessary if you just want to see the high-level performance of your grid. By switching save_predictions to False , you can skip the prediction saving. You'll still get your evaluation metrics, so you can look at performance. Don't worry, you can still get your predictions back later by rerunning the Experiment later at default settings, which will find your already-trained models, generate predictions, and save them. CLI: triage experiment myexperiment.yaml --no-save-predictions Python: SingleThreadedExperiment(..., save_predictions=False)","title":"Skipping Prediction Syncing"},{"location":"experiments/running/#running-parts-of-an-experiment","text":"If you would like incrementally build, or just incrementally run parts of the Experiment look at their outputs, you can do so. Running a full experiment requires the experiment config to be filled out, but when you're getting started using Triage it can be easier to build the experiment piece by piece and see the results as they come in. Make sure logging is set to INFO level before running this to ensure you get all the log messages. Running parts of an experiment is only supported through the Python interface.","title":"Running parts of an Experiment"},{"location":"experiments/running/#python_6","text":"experiment.run() will run until it no longer has enough configuration to proceed. You will see information in the logs telling you about the steps it was able to perform. If you initialize the Experiment with cleanup=False , you can view the intermediate tables that are built. They are modified with the experiment hash that the experiment calculates, but this will be printed out in the log messages. labels_*<experiment_hash>* for the labels generated per entity and as of date. tmp_sparse_states_*<experiment_hash>* for the membership in each cohort per entity and as_of_date To reproduce the entire Experiment piece by piece, you can run the following. Each one of these methods requires some portion of experiment config to be passed: experiment.split_definitions will parse temporal config and create time splits. It only requires temporal_config . experiment.generate_cohort() will use the cohort config and as of dates from the temporal config to generate an internal table keeping track of what entities are in the cohort on different dates. It requires temporal_config and cohort_config . experiment.generate_labels() will use the label config and as of dates from the temporal config to generate an internal labels table. It requires temporal_config and label_config . experiment.generate_preimputation_features() will use the feature aggregation config and as of dates from the temporal config to generate internal features tables. It requires temporal_config and feature_aggregations . experiment.generate_imputed_features() will use the imputation sections of the feature aggregation config and the results from the preimputed features to create internal imputed features tables. It requires temporal_config and feature_aggregations . experiment.build_matrices() will use all of the internal tables generated before this point, along with feature grouping config, to generate all needed matrices. It requires temporal_config , cohort_config , label_config , and feature_aggregations , though it will also use feature_group_definitions , feature_group_strategies , and user_metadata if present. experiment.train_and_test_models() will use the generated matrices, grid config and evaluation metric config to train and test all needed models. It requires all configuration keys.","title":"Python"},{"location":"experiments/running/#evaluating-results-of-an-experiment","text":"After the experiment run, a variety of schemas and tables will be created and populated in the configured database: model_metadata.experiments - The experiment configuration and a hash model_metadata.matrices - Each train or test matrix that is built has a row here, with some basic metadata model_metadata.experiment_matrices - A many-to-many table between experiments and matrices. This will have a row if the experiment used the matrix, regardless of whether or not it had to build it model_metadata.models - A model describes a trained classifier; you'll have one row for each trained file that gets saved. model_metadata.experiment_models - A many-to-many table between experiments and models. This will have a row if the experiment used the model, regardless of whether or not it had to build it model_metadata.model_groups - A model groups refers to all models that share parameters like classifier type, hyperparameters, etc, but have different training windows . Look at these to see how classifiers perform over different training windows. model_metadata.matrices - Each matrix that was used for training and testing has metadata written about it such as the matrix hash, length, and time configuration. train_results.feature_importances - The sklearn feature importances results for each trained model train_results.predictions - Prediction probabilities for train matrix entities generated against trained models train_results.evaluations - Metric scores of trained models on the training data. test_results.predictions - Prediction probabilities for test matrix entities generated against trained models test_results.evaluations - Metric scores of trained models over given testing windows test_results.individual_importances - Individual feature importance scores for test matrix entities. Here's an example query, which returns the top 10 model groups by precision at the top 100 entities: select model_groups.model_group_id, model_groups.model_type, model_groups.hyperparameters, max(test_evaluations.value) as max_precision from model_metadata.model_groups join model_metadata.models using (model_group_id) join test_results.evaluations using (model_id) where metric = 'precision@' and parameter = '100_abs' group by 1,2,3 order by 4 desc limit 10","title":"Evaluating results of an Experiment"},{"location":"experiments/running/#inspecting-an-experiment-before-running","text":"Before you run an experiment, you can inspect properties of the Experiment object to ensure that it is configured in the way you want. Some examples: experiment.all_as_of_times for debugging temporal config. This will show all dates that features and labels will be calculated at. experiment.feature_dicts will output a list of feature dictionaries, representing the feature tables and columns configured in this experiment experiment.matrix_build_tasks will output a list representing each matrix that will be built.","title":"Inspecting an Experiment before running"},{"location":"experiments/running/#optimizing-experiment-performance","text":"","title":"Optimizing Experiment Performance"},{"location":"experiments/running/#profiling-an-experiment","text":"Experiment running slowly? Try the profile keyword argument, or --profile in the command line. This will output a cProfile file to the project path's profiling_stats directory. This is a binary format but can be read with a variety of visualization programs. snakeviz - A browser based graphical viewer. tuna - Another browser based graphical viewer gprof2dot - A command-line tool to convert files to graphviz format pyprof2calltree - A command-line tool to convert files to Valgrind log format, for viewing in established viewers like KCacheGrind Looking at the profile through a visualization program, you can see which portions of the experiment are taking up the most time. Based on this, you may be able to prioritize changes. For instance, if cohort/label/feature table generation are taking up the bulk of the time, you may add indexes to source tables, or increase the number of database processes. On the other hand, if model training is the culprit, you may temporarily try a smaller grid to get results more quickly.","title":"Profiling an Experiment"},{"location":"experiments/running/#materialize_subquery_fromobjs","text":"By default, experiments will inspect the from_obj of every feature aggregation to see if it looks like a subquery, create a table out of it if so, index it on the knowledge_date_column and entity_id , and use that for running feature queries. This can make feature generation go a lot faster if the from_obj takes a decent amount of time to run and/or there are a lot of as-of-dates in the experiment. It won't do this for from_objs that are just tables, or simple joins (e.g. entities join events using (entity_id) ) as the existing indexes you have on those tables should work just fine. You can turn this off if you'd like, which you may want to do if the from_obj subqueries return a lot of data and you want to save as much disk space as possible. The option is turned off by passing materialize_subquery_fromobjs=False to the Experiment.","title":"materialize_subquery_fromobjs"},{"location":"experiments/running/#build-features-independently-of-cohort","text":"By default the feature queries generated by your feature configuration on any given date are joined with the cohort table on that date, which means that no features for entities not in the cohort are saved. This is to save time and database disk space when your cohort on any given date is not very large and allow you to iterate on feature building quickly by default. However, this means that anytime you change your cohort, you have to rebuild all of your features. Depending on your experiment setup (for instance, multiple large cohorts that you experiment with), this may be time-consuming. Change this by passing features_ignore_cohort=True to the Experiment constructor, or --save-all-features to the command-line.","title":"Build Features Independently of Cohort"},{"location":"experiments/running/#experiment-classes","text":"SingleThreadedExperiment : An experiment that performs all tasks serially in a single thread. Good for simple use on small datasets, or for understanding the general flow of data through a pipeline. MultiCoreExperiment : An experiment that makes use of the pebble library to parallelize various time-consuming steps. Takes an n_processes keyword argument to control how many workers to use. RQExperiment : An experiment that makes use of the python-rq library to enqueue individual tasks onto the default queue, and wait for the jobs to be finished before moving on. python-rq requires Redis and any number of worker processes running the Triage codebase. Triage does not set up any of this needed infrastructure for you. Available through the RQ extra ( pip install triage[rq] )","title":"Experiment Classes"},{"location":"experiments/temporal-validation/","text":"Temporal Validation Deep Dive \u00b6 A temporal validation deep dive is currently available in the Dirty Duck tutorial. Dirty Duck - Temporal Cross-validation You can produce the time graphs detailed in the Dirty Duck deep dive using the Triage CLI or through calling Python code directly. The graphs use matplotlib, so you'll need a matplotlib backend to use. Refer to the matplotlib docs for more details. Python Code \u00b6 Plotting is supported through the visualize_chops function, which takes a fully configured Timechop object. You may store the configuration for this object in a YAML file if you wish and load from a file, but in this example we directly set the parameters as arguments to the Timechop object. This would enable faster iteration of time config in a notebook setting. from triage.component.timechop.plotting import visualize_chops from triage.component.timechop import Timechop chopper = Timechop( feature_start_time='2010-01-01' feature_end_time='2015-01-01' # latest date included in features label_start_time='2012-01-01' # earliest date for which labels are avialable label_end_time='2015-01-01' # day AFTER last label date (all dates in any model are < this date) model_update_frequency='6month' # how frequently to retrain models training_as_of_date_frequencies='1day' # time between as of dates for same entity in train matrix test_as_of_date_frequencies='3month' # time between as of dates for same entity in test matrix max_training_histories=['6month', '3month'] # length of time included in a train matrix test_durations=['0day', '1month', '2month'] # length of time included in a test matrix (0 days will give a single prediction immediately after training end) training_label_timespans=['1month'] # time period across which outcomes are labeled in train matrices test_label_timespans=['7day'] # time period across which outcomes are labeled in test matrices ) visualize_chops(chopper) Triage CLI \u00b6 The Triage CLI exposes the showtimechops command which just takes a YAML file as input. This YAML file is expected to have a temporal_config section with Timechop parameters. You can use a full experiment config, or just create a YAML file with only temporal config parameters; the temporal config just has to be present. Here, we use the example_experiment_config.yaml from the Triage repository root as an example. triage showtimechops example_experiment_config.yaml Result \u00b6 Using either method, you should see output similar to this:","title":"Temporal Validation Deep Dive"},{"location":"experiments/temporal-validation/#temporal-validation-deep-dive","text":"A temporal validation deep dive is currently available in the Dirty Duck tutorial. Dirty Duck - Temporal Cross-validation You can produce the time graphs detailed in the Dirty Duck deep dive using the Triage CLI or through calling Python code directly. The graphs use matplotlib, so you'll need a matplotlib backend to use. Refer to the matplotlib docs for more details.","title":"Temporal Validation Deep Dive"},{"location":"experiments/temporal-validation/#python-code","text":"Plotting is supported through the visualize_chops function, which takes a fully configured Timechop object. You may store the configuration for this object in a YAML file if you wish and load from a file, but in this example we directly set the parameters as arguments to the Timechop object. This would enable faster iteration of time config in a notebook setting. from triage.component.timechop.plotting import visualize_chops from triage.component.timechop import Timechop chopper = Timechop( feature_start_time='2010-01-01' feature_end_time='2015-01-01' # latest date included in features label_start_time='2012-01-01' # earliest date for which labels are avialable label_end_time='2015-01-01' # day AFTER last label date (all dates in any model are < this date) model_update_frequency='6month' # how frequently to retrain models training_as_of_date_frequencies='1day' # time between as of dates for same entity in train matrix test_as_of_date_frequencies='3month' # time between as of dates for same entity in test matrix max_training_histories=['6month', '3month'] # length of time included in a train matrix test_durations=['0day', '1month', '2month'] # length of time included in a test matrix (0 days will give a single prediction immediately after training end) training_label_timespans=['1month'] # time period across which outcomes are labeled in train matrices test_label_timespans=['7day'] # time period across which outcomes are labeled in test matrices ) visualize_chops(chopper)","title":"Python Code"},{"location":"experiments/temporal-validation/#triage-cli","text":"The Triage CLI exposes the showtimechops command which just takes a YAML file as input. This YAML file is expected to have a temporal_config section with Timechop parameters. You can use a full experiment config, or just create a YAML file with only temporal config parameters; the temporal config just has to be present. Here, we use the example_experiment_config.yaml from the Triage repository root as an example. triage showtimechops example_experiment_config.yaml","title":"Triage CLI"},{"location":"experiments/temporal-validation/#result","text":"Using either method, you should see output similar to this:","title":"Result"},{"location":"experiments/upgrade-to-v5/","text":"Upgrading your experiment configuration to v5 \u00b6 This document details the steps needed to update a triage v3 or v4 configuration to v5, mimicing the old behavior (as opposed to taking advantage of new options) as much as possible. In the experiment configuration v5, several things were changed: state_config becomes cohort_config , and receives new options label_config is changed to take a parameterized query model_group_keys is changed to have more robust defaults, and values specified in the config file act as overrides for the defaults instead of additions to them. state_config -> cohort_config \u00b6 Upgrading the state config is fairly straightforward, as no functionality was removed. The key at which the state table-based configuration can be passed has changed. Before it resided at the top-level state_config key, whereas now it is in the optional dense_states key within the top-level cohort_config key. Old: state_config: table_name: 'states' state_filters: - 'state_one AND state_two' - '(state_one OR state_two) AND state_three' New: cohort_config: dense_states: table_name: 'states' state_filters: - 'state_one AND state_two' - '(state_one OR state_two) AND state_three' label_config \u00b6 The label config has had functionality changed, so there is more conversion that needs to happen. Instead of taking in an 'events' table and making assumptions suitable for inspections tasks based on that table, for transparency and flexibility this now takes a parameterized query, as well as an optional include_missing_labels_in_train_as boolean. Leaving out this boolean value reproduces the inspections behavior (missing labels are treated as null), so to upgrade old configurations it is not needed. Old: events_table: 'events' New: label_config: query: | select events.entity_id, bool_or(outcome::bool)::integer as outcome from events where '{as_of_date}' <= outcome_date and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id model_group_keys \u00b6 The model group configuration was changed quite a bit. Before, the Experiment defined a few default grouping keys and would treat anything included in the config as additional. In practice, there were many keys that were almost always included as additional model group keys, and these are now default. There are also other keys that generally make sense if certain things are iterated on (e.g. feature groups). The goal is for most projects to simply leave out this configuration value entirely. If possible, this is the recommended route to go. But for the purposes of this guide, this change should duplicate the old behavior exactly. Old (empty, using defaults): New: model_group_keys: ['class_path', 'parameters', 'feature_names'] Old (more standard in practice, adding some temporal parameters): model_group_keys: ['label_timespan', 'as_of_date_frequency', 'max_training_history'] New: model_group_keys: ['class_path', 'parameters', 'feature_names', 'label_timespan', 'as_of_date_frequency', 'max_training_history'] Upgrading the experiment config version \u00b6 At this point, you should be able to bump the top-level experiment config version to v5: Old: config_version: 'v4' New: config_version: 'v5'","title":"Upgrading your experiment configuration to v5"},{"location":"experiments/upgrade-to-v5/#upgrading-your-experiment-configuration-to-v5","text":"This document details the steps needed to update a triage v3 or v4 configuration to v5, mimicing the old behavior (as opposed to taking advantage of new options) as much as possible. In the experiment configuration v5, several things were changed: state_config becomes cohort_config , and receives new options label_config is changed to take a parameterized query model_group_keys is changed to have more robust defaults, and values specified in the config file act as overrides for the defaults instead of additions to them.","title":"Upgrading your experiment configuration to v5"},{"location":"experiments/upgrade-to-v5/#state_config-cohort_config","text":"Upgrading the state config is fairly straightforward, as no functionality was removed. The key at which the state table-based configuration can be passed has changed. Before it resided at the top-level state_config key, whereas now it is in the optional dense_states key within the top-level cohort_config key. Old: state_config: table_name: 'states' state_filters: - 'state_one AND state_two' - '(state_one OR state_two) AND state_three' New: cohort_config: dense_states: table_name: 'states' state_filters: - 'state_one AND state_two' - '(state_one OR state_two) AND state_three'","title":"state_config -&gt; cohort_config"},{"location":"experiments/upgrade-to-v5/#label_config","text":"The label config has had functionality changed, so there is more conversion that needs to happen. Instead of taking in an 'events' table and making assumptions suitable for inspections tasks based on that table, for transparency and flexibility this now takes a parameterized query, as well as an optional include_missing_labels_in_train_as boolean. Leaving out this boolean value reproduces the inspections behavior (missing labels are treated as null), so to upgrade old configurations it is not needed. Old: events_table: 'events' New: label_config: query: | select events.entity_id, bool_or(outcome::bool)::integer as outcome from events where '{as_of_date}' <= outcome_date and outcome_date < '{as_of_date}'::timestamp + interval '{label_timespan}' group by entity_id","title":"label_config"},{"location":"experiments/upgrade-to-v5/#model_group_keys","text":"The model group configuration was changed quite a bit. Before, the Experiment defined a few default grouping keys and would treat anything included in the config as additional. In practice, there were many keys that were almost always included as additional model group keys, and these are now default. There are also other keys that generally make sense if certain things are iterated on (e.g. feature groups). The goal is for most projects to simply leave out this configuration value entirely. If possible, this is the recommended route to go. But for the purposes of this guide, this change should duplicate the old behavior exactly. Old (empty, using defaults): New: model_group_keys: ['class_path', 'parameters', 'feature_names'] Old (more standard in practice, adding some temporal parameters): model_group_keys: ['label_timespan', 'as_of_date_frequency', 'max_training_history'] New: model_group_keys: ['class_path', 'parameters', 'feature_names', 'label_timespan', 'as_of_date_frequency', 'max_training_history']","title":"model_group_keys"},{"location":"experiments/upgrade-to-v5/#upgrading-the-experiment-config-version","text":"At this point, you should be able to bump the top-level experiment config version to v5: Old: config_version: 'v4' New: config_version: 'v5'","title":"Upgrading the experiment config version"},{"location":"experiments/upgrade-to-v6/","text":"Upgrading your experiment configuration to v6 \u00b6 This document details the steps needed to update a triage v5 configuration to v6, mimicking the old behavior. Experiment configuration v6 includes only one change from v5: When specifying the cohort_config , if a query is given , the {af_of_date} is no longer quoted or casted by Triage. Instead, the user must perform the quoting and casting, as is done already for the label_config . Old: cohort_config: query: | SELECT DISTINCT entity_id FROM semantic.events WHERE event = 'booking' AND startdt <@ daterange(({as_of_date} - '3 years'::interval)::date, {as_of_date}) AND enddt < {as_of_date} LIMIT 100 name: 'booking_last_3_years_limit_100' New: cohort_config: query: | SELECT DISTINCT entity_id FROM semantic.events WHERE event = 'booking' AND startdt <@ daterange(('{as_of_date}'::date - '3 years'::interval)::date, '{as_of_date}'::date) AND enddt < '{as_of_date}' LIMIT 100 name: 'booking_last_3_years_limit_100' Upgrading the experiment config version \u00b6 At this point, you should be able to bump the top-level experiment config version to v6: Old: config_version: 'v5' New: config_version: 'v6'","title":"Upgrading your experiment configuration to v6"},{"location":"experiments/upgrade-to-v6/#upgrading-your-experiment-configuration-to-v6","text":"This document details the steps needed to update a triage v5 configuration to v6, mimicking the old behavior. Experiment configuration v6 includes only one change from v5: When specifying the cohort_config , if a query is given , the {af_of_date} is no longer quoted or casted by Triage. Instead, the user must perform the quoting and casting, as is done already for the label_config . Old: cohort_config: query: | SELECT DISTINCT entity_id FROM semantic.events WHERE event = 'booking' AND startdt <@ daterange(({as_of_date} - '3 years'::interval)::date, {as_of_date}) AND enddt < {as_of_date} LIMIT 100 name: 'booking_last_3_years_limit_100' New: cohort_config: query: | SELECT DISTINCT entity_id FROM semantic.events WHERE event = 'booking' AND startdt <@ daterange(('{as_of_date}'::date - '3 years'::interval)::date, '{as_of_date}'::date) AND enddt < '{as_of_date}' LIMIT 100 name: 'booking_last_3_years_limit_100'","title":"Upgrading your experiment configuration to v6"},{"location":"experiments/upgrade-to-v6/#upgrading-the-experiment-config-version","text":"At this point, you should be able to bump the top-level experiment config version to v6: Old: config_version: 'v5' New: config_version: 'v6'","title":"Upgrading the experiment config version"},{"location":"experiments/upgrading/","text":"Upgrading an Experiment config \u00b6 v5 \u2192 v6 v3/v4 \u2192 v5","title":"Upgrading an Experiment"},{"location":"experiments/upgrading/#upgrading-an-experiment-config","text":"v5 \u2192 v6 v3/v4 \u2192 v5","title":"Upgrading an Experiment config"}]}